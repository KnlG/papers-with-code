[
{"0": "Computer Vision", "1": "Natural Language Processing", "2": "Medical", "3": "Methodology", "4": "Miscellaneous", "5": "Graphs", "6": "Playing Games", "7": "Speech", "8": "Time Series", "9": "Computer Code", "10": "Audio", "11": "Robots", "12": "Music", "13": "Reasoning", "14": "Knowledge Base", "15": "Adversarial"},
{"field": "Playing Games", "subfields": {"0": "3D", "1": "Video Games", "2": "Continuous Control", "3": "Atari Games", "4": "General Reinforcement Learning", "5": "Real-Time Strategy Games", "6": "Hierarchical Reinforcement Learning", "7": "Board Games", "8": "Card Games", "9": "text-based games", "10": "Game of Go", "11": "FPS Games", "12": "Game of Chess", "13": "Offline RL", "14": "Game of Football", "15": "Game of Poker", "16": "NetHack", "17": "Game of Shogi", "18": "Game of Suduko", "19": "Game of Cricket", "20": "Multi-Agent Path Finding"}},
{"field": "Knowledge Base", "subfields": {"0": "Knowledge Graphs", "1": "Malware Classification", "2": "Causal Discovery", "3": "Knowledge Base-duplicate", "4": "Text Simplification", "5": "Knowledge Graph Completion", "6": "RDF Dataset Discovery", "7": "Knowledge Graphs Data Curation", "8": "cross-lingual sememe prediction"}},
{"field": "Graphs", "subfields": {"0": "Representation Learning", "1": "Anomaly Detection", "2": "Link Prediction", "3": "Graph Embedding", "4": "Node Classification", "5": "Contrastive Learning", "6": "Graph Classification", "7": "Community Detection", "8": "Graph Learning", "9": "Learning-To-Rank", "10": "Graph Generation", "11": "Graph Clustering", "12": "Graph Matching", "13": "Triple Classification", "14": "graph construction", "15": "Topological Data Analysis", "16": "Material Classification", "17": "Graph Regression", "18": "Graph Similarity", "19": "graph partitioning", "20": "Sketch", "21": "Physics-informed machine learning", "22": "Collaborative Ranking", "23": "Spectral Graph Clustering", "24": "Vector Graphics", "25": "Molecular Dynamics", "26": "Connectivity Estimation", "27": "MD17 dataset", "28": "hypergraph embedding", "29": "Graph-To-Graph Translation", "30": "Hypergraph Matching", "31": "Link Sign Prediction", "32": "hypergraph partitioning", "33": "Gene Interaction Prediction", "34": "Graph Ranking", "35": "Triad Prediction", "36": "Graphon Estimation"}},
{"field": "Natural Language Processing", "subfields": {"0": "Representation Learning", "1": "Machine Translation", "2": "Question Answering", "3": "Language Modelling", "4": "Text Classification", "5": "Sentiment Analysis", "6": "Data Augmentation", "7": "Text Generation", "8": "Named Entity Recognition", "9": "Text Summarization", "10": "Reading Comprehension", "11": "Relation Extraction", "12": "Natural Language Inference", "13": "Information Retrieval", "14": "Image Captioning", "15": "Dependency Parsing", "16": "Dialogue", "17": "Semantic Textual Similarity", "18": "Emotion Recognition", "19": "Semantic Parsing", "20": "Sentence Embeddings", "21": "Sentence Pair Modeling", "22": "Part-Of-Speech Tagging", "23": "Topic Models", "24": "Coreference Resolution", "25": "Entity Linking", "26": "Cross-Lingual", "27": "3D Absolute Human Pose Estimation", "28": "Semantic Role Labeling", "29": "Information Extraction", "30": "Question Generation", "31": "Relational Reasoning", "32": "Word Sense Disambiguation", "33": "Open Information Extraction", "34": "Relation Classification", "35": "Data Mining", "36": "Abuse Detection", "37": "Constituency Parsing", "38": "Tokenization", "39": "Grammatical Error Correction", "40": "Fake News Detection", "41": "Language Identification", "42": "Morphological Analysis", "43": "Slot Filling", "44": "Text Matching", "45": "Chatbot", "46": "Chunking", "47": "Chinese", "48": "Lemmatization", "49": "Dialogue Understanding", "50": "Text Simplification", "51": "Paraphrase Identification", "52": "Hate Speech Detection", "53": "Intent Detection", "54": "Word Alignment", "55": "Entity Typing", "56": "Language Acquisition", "57": "Stance Detection", "58": "Entity Alignment", "59": "Text-To-Speech Synthesis", "60": "Entity Disambiguation", "61": "Entity Extraction using GAN", "62": "Graph-to-Sequence", "63": "Aspect-Based Sentiment Analysis", "64": "Multi-Label Text Classification", "65": "Ad-Hoc Information Retrieval", "66": "Document Ranking", "67": "Amr Parsing", "68": "Sarcasm Detection", "69": "Conversational Response Selection", "70": "Discourse Parsing", "71": "Intent Classification", "72": "Sentence Summarization", "73": "Linguistic Acceptability", "74": "Abusive Language", "75": "Morphological Tagging", "76": "Morphological Inflection", "77": "Multimodal Deep Learning", "78": "Knowledge Base Population", "79": "Subjectivity Analysis", "80": "Word Sense Induction", "81": "Code Summarization", "82": "Semantic Composition", "83": "Bias Detection", "84": "Data-to-Text Generation", "85": "Nested Mention Recognition", "86": "Phrase Grounding", "87": "Question Similarity", "88": "Keyword Extraction", "89": "Lexical Simplification", "90": "Rumour Detection", "91": "Text Clustering", "92": "Cross-Lingual Transfer", "93": "Entity Resolution", "94": "Humor Detection", "95": "Lexical Normalization", "96": "Propaganda detection", "97": "CCG Supertagging", "98": "Conversational Response Generation", "99": "Sentence Ordering", "100": "Clickbait Detection", "101": "Text Attribute Transfer", "102": "Negation Detection", "103": "Arabic Text Diacritization", "104": "Automated Essay Scoring", "105": "Cross-Lingual Document Classification", "106": "Passage Re-Ranking", "107": "Abstractive Text Summarization", "108": "Aggression Identification", "109": "Attribute Value Extraction", "110": "Complex Word Identification", "111": "Cross-Lingual Bitext Mining", "112": "Dialog Act Classification", "113": "Hypernym Discovery", "114": "Lexical Analysis", "115": "Meeting Summarization", "116": "Relationship Extraction (Distant Supervised)", "117": "Anaphora Resolution", "118": "Abstract Argumentation", "119": "Action Parsing", "120": "Arabic Sentiment Analysis", "121": "Author Attribution", "122": "Dialogue Rewriting", "123": "Memex Question Answering", "124": "Misogynistic Aggression Identification", "125": "Natural Language Transduction", "126": "Semantic Retrieval", "127": "Sentence Compression", "128": "Table-based Fact Verification", "129": "Thai Word Segmentation", "130": "Commonsense Reasoning for RL", "131": "Context Query Reformulation", "132": "Extractive Tags Summarization", "133": "Gender Bias Detection", "134": "Joint NER and Classification", "135": "Meme Classification", "136": "Multi-Grained Named Entity Recognition", "137": "Multilingual Machine Comprehension in English Hindi", "138": "News Annotation", "139": "Overlapping Mention Recognition", "140": "Phrase Vector Embedding", "141": "Query Wellformedness", "142": "Table-to-Text Generation", "143": "Text Effects Transfer", "144": "Text Style Transfer", "145": "Twitter Event Detection", "146": "Web Page Tagging", "147": "Zero-Shot Machine Translation", "148": "incongruity detection", "149": "multi-word expression embedding", "150": "multi-word expression sememe prediction", "151": "Speculation Detection", "152": "Automated Writing Evaluation", "153": "Automatic Writing", "154": "Counterspeech Detection", "155": "Document Classification", "156": "Extractive Document Summarization", "157": "Meme Captioning", "158": "Relation Mention Extraction", "159": "Event Extraction"}},
{"field": "Miscellaneous", "subfields": {"0": "Recommendation Systems", "1": "3D Car Instance Understanding", "2": "Continual Learning", "3": "Model Compression", "4": "Topic Models", "5": "Causal Inference", "6": "Malware Classification", "7": "Multi-Armed Bandits", "8": "Intrusion Detection", "9": "Hypothesis Testing", "10": "misinformation", "11": "Multi-Modal", "12": "Click-Through Rate Prediction", "13": "Survival Analysis", "14": "Deep Clustering", "15": "Remote Sensing", "16": "Fraud Detection", "17": "Sensor Fusion", "18": "Robotic Grasping", "19": "Automated Theorem Proving", "20": "Data Visualization", "21": "Prediction Intervals", "22": "Weather Forecasting", "23": "Knowledge Tracing", "24": "Ecommerce", "25": "Open Set Learning", "26": "Fault Detection", "27": "Data Summarization", "28": "Molecular Property Prediction", "29": "Autonomous Driving", "30": "Artificial Life", "31": "Physical Simulations", "32": "Load Forecasting", "33": "Brain Decoding", "34": "Offline RL", "35": "Table Detection", "36": "Vulnerability Detection", "37": "Mathematical Proofs", "38": "Multilingual text classification", "39": "Non-Intrusive Load Monitoring", "40": "Recipe Generation", "41": "Image/Document Clustering", "42": "Classification Of Variable Stars", "43": "Cyber Attack Detection", "44": "Interpretability Techniques for Deep Learning", "45": "Multi-target regression", "46": "Twitter Bot Detection", "47": "Seismic Interpretation", "48": "Deception Detection", "49": "Air Pollution Prediction", "50": "MD17 dataset", "51": "Problem Decomposition", "52": "Traffic Classification", "53": "Crime Prediction", "54": "Crop Classification", "55": "Cryptanalysis", "56": "Next-basket recommendation", "57": "X-Ray Diffraction (XRD)", "58": "Ancient Text Restoration", "59": "Business Taxonomy Construction", "60": "Crowd Flows Prediction", "61": "Food recommendation", "62": "Gender Bias Detection", "63": "Gravitational Wave Detection", "64": "JSONiq Query Execution", "65": "Link Quality Estimation", "66": "Mobile Security", "67": "Modeling Local Geometric Structure", "68": "Network Congestion Control", "69": "Neural Network Security", "70": "Oceanic Eddy Classification", "71": "Outdoor Positioning", "72": "Photometric Redshift Estimation", "73": "Pulsar Prediction", "74": "Radio Interferometry", "75": "Sequential Distribution Function Estimation", "76": "Sequential Quantile Estimation", "77": "Smart Grid Prediction", "78": "Time Offset Calibration", "79": "Non-Linear Elasticity", "80": "Advertising", "81": "Air Quality Inference", "82": "Cyber Attack Investigation", "83": "Home Activity Monitoring"}},
{"field": "Adversarial", "subfields": {"0": "Adversarial Attack", "1": "Adversarial Defense", "2": "Data Poisoning", "3": "Adversarial Text", "4": "Inference Attack", "5": "Website Fingerprinting Attacks"}},
{"field": "Methodology", "subfields": {"0": "Representation Learning", "1": "Transfer Learning", "2": "Clustering", "3": "Domain Adaptation", "4": "Word Embeddings", "5": "Meta-Learning", "6": "Data Augmentation", "7": "AutoML", "8": "Few-Shot Learning", "9": "Zero-Shot Learning", "10": "Anomaly Detection", "11": "Dimensionality Reduction", "12": "Bayesian Inference", "13": "Neural Architecture Search", "14": "Quantization", "15": "Gaussian Processes", "16": "Metric Learning", "17": "Feature Selection", "18": "Active Learning", "19": "Feature Engineering", "20": "Imitation Learning", "21": "Q-Learning", "22": "Stochastic Optimization", "23": "Model Selection", "24": "Density Estimation", "25": "Continual Learning", "26": "Latent Variable Models", "27": "Model Compression", "28": "Multi-Label Classification", "29": "Structured Prediction", "30": "Federated Learning", "31": "EEG", "32": "Multi-agent Reinforcement Learning", "33": "Outlier Detection", "34": "Incremental Learning", "35": "Computed Tomography (CT)", "36": "Matrix Completion", "37": "Dictionary Learning", "38": "Combinatorial Optimization", "39": "Feature Importance", "40": "Network Pruning", "41": "Hierarchical Reinforcement Learning", "42": "Electrocardiography (ECG)", "43": "Multiple Instance Learning", "44": "Interpretable Machine Learning", "45": "Point Processes", "46": "Efficient Exploration", "47": "Policy Gradient Methods", "48": "One-Shot Learning", "49": "Entity Embeddings", "50": "Unsupervised Pre-training", "51": "Bayesian Optimisation", "52": "Multi-Label Learning", "53": "Quantum Machine Learning", "54": "Inductive logic programming", "55": "Tensor Networks", "56": "Multi-Label Text Classification", "57": "Sparse Learning", "58": "Decision Making Under Uncertainty", "59": "One-class classifier", "60": "Unsupervised Representation Learning", "61": "L2 Regularization", "62": "Normalising Flows", "63": "Accuracy Metrics", "64": "Multi-Goal Reinforcement Learning", "65": "Mutual Information Estimation", "66": "Generalized Few-Shot Learning", "67": "Long-tail Learning", "68": "Extreme Multi-Label Classification", "69": "Multiobjective Optimization", "70": "Distributional Reinforcement Learning", "71": "Model extraction", "72": "Information Plane", "73": "Generalized Few-Shot Classification", "74": "Privacy Preserving Deep Learning", "75": "Automatic Machine Learning Model Selection", "76": "Detection of Dependencies", "77": "Influence Approximation", "78": "Quantum Circuit Equivalence Checking", "79": "Clustering Algorithms Evaluation", "80": "Quantum Circuit Mapping", "81": "Web Credibility", "82": "Core set discovery", "83": "Transfer Reinforcement Learning"}},
{"field": "Medical", "subfields": {"0": "Semantic Segmentation", "1": "Medical Image Segmentation", "2": "3D", "3": "EEG", "4": "3D Action Recognition", "5": "Drug Discovery", "6": "3D Absolute Human Pose Estimation", "7": "Electrocardiography (ECG)", "8": "COVID-19 Diagnosis", "9": "Medical Diagnosis", "10": "Medical Image Registration", "11": "Cancer", "12": "Disease Prediction", "13": "Synthetic Data Generation", "14": "Sleep Quality", "15": "Mortality Prediction", "16": "Epidemiology", "17": "Medical Image Generation", "18": "Length-of-Stay prediction", "19": "noise estimation", "20": "Photoplethysmography (PPG)", "21": "Skin", "22": "Histopathological Image Classification", "23": "Multi-tissue Nucleus Segmentation", "24": "Pneumonia Detection", "25": "Seizure Detection", "26": "Breast Tumour Classification", "27": "Diabetic Retinopathy Detection", "28": "Protein Secondary Structure Prediction", "29": "Colorectal Gland Segmentation:", "30": "Medical Relation Extraction", "31": "Skull Stripping", "32": "Electromyography (EMG)", "33": "Tomography", "34": "Patient Outcomes", "35": "Computational Phenotyping", "36": "Lung Nodule Classification", "37": "Mitosis Detection", "38": "Mammogram", "39": "Seizure prediction", "40": "Molecular Dynamics", "41": "Lung Disease Classification", "42": "Lung Nodule Detection", "43": "Magnetic Resonance Fingerprinting", "44": "Metal Artifact Reduction", "45": "Multi-Label Classification Of Biomedical Texts", "46": "Readmission Prediction", "47": "X-Ray", "48": "Automatic Sleep Stage Classification", "49": "Diabetic Foot Ulcer Detection", "50": "ECG Classification", "51": "Eeg Decoding", "52": "Epilepsy Prediction", "53": "Immune Repertoire Classification", "54": "Participant Intervention Comparison Outcome Extraction", "55": "Protein Function Prediction", "56": "Surgical Gesture Recognition", "57": "Surgical Skills Evaluation", "58": "Ultrasound", "59": "Cancer Metastasis Detection", "60": "Chemical Reaction Prediction", "61": "Diabetes Prediction", "62": "Knee Osteoarthritis Prediction", "63": "Medical Report Generation", "64": "Medical Super-Resolution", "65": "Molecule Interpretation", "66": "Pain Intensity Regression", "67": "Population Assignment", "68": "Pulmonary Embolism Detection", "69": "Single-cell modeling", "70": "White Matter Fiber Tractography", "71": "breast density classification", "72": "Atrial Fibrillation", "73": "Age-Related Macular Degeneration Classification", "74": "Ecg Risk Stratification", "75": "Malaria Risk Exposure Prediction", "76": "Medical Code Prediction", "77": "Motion Correction In Multishot Mri", "78": "Multi Diseases Detection", "79": "Muscular Movement Recognition", "80": "Sequential Diagnosis"}},
{"field": "Reasoning", "subfields": {"0": "Decision Making", "1": "Variational Inference", "2": "General Reinforcement Learning", "3": "Common Sense Reasoning", "4": "Program Synthesis", "5": "Visual Reasoning", "6": "Program Repair", "7": "Decision Making Under Uncertainty", "8": "Systematic Generalization", "9": "Natural Language Visual Grounding", "10": "Math Word Problem Solving", "11": "Causal Identification", "12": "Commonsense Reasoning for RL", "13": "Pre-election ratings estimation"}},
{"field": "Computer Vision", "subfields": {"0": "Semantic Segmentation", "1": "Image Classification", "2": "Object Detection", "3": "Domain Adaptation", "4": "Image Generation", "5": "Pose Estimation", "6": "Data Augmentation", "7": "Autonomous Vehicles", "8": "Super-Resolution", "9": "Denoising", "10": "Video", "11": "Activity Recognition", "12": "Facial Recognition and Modelling", "13": "Temporal Action Localization", "14": "3D Car Instance Understanding", "15": "Depth Estimation", "16": "Few-Shot Learning", "17": "Action Recognition", "18": "Zero-Shot Learning", "19": "Anomaly Detection", "20": "Video Classification", "21": "Dimensionality Reduction", "22": "Image Retrieval", "23": "Medical Image Segmentation", "24": "Quantization", "25": "3D", "26": "Instance Segmentation", "27": "Scene Parsing", "28": "Object Recognition", "29": "Action Localization", "30": "Optical Flow Estimation", "31": "Person Re-Identification", "32": "Style Transfer", "33": "fairness", "34": "Image Captioning", "35": "Visual Question Answering", "36": "Adversarial Attack", "37": "Self-Supervised Learning", "38": "Continuous Control", "39": "Emotion Recognition", "40": "Gesture Recognition", "41": "Video Object Segmentation", "42": "Contrastive Learning", "43": "Object Tracking", "44": "Image Restoration", "45": "Action Detection", "46": "Image Reconstruction", "47": "Optical Character Recognition", "48": "Image Inpainting", "49": "Object Localization", "50": "Saliency Detection", "51": "Object Classification", "52": "Trajectory Prediction", "53": "Scene Text", "54": "3D Action Recognition", "55": "Video Semantic Segmentation", "56": "Image Enhancement", "57": "Image Quality Assessment", "58": "Visual Tracking", "59": "Motion Capture", "60": "Deblurring", "61": "Image-to-Image Translation", "62": "Hand", "63": "3D Absolute Human Pose Estimation", "64": "Image Compression", "65": "Image Registration", "66": "Motion Estimation", "67": "Human Interaction Recognition", "68": "Action Classification", "69": "Hand Gesture Recognition", "70": "Video Captioning", "71": "Image Clustering", "72": "Colorization", "73": "Dehazing", "74": "Scene Text Detection", "75": "Crowds", "76": "Object Reconstruction", "77": "Electron Microscopy", "78": "Stereo Matching", "79": "Compressive Sensing", "80": "Out-of-Distribution Detection", "81": "Saliency Prediction", "82": "Few-Shot Transfer Learning for Saliency Prediction", "83": "Hyperspectral", "84": "Edge Detection", "85": "3D Semantic Segmentation", "86": "Activity Prediction", "87": "Image Manipulation", "88": "Eye Tracking", "89": "Omniglot", "90": "Scene Classification", "91": "Visual Localization", "92": "Robot Navigation", "93": "Rain Removal", "94": "Human-Object Interaction Detection", "95": "Image Dehazing", "96": "Point Cloud Registration", "97": "Medical Diagnosis", "98": "Visual Place Recognition", "99": "3D Object Reconstruction", "100": "whole slide images", "101": "Remote Sensing", "102": "Texture Synthesis", "103": "Human Parsing", "104": "Boundary Detection", "105": "Face Model", "106": "Stereo Matching Hand", "107": "3D Face Reconstruction", "108": "Point Cloud Generation", "109": "Face Reconstruction", "110": "Human Detection", "111": "Material Classification", "112": "Disease Prediction", "113": "Depth Completion", "114": "Novel View Synthesis", "115": "3D Human Pose Estimation", "116": "Document Layout Analysis", "117": "Interest Point Detection", "118": "Gaze Estimation", "119": "Template Matching", "120": "Disparity Estimation", "121": "Image Matting", "122": "Motion Segmentation", "123": "Video Summarization", "124": "Camera Localization", "125": "Temporal Localization", "126": "Hand Pose Estimation", "127": "Future prediction", "128": "Image Recognition", "129": "MULTI-VIEW LEARNING", "130": "Object Discovery", "131": "Reconstruction", "132": "Color Constancy", "133": "Image Categorization", "134": "Activity Detection", "135": "Automatic Post-Editing", "136": "Pose Tracking", "137": "Intelligent Surveillance", "138": "Deep Attention", "139": "Image Cropping", "140": "Scene Flow Estimation", "141": "Scene Generation", "142": "Semi-Supervised Video Object Segmentation", "143": "Texture Classification", "144": "Visual Recognition", "145": "Sign Language Recognition", "146": "Video Question Answering", "147": "Virtual Try-on", "148": "Sketch", "149": "Activity Recognition In Videos", "150": "RGB Salient Object Detection", "151": "3D Point Cloud Classification", "152": "Interactive Segmentation", "153": "Video Inpainting", "154": "Image Super-Resolution", "155": "Face Generation", "156": "Image Matching", "157": "Autonomous Driving", "158": "Shadow Detection", "159": "Image Forensics", "160": "Object Counting", "161": "Person Search", "162": "Viewpoint Estimation", "163": "3D Object Detection", "164": "3D FACE MODELING", "165": "Caricature", "166": "Lipreading", "167": "Weakly Supervised Action Localization", "168": "motion synthesis", "169": "DeepFake Detection", "170": "GPR", "171": "Reflection Removal", "172": "Rotated MNIST", "173": "Facial Expression Recognition", "174": "Few-Shot Semantic Segmentation", "175": "Human Part Segmentation", "176": "Iris Recognition", "177": "Line Segment Detection", "178": "Probabilistic Deep Learning", "179": "Text Spotting", "180": "Traffic Sign Detection", "181": "Video Quality Assessment", "182": "indoor scene understanding", "183": "Human Dynamics", "184": "Multi-Object Tracking", "185": "Talking Head Generation", "186": "Action Anticipation", "187": "Contour Detection", "188": "Human action generation", "189": "Image Deconvolution", "190": "Motion Detection", "191": "Vision-Language Navigation", "192": "Spoof Detection", "193": "Action Quality Assessment", "194": "Camouflaged Object Segmentation", "195": "Cloud Detection", "196": "Image Manipulation Detection", "197": "Image Quality Estimation", "198": "Infrared And Visible Image Fusion", "199": "Object Segmentation", "200": "image smoothing", "201": "3D Multi-Person Pose Estimation", "202": "3D Face Animation", "203": "Facial Landmark Detection", "204": "Art Analysis", "205": "Event-based vision", "206": "Horizon Line Estimation", "207": "Image Morphing", "208": "Material Recognition", "209": "Multiview Learning", "210": "Person Identification", "211": "Person Recognition", "212": "Road Damage Detection", "213": "Surface Normals Estimation", "214": "Symmetry Detection", "215": "Video Enhancement", "216": "Gait Recognition", "217": "Dense Pixel Correspondence Estimation", "218": "Document Image Classification", "219": "Face Reenactment", "220": "Image Imputation", "221": "Scanpath prediction", "222": "Segmentation Of Remote Sensing Imagery", "223": "Sign Language Translation", "224": "The Semantic Segmentation Of Remote Sensing Imagery", "225": "Video Reconstruction", "226": "Video Understanding", "227": "3D Shape Reconstruction From A Single 2D Image", "228": "Content-Based Image Retrieval", "229": "Image/Document Clustering", "230": "Instance Search", "231": "Scene Understanding", "232": "severity prediction", "233": "3D Reconstruction", "234": "3D Shape Modeling", "235": "Document Binarization", "236": "Fake Image Detection", "237": "Image Similarity Search", "238": "Image Steganography", "239": "Image Stitching", "240": "Multimodal Machine Translation", "241": "Person Retrieval", "242": "Talking Face Generation", "243": "Text based Person Retrieval", "244": "Unsupervised Object Segmentation", "245": "Video Retrieval", "246": "3D Hand Pose Estimation", "247": "Deception Detection", "248": "Face Recognition", "249": "Unsupervised Image-To-Image Translation", "250": "3D Shape Reconstruction", "251": "Camouflage Segmentation", "252": "Cross-Domain Few-Shot", "253": "Face Detection", "254": "Foveation", "255": "GAN image forensics", "256": "Gaze Prediction", "257": "Handwriting Verification", "258": "Handwriting generation", "259": "Point Cloud Super Resolution", "260": "Sar Image Despeckling", "261": "Video Prediction", "262": "Weakly-supervised instance segmentation", "263": "motion retargeting", "264": "Animation", "265": "Observation Completion", "266": "Text-To-Image", "267": "4D Spatio Temporal Semantic Segmentation", "268": "6D Pose Estimation", "269": "Defocus Estimation", "270": "Depth Image Estimation", "271": "Detect Forged Images And Videos", "272": "Food Recognition", "273": "Generalized Zero-Shot Learning - Unseen", "274": "Historical Color Image Dating", "275": "Intrinsic Image Decomposition", "276": "JPEG Artifact Removal", "277": "Multiple People Tracking", "278": "Referring Expression Segmentation", "279": "Scene-Aware Dialogue", "280": "Single-object discovery", "281": "Sketch-Based Image Retrieval", "282": "Spatial Relation Recognition", "283": "Traffic Accident Detection", "284": "Universal Domain Adaptation", "285": "Video Forensics", "286": "Video Restoration", "287": "Video Style Transfer", "288": "Visual Sentiment Prediction", "289": "Zero Shot Segmentation", "290": "drone-based object tracking", "291": "Forgery", "292": "Pornography Detection", "293": "3D Depth Estimation", "294": "3D Multi-Person Pose Estimation (absolute)", "295": "3D Multi-Person Pose Estimation (root-relative)", "296": "3D Object Classification", "297": "Amodal Layout Estimation", "298": "Audio-Visual Synchronization", "299": "Autonomous Navigation", "300": "Birds Eye View Object Detection", "301": "Blink estimation", "302": "Constrained Diffeomorphic Image Registration", "303": "Detecting Shadows", "304": "Dynamic Texture Recognition", "305": "Event data classification", "306": "Face Anonymization", "307": "Face Verification", "308": "Few-Shot Image Classification", "309": "Fine-Grained Image Classification", "310": "Flooded Building Segmentation", "311": "Gait Identification", "312": "House Generation", "313": "Image Shadow Removal", "314": "Image Text Removal", "315": "Image-To-Gps Verification", "316": "Kiss Detection", "317": "Landmine", "318": "Learning to Paint", "319": "Lightfield", "320": "Lip to Speech Synthesis", "321": "Manufacturing Quality Control", "322": "Metamerism", "323": "Motion Forecasting", "324": "Multi-Person Pose Estimation and Tracking", "325": "Multi-modal image segmentation", "326": "Multi-object colocalization", "327": "Multi-object discovery", "328": "Occluded 3D Object Symmetry Detection", "329": "Part-level Panoptic Segmentation", "330": "Personality Trait Recognition", "331": "Physical Attribute Prediction", "332": "Pose Retrieval", "333": "Pulmorary Vessel Segmentation", "334": "Safety Perception Recognition", "335": "Shape Representation Of 3D Point Clouds", "336": "Steganographics", "337": "Trademark Retrieval", "338": "Transform A Video Into A Comics", "339": "Transparency Separation", "340": "Transparent Object Detection", "341": "Typeface Completion", "342": "Unbalanced Segmentation", "343": "Vehicle Key-Point and Orientation Estimation", "344": "Video Correspondence Flow", "345": "Video Generation", "346": "Visual Social Relationship Recognition", "347": "Visual Speech Recognition", "348": "Weakly-supervised panoptic segmentation", "349": "Window Detection", "350": "interestingness detection", "351": "Spectral Estimation", "352": "computational manga", "353": "3D Canonical Hand Pose Estimation", "354": "Comics processing", "355": "Damaged Building Detection", "356": "Document To Image Conversion", "357": "Frame Duplication Detection", "358": "Image Comprehension", "359": "Image Declipping", "360": "Landmark Tracking", "361": "Logo Recognition", "362": "Materials Imaging", "363": "Motion Detection In Non-Stationary Scenes", "364": "Population Mapping", "365": "Prediction Of Occupancy Grid Maps", "366": "Small Object Detection", "367": "Spectrum Cartography", "368": "Thermal Image Denoising", "369": "Yield Mapping In Apple Orchards", "370": "Animal Pose Estimation"}},
{"field": "Audio", "subfields": {"0": "Music Generation", "1": "Audio Classification", "2": "Sound Event Detection", "3": "Audio Generation", "4": "Acoustic Scene Classification", "5": "Audio Tagging", "6": "Image Super-Resolution", "7": "Environmental Sound Classification", "8": "Instance Search", "9": "Chord Recognition", "10": "Audio Denoising", "11": "Audio Signal Recognition", "12": "Bird Classification", "13": "Acoustic Novelty Detection", "14": "Audio declipping", "15": "Audio-Visual Synchronization", "16": "Direction of Arrival Estimation", "17": "Shooter Localization", "18": "Voice Anti-spoofing", "19": "Vowel Classification"}},
{"field": "Time Series", "subfields": {"0": "Time Series", "1": "Gesture Recognition", "2": "Imputation", "3": "EEG", "4": "Trajectory Prediction", "5": "Time Series Forecasting", "6": "Time Series Classification", "7": "Activity Prediction", "8": "Time Series Prediction", "9": "Medical Diagnosis", "10": "Time Series Analysis", "11": "Stock Market Prediction", "12": "Traffic Prediction", "13": "Change Point Detection", "14": "Time Series Clustering", "15": "Portfolio Optimization", "16": "Irregular Time Series", "17": "Spatio-Temporal Forecasting", "18": "Time Series Regression", "19": "Time-to-Event Prediction", "20": "Time Series Averaging", "21": "Unsupervised Spatial Clustering", "22": "Clustering Multivariate Time Series", "23": "Moving Point Cloud Processing", "24": "Sleep spindles detection", "25": "Time Series Alignment", "26": "Time Series Denoising"}},
{"field": "Speech", "subfields": {"0": "Speech Recognition", "1": "3D", "2": "Emotion Recognition", "3": "Speech Synthesis", "4": "Speech Enhancement", "5": "Speaker Verification", "6": "Voice Conversion", "7": "End-To-End Speech Recognition", "8": "Speech Separation", "9": "Keyword Spotting", "10": "Spoken Language Understanding", "11": "Speaker Recognition", "12": "Text-To-Speech Synthesis", "13": "Speaker Diarization", "14": "Speaker Identification", "15": "Spoken Dialogue Systems", "16": "Speaker Separation", "17": "Unsupervised Representation Learning", "18": "Large Vocabulary Continuous Speech Recognition", "19": "Acoustic Modelling", "20": "Audio-Visual Speech Recognition", "21": "Text-Independent Speaker Recognition", "22": "Manner Of Articulation Detection", "23": "Speech-to-Gesture Translation", "24": "Unsupervised Speech Recognition", "25": "Acoustic Question Answering", "26": "Speaking Style Synthesis", "27": "Voice Query Recognition"}},
{"field": "Music", "subfields": {"0": "Style Transfer", "1": "Music Generation", "2": "Music Information Retrieval", "3": "3D Human Pose Estimation", "4": "Music Source Separation", "5": "Music Modeling", "6": "Music Classification", "7": "3D Face Animation", "8": "Music Auto-Tagging", "9": "Music Genre Recognition", "10": "Cover song identification", "11": "Drum Transcription", "12": "Melody Extraction", "13": "Music Emotion Recognition", "14": "Piano Music Modeling", "15": "Detection Of Instrumentals Musical Tracks", "16": "Recognizing Seven Different Dastgahs Of Iranian Classical Music"}},
{"field": "Robots", "subfields": {"0": "Semantic Segmentation", "1": "Speech Recognition", "2": "General Reinforcement Learning", "3": "Visual Odometry", "4": "Robot Navigation", "5": "Human robot  interaction", "6": "Motion Planning", "7": "Visual Navigation", "8": "Robotic Grasping", "9": "Safe Exploration", "10": "Artificial Life", "11": "Industrial Robots", "12": "Legged Robots", "13": "3D Shape Generation", "14": "Deformable Object Manipulation", "15": "Optimal Motion Planning", "16": "Robot Task Planning", "17": "Vision-based navigation with language-based assistance", "18": "Marine Robot Navigation"}},
{"subfield": "Video Games", "tasks": {"0": "Atari Games", "1": "Starcraft II", "2": "Real-Time Strategy Games", "3": "FPS Games", "4": "Dota 2", "5": "Game of Doom", "6": "SNES Games", "7": "League of Legends", "8": "Video Games"}},
{"subfield": "Anomaly Detection", "tasks": {"0": "Anomaly Detection", "1": "Unsupervised Anomaly Detection", "2": "Anomaly Detection In Surveillance Videos", "3": "Abnormal Event Detection In Video", "4": "Group Anomaly Detection", "5": "Anomaly Detection in Edge Streams", "6": "Unsupervised Anomaly Detection In Sound"}},
{"subfield": "Representation Learning", "tasks": {"0": "Representation Learning", "1": "Word Embeddings", "2": "Graph Embedding", "3": "Network Embedding", "4": "Sentence Embeddings", "5": "Unsupervised Representation Learning", "6": "Graph Representation Learning", "7": "Knowledge Graph Embedding", "8": "Sentence Embedding", "9": "Knowledge Graph Embeddings", "10": "Learning Word Embeddings", "11": "Document Embedding", "12": "Multilingual Word Embeddings", "13": "Learning Semantic Representations", "14": "Learning Network Representations", "15": "Sentence Embeddings For Biomedical Texts", "16": "Learning Representation Of Multi-View Data", "17": "Learning Representation On Graph"}},
{"field": "Computer Code", "subfields": {"0": "Activity Recognition", "1": "Dimensionality Reduction", "2": "Feature Selection", "3": "Federated Learning", "4": "Program Synthesis", "5": "Code Generation", "6": "Text-To-Sql", "7": "Program Repair", "8": "Program induction", "9": "Code Summarization", "10": "Type prediction", "11": "Code Search", "12": "Value prediction", "13": "3D Face Animation", "14": "Remote Sensing Image Classification", "15": "Learning to Execute", "16": "Low-rank compression", "17": "SQL-to-Text", "18": "Contextual Embedding for Source Code", "19": "Sql Chatbots", "20": "Write Computer Programs From Specifications"}},
{"subfield": "3D", "tasks": {"0": "3D Reconstruction", "1": "3D Pose Estimation", "2": "3D Shape Reconstruction", "3": "3D Shape Representation", "4": "No-Reference Image Quality Assessment", "5": "3D Object Classification", "6": "3D Shape Classification", "7": "Neural Rendering", "8": "3D FACE MODELING", "9": "3D Shape Generation", "10": "3D Scene Reconstruction", "11": "3D Point Cloud Matching", "12": "FPS Games", "13": "3D Depth Estimation", "14": "Talking Head Generation", "15": "Face Reenactment", "16": "3D Object Retrieval", "17": "3D Shape Recognition", "18": "3D Shape Reconstruction From A Single 2D Image", "19": "3D Volumetric Reconstruction", "20": "Classify 3D Point Clouds", "21": "Generating 3D Point Clouds", "22": "3D Feature Matching", "23": "3D Geometry Perception", "24": "3D Plane Detection", "25": "3D Surface Generation", "26": "Point Set Upsampling", "27": "3D Object Detection From Monocular Images", "28": "Underwater 3D Scene Reconstruction", "29": "Multi-View 3D Shape Retrieval", "30": "3D"}},
{"subfield": "Recommendation Systems", "tasks": {"0": "Remote Sensing Image Classification", "1": "Segmentation Of Remote Sensing Imagery", "2": "The Semantic Segmentation Of Remote Sensing Imagery", "3": "Lake Ice Monitoring", "4": "Change detection for remote sensing images", "5": "Lake Detection", "6": "Building change detection for remote sensing images", "7": "Denoising Of Radar Micro-Doppler Signatures", "8": "Extracting Buildings In Remote Sensing Images", "9": "Remote Sensing"}},
{"subfield": "Representation Learning", "tasks": {"0": "Representation Learning", "1": "Word Embeddings", "2": "Graph Embedding", "3": "Network Embedding", "4": "Sentence Embeddings", "5": "Unsupervised Representation Learning", "6": "Graph Representation Learning", "7": "Knowledge Graph Embedding", "8": "Sentence Embedding", "9": "Knowledge Graph Embeddings", "10": "Learning Word Embeddings", "11": "Document Embedding", "12": "Multilingual Word Embeddings", "13": "Learning Semantic Representations", "14": "Learning Network Representations", "15": "Sentence Embeddings For Biomedical Texts", "16": "Learning Representation Of Multi-View Data", "17": "Learning Representation On Graph"}},
{"subfield": "Reading Comprehension", "tasks": {"0": "Emotion Recognition", "1": "Multimodal Emotion Recognition", "2": "Speech Emotion Recognition", "3": "Emotion Recognition in Conversation", "4": "Emotion-Cause Pair Extraction", "5": "Emotion Cause Extraction", "6": "Video Emotion Recognition", "7": "Emotion Recognition in Context"}},
{"subfield": "Instance Segmentation", "tasks": {"0": "Hand Pose Estimation", "1": "Hand Gesture Recognition", "2": "Hand-Gesture Recognition", "3": "Hand Segmentation", "4": "Gesture-to-Gesture Translation", "5": "Hand Keypoint Localization", "6": "Hand"}},
{"subfield": "Representation Learning", "tasks": {"0": "Representation Learning", "1": "Word Embeddings", "2": "Graph Embedding", "3": "Network Embedding", "4": "Sentence Embeddings", "5": "Unsupervised Representation Learning", "6": "Graph Representation Learning", "7": "Knowledge Graph Embedding", "8": "Sentence Embedding", "9": "Knowledge Graph Embeddings", "10": "Learning Word Embeddings", "11": "Document Embedding", "12": "Multilingual Word Embeddings", "13": "Learning Semantic Representations", "14": "Learning Network Representations", "15": "Sentence Embeddings For Biomedical Texts", "16": "Learning Representation Of Multi-View Data", "17": "Learning Representation On Graph"}},
{"subfield": "Semantic Segmentation", "tasks": {"0": "Semantic Segmentation", "1": "Tumor Segmentation", "2": "Real-Time Semantic Segmentation", "3": "Scene Segmentation", "4": "3D Semantic Segmentation", "5": "Panoptic Segmentation", "6": "Weakly-Supervised Semantic Segmentation", "7": "3D Part Segmentation", "8": "Semi-Supervised Semantic Segmentation", "9": "One-Shot Segmentation", "10": "indoor scene understanding", "11": "Unsupervised Semantic Segmentation", "12": "4D Spatio Temporal Semantic Segmentation", "13": "Room Layout Estimation", "14": "Spinal Cord Gray Matter - Segmentation", "15": "Attentive segmentation networks", "16": "UNET Segmentation", "17": "Histopathological Segmentation", "18": "Road Segementation"}},
{"subfield": "Relation Extraction", "tasks": {"0": "Event Extraction", "1": "Joint Entity and Relation Extraction", "2": "Temporal Information Extraction", "3": "Low Resource Named Entity Recognition", "4": "Definition Extraction", "5": "Drug\u2013drug interaction extraction", "6": "Information Extraction"}},
{"subfield": "Object Recognition", "tasks": {"0": "Remote Sensing Image Classification", "1": "Segmentation Of Remote Sensing Imagery", "2": "The Semantic Segmentation Of Remote Sensing Imagery", "3": "Lake Ice Monitoring", "4": "Change detection for remote sensing images", "5": "Lake Detection", "6": "Building change detection for remote sensing images", "7": "Denoising Of Radar Micro-Doppler Signatures", "8": "Extracting Buildings In Remote Sensing Images", "9": "Remote Sensing"}},
{"subfield": "Scene Parsing", "tasks": {"0": "Medical Diagnosis", "1": "Retinal OCT Disease Classification", "2": "Thoracic Disease Classification", "3": "Blood Cell Count", "4": "CBC TEST", "5": "Predicting Drug-Induced Laboratory Test Effects"}},
{"subfield": "Emotion Recognition", "tasks": {"0": "Emotion Recognition", "1": "Multimodal Emotion Recognition", "2": "Speech Emotion Recognition", "3": "Emotion Recognition in Conversation", "4": "Emotion-Cause Pair Extraction", "5": "Emotion Cause Extraction", "6": "Video Emotion Recognition", "7": "Emotion Recognition in Context"}},
{"subfield": "Time Series", "tasks": {"0": "EEG", "1": "EEG Artifact Removal", "2": "Attention Score Prediction", "3": "Noise Level Prediction", "4": "Semanticity prediction", "5": "LWR Classification", "6": "EEG Denoising"}},
{"subfield": "3D", "tasks": {"0": "3D Reconstruction", "1": "3D Pose Estimation", "2": "3D Shape Reconstruction", "3": "3D Shape Representation", "4": "No-Reference Image Quality Assessment", "5": "3D Object Classification", "6": "3D Shape Classification", "7": "Neural Rendering", "8": "3D FACE MODELING", "9": "3D Shape Generation", "10": "3D Scene Reconstruction", "11": "3D Point Cloud Matching", "12": "FPS Games", "13": "3D Depth Estimation", "14": "Talking Head Generation", "15": "Face Reenactment", "16": "3D Object Retrieval", "17": "3D Shape Recognition", "18": "3D Shape Reconstruction From A Single 2D Image", "19": "3D Volumetric Reconstruction", "20": "Classify 3D Point Clouds", "21": "Generating 3D Point Clouds", "22": "3D Feature Matching", "23": "3D Geometry Perception", "24": "3D Plane Detection", "25": "3D Surface Generation", "26": "Point Set Upsampling", "27": "3D Object Detection From Monocular Images", "28": "Underwater 3D Scene Reconstruction", "29": "Multi-View 3D Shape Retrieval", "30": "3D"}},
{"subfield": "Speech Recognition", "tasks": {"0": "Speech Recognition", "1": "Visual Speech Recognition", "2": "Distant Speech Recognition", "3": "Sequence-To-Sequence Speech Recognition", "4": "Robust Speech Recognition", "5": "Noisy Speech Recognition", "6": "Accented Speech Recognition", "7": "English Conversational Speech Recognition"}},
{"subfield": "Gesture Recognition", "tasks": {"0": "Medical Diagnosis", "1": "Retinal OCT Disease Classification", "2": "Thoracic Disease Classification", "3": "Blood Cell Count", "4": "CBC TEST", "5": "Predicting Drug-Induced Laboratory Test Effects"}},
{"subfield": "3D", "tasks": {"0": "Unsupervised Image-To-Image Translation", "1": "Synthetic-to-Real Translation", "2": "Multimodal Unsupervised Image-To-Image Translation", "3": "Cross-View Image-to-Image Translation", "4": "Fundus to Angiography Generation", "5": "Photo-To-Caricature Translation", "6": "Bird View Synthesis", "7": "Real-to-Cartoon translation", "8": "Facial Makeup Transfer", "9": "Cartoon-To-Real Translation"}},
{"task": "Remote Sensing", "papers": {}},
{"subfield": "Medical Image Segmentation", "tasks": {"0": "Optical Character Recognition", "1": "Handwriting Recognition", "2": "Handwritten Digit Recognition", "3": "Irregular Text Recognition", "4": "Offline Handwritten Chinese Character Recognition", "5": "Word Spotting In Handwritten Documents", "6": "Handwritten Digit Image Synthesis", "7": "Handwritten Chinese Text Recognition"}},
{"subfield": "Quantization", "tasks": {"0": "Object Localization", "1": "Weakly-Supervised Object Localization", "2": "Image-Based Localization", "3": "Monocular 3D Object Localization", "4": "Active Object Localization", "5": "Unsupervised Object Localization"}},
{"task": "Emotion Recognition in Context", "papers": {}},
{"task": "Extracting Buildings In Remote Sensing Images", "papers": {}},
{"task": "Information Extraction", "papers": {}},
{"task": "Hand", "papers": {}},
{"task": "Drug\u2013drug interaction extraction", "papers": {"0": "Drug\u2013drug interaction extraction via hierarchical RNNs on sequence and shortest dependency paths"}},
{"task": "Emotion Cause Extraction", "papers": {"0": "RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction"}},
{"task": "Video Emotion Recognition", "papers": {}},
{"task": "Definition Extraction", "papers": {}},
{"task": "Low Resource Named Entity Recognition", "papers": {"0": "Zero-Resource Cross-Lingual Named Entity Recognition", "1": "Zero-Resource Cross-Lingual Named Entity Recognition", "2": "Zero-Resource Cross-Lingual Named Entity Recognition"}},
{"task": "EEG Denoising", "papers": {}},
{"task": "Temporal Information Extraction", "papers": {"0": "CATENA: CAusal and TEmporal relation extraction from NAtural language texts", "1": "A Structured Learning Approach to Temporal Relation Extraction"}},
{"task": "English Conversational Speech Recognition", "papers": {}},
{"task": "LWR Classification", "papers": {"0": "PhyAAt: Physiology of Auditory Attention to Speech Dataset"}},
{"task": "Accented Speech Recognition", "papers": {"0": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "1": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "2": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "3": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin"}},
{"task": "Handwritten Chinese Text Recognition", "papers": {}},
{"task": "Active Object Localization", "papers": {}},
{"task": "Unsupervised Object Localization", "papers": {}},
{"task": "Cartoon-To-Real Translation", "papers": {"0": "0.5 Petabyte Simulation of a 45-Qubit Quantum Circuit"}},
{"task": "Monocular 3D Object Localization", "papers": {}},
{"task": "Image-Based Localization", "papers": {"0": "University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization"}},
{"task": "Object Localization", "papers": {"0": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection", "1": "Frustum PointNets for 3D Object Detection from RGB-D Data", "2": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection", "3": "Frustum PointNets for 3D Object Detection from RGB-D Data", "4": "Frustum PointNets for 3D Object Detection from RGB-D Data", "5": "Frustum PointNets for 3D Object Detection from RGB-D Data", "6": "Frustum PointNets for 3D Object Detection from RGB-D Data", "7": "Frustum PointNets for 3D Object Detection from RGB-D Data", "8": "Frustum PointNets for 3D Object Detection from RGB-D Data", "9": "Locating Objects Without Bounding Boxes", "10": "Locating Objects Without Bounding Boxes", "11": "Locating Objects Without Bounding Boxes", "12": "Co-localization with Category-Consistent Features and Geodesic Distance Propagation", "13": "Co-localization with Category-Consistent Features and Geodesic Distance Propagation"}},
{"task": "Weakly-Supervised Object Localization", "papers": {"0": "Attributional Robustness Training using Input-Gradient Spatial Alignment", "1": "Self-produced Guidance for Weakly-supervised Object Localization", "2": "Rethinking Softmax with Cross-Entropy: Neural Network Classifier as Mutual Information Estimator", "3": "Self-produced Guidance for Weakly-supervised Object Localization"}},
{"task": "Word Spotting In Handwritten Documents", "papers": {}},
{"paper": "RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction", "abstract": "The emotion cause extraction (ECE) task aims at discovering the potential causes behind a certain emotion expression in a document. Techniques including rule-based methods, traditional machine learning methods and deep neural networks have been proposed to solve this task.However, most of the previous work considered ECE as a set of independent clause classification problems and ignored the relations between multiple clauses in a document. In this work, we propose a joint emotion cause extraction framework, named RNN-Transformer Hierarchical Network (RTHN), to encode and classify multiple clauses synchronously. RTHN is composed of a lower word-level encoder based on RNNs to encode multiple words in each clause, and an upper clause-level encoder based on Transformer to learn the correlation between multiple clauses in a document. We furthermore propose ways to encode the relative position and global predication information into Transformer that can capture the causality between clauses and make RTHN more efficient. We finally achieve the best performance among 12 compared systems and improve the F1 score of the state-of-the-art from 72.69\\% to 76.77\\%."},
{"task": "Handwritten Digit Image Synthesis", "papers": {}},
{"paper": "A Structured Learning Approach to Temporal Relation Extraction", "abstract": "Identifying temporal relations between events is an essential step towards natural language understanding. However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events.Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem."},
{"paper": "CATENA: CAusal and TEmporal relation extraction from NAtural language texts", "abstract": "We present CATENA, a sieve-based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model. We evaluate the performance of each sieve, showing that the rule-based, the machine-learned and the reasoning components all contribute to achieving state-of-the-art performance on TempEval-3 and TimeBank-Dense data.Although causal relations are much sparser than temporal ones, the architecture and the selected features are mostly suitable to serve both tasks. The effects of the interaction between the temporal and the causal components, although limited, yield promising results and confirm the tight connection between the temporal and the causal dimension of texts."},
{"paper": "Drug\u2013drug interaction extraction via hierarchical RNNs on sequence and shortest dependency paths", "abstract": "Motivation\r\nAdverse events resulting from drug-drug interactions (DDI) pose a serious health issue. The ability to automatically extract DDIs described in the biomedical literature could further efforts for ongoing pharmacovigilance.Most of neural networks-based methods typically focus on sentence sequence to identify these DDIs, however the shortest dependency path (SDP) between the two entities contains valuable syntactic and semantic information. Effectively exploiting such information may improve DDI extraction. Results\r\nIn this article, we present a hierarchical recurrent neural networks (RNNs)-based method to integrate the SDP and sentence sequence for DDI extraction task. Firstly, the sentence sequence is divided into three subsequences. Then, the bottom RNNs model is employed to learn the feature representation of the subsequences and SDP, and the top RNNs model is employed to learn the feature representation of both sentence sequence and SDP. Furthermore, we introduce the embedding attention mechanism to identify and enhance keywords for the DDI extraction task. We evaluate our approach using the DDI extraction 2013 corpus. Our method is competitive or superior in performance as compared with other state-of-the-art methods. Experimental results show that the sentence sequence and SDP are complementary to each other. Integrating the sentence sequence with SDP can effectively improve the DDI extraction performance."},
{"paper": "Zero-Resource Cross-Lingual Named Entity Recognition", "abstract": "Recently, neural methods have achieved state-of-the-art (SOTA) results in Named Entity Recognition (NER) tasks for many languages without the need for manually crafted features. However, these models still require manually annotated training data, which is not available for many languages.In this paper, we propose an unsupervised cross-lingual NER model that can transfer NER knowledge from one language to another in a completely unsupervised way without relying on any bilingual dictionary or parallel data. Our model achieves this through word-level adversarial learning and augmented fine-tuning with parameter sharing and feature augmentation. Experiments on five different languages demonstrate the effectiveness of our approach, outperforming existing models by a good margin and setting a new SOTA for each language pair."},
{"task": "Offline Handwritten Chinese Character Recognition", "papers": {}},
{"paper": "PhyAAt: Physiology of Auditory Attention to Speech Dataset", "abstract": "Auditory attention to natural speech is a complex brain process. Its quantification from physiological signals can be valuable to improving and widening the range of applications of current brain-computer-interface systems, however it remains a challenging task.In this article, we present a dataset of physiological signals collected from an experiment on auditory attention to natural speech. In this experiment, auditory stimuli consisting of reproductions of English sentences in different auditory conditions were presented to 25 non-native participants, who were asked to transcribe the sentences. During the experiment, 14 channel electroencephalogram, galvanic skin response, and photoplethysmogram signals were collected from each participant. Based on the number of correctly transcribed words, an attention score was obtained for each auditory stimulus presented to subjects. A strong correlation ($p<<0.0001$) between the attention score and the auditory conditions was found. We also formulate four different predictive tasks involving the collected dataset and develop a feature extraction framework. The results for each predictive task are obtained using a Support Vector Machine with spectral features, and are better than chance level. The dataset has been made publicly available for further research, along with a python library - phyaat to facilitate the preprocessing, modeling, and reproduction of the results presented in this paper. The dataset and other resources are shared on webpage - https://phyaat.github.io."},
{"paper": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "abstract": "We show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural\nnetworks, end-to-end learning allows us to handle a diverse variety of speech\nincluding noisy environments, accents and different languages.Key to our\napproach is our application of HPC techniques, resulting in a 7x speedup over\nour previous system. Because of this efficiency, experiments that previously\ntook weeks now run in days. This enables us to iterate more quickly to identify\nsuperior architectures and algorithms. As a result, in several cases, our\nsystem is competitive with the transcription of human workers when benchmarked\non standard datasets. Finally, using a technique called Batch Dispatch with\nGPUs in the data center, we show that our system can be inexpensively deployed\nin an online setting, delivering low latency when serving users at scale."},
{"task": "Irregular Text Recognition", "papers": {}},
{"task": "Optical Character Recognition", "papers": {"0": "Attention-based Extraction of Structured Information from Street View Imagery"}},
{"task": "Handwritten Digit Recognition", "papers": {"0": "Effective Handwritten Digit Recognition using Deep Convolution Neural Network"}},
{"task": "Real-to-Cartoon translation", "papers": {}},
{"paper": "Rethinking Softmax with Cross-Entropy: Neural Network Classifier as Mutual Information Estimator", "abstract": "Mutual information is widely applied to learn latent representations of observations, whilst its implication in classification neural networks remain to be better explained. We show that optimising the parameters of classification neural networks with softmax cross-entropy is equivalent to maximising the mutual information between inputs and labels under the balanced data assumption.Through experiments on synthetic and real datasets, we show that softmax cross-entropy can estimate mutual information approximately. When applied to image classification, this relation helps approximate the point-wise mutual information between an input image and a label without modifying the network structure. To this end, we propose infoCAM, informative class activation map, which highlights regions of the input image that are the most relevant to a given label based on differences in information. The activation map helps localise the target object in an input image. Through experiments on the semi-supervised object localisation task with two real-world datasets, we evaluate the effectiveness of our information-theoretic approach."},
{"task": "Facial Makeup Transfer", "papers": {}},
{"task": "Handwriting Recognition", "papers": {"0": "AKHCRNet: Bengali Handwritten Character Recognition Using Deep Learning"}},
{"paper": "Co-localization with Category-Consistent Features and Geodesic Distance Propagation", "abstract": "Co-localization is the problem of localizing objects of the same class using only the set of images that contain them. This is a challenging task because the object detector must be built without negative examples that can lead to more informative supervision signals.The main idea of our method is to cluster the feature space of a generically pre-trained CNN, to find a set of CNN features that are consistently and highly activated for an object category, which we call category-consistent CNN features. Then, we propagate their combined activation map using superpixel geodesic distances for co-localization. In our first set of experiments, we show that the proposed method achieves state-of-the-art performance on three related benchmarks: PASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that our method is able to detect and localize truly unseen categories, on six held-out ImageNet categories with accuracy that is significantly higher than previous state-of-the-art. Our intuitive approach achieves this success without any region proposals or object detectors and can be based on a CNN that was pre-trained purely on image classification tasks without further fine-tuning."},
{"paper": "Attributional Robustness Training using Input-Gradient Spatial Alignment", "abstract": "Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust.Recently, it has been shown that the explanations could be manipulated easily by adding visually imperceptible perturbations to the input while keeping the model's prediction intact. In this work, we study the problem of attributional robustness (i.e. models having robust explanations) by showing an upper bound for attributional vulnerability in terms of spatial correlation between the input image and its explanation map. We propose a training methodology that learns robust features by minimizing this upper bound using soft-margin triplet loss. Our methodology of robust attribution training (\\textit{ART}) achieves the new state-of-the-art attributional robustness measure by a margin of $\\approx$ 6-18 $\\%$ on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust training technique (\\textit{ART}) in the downstream task of weakly supervised object localization by achieving the new state-of-the-art performance on CUB-200 dataset."},
{"paper": "Self-produced Guidance for Weakly-supervised Object Localization", "abstract": "Weakly supervised methods usually generate localization results based on\nattention maps produced by classification networks. However, the attention maps\nexhibit the most discriminative parts of the object which are small and sparse.We propose to generate Self-produced Guidance (SPG) masks which separate the\nforeground, the object of interest, from the background to provide the\nclassification networks with spatial correlation information of pixels. A\nstagewise approach is proposed to incorporate high confident object regions to\nlearn the SPG masks. The high confident regions within attention maps are\nutilized to progressively learn the SPG masks. The masks are then used as an\nauxiliary pixel-level supervision to facilitate the training of classification\nnetworks. Extensive experiments on ILSVRC demonstrate that SPG is effective in\nproducing high-quality object localizations maps. Particularly, the proposed\nSPG achieves the Top-1 localization error rate of 43.83% on the ILSVRC\nvalidation set, which is a new state-of-the-art error rate."},
{"paper": "0.5 Petabyte Simulation of a 45-Qubit Quantum Circuit", "abstract": "Near-term quantum computers will soon reach sizes that are challenging to directly simulate, even when employing the most powerful supercomputers. Yet, the ability to simulate these early devices using classical computers is crucial for calibration, validation, and benchmarking.In order to make use of the full potential of systems featuring multi- and many-core processors, we use automatic code generation and optimization of compute kernels, which also enables performance portability. We apply a scheduling algorithm to quantum supremacy circuits in order to reduce the required communication and simulate a 45-qubit circuit on the Cori II supercomputer using 8,192 nodes and 0.5 petabytes of memory. To our knowledge, this constitutes the largest quantum circuit simulation to this date. Our highly-tuned kernels in combination with the reduced communication requirements allow an improvement in time-to-solution over state-of-the-art simulations by more than an order of magnitude at every scale."},
{"paper": "Locating Objects Without Bounding Boxes", "abstract": "Recent advances in convolutional neural networks (CNN) have achieved\nremarkable results in locating objects in images. In these networks, the\ntraining procedure usually requires providing bounding boxes or the maximum\nnumber of expected objects.In this paper, we address the task of estimating\nobject locations without annotated bounding boxes which are typically\nhand-drawn and time consuming to label. We propose a loss function that can be\nused in any fully convolutional network (FCN) to estimate object locations. This loss function is a modification of the average Hausdorff distance between\ntwo unordered sets of points. The proposed method has no notion of bounding\nboxes, region proposals, or sliding windows. We evaluate our method with three\ndatasets designed to locate people's heads, pupil centers and plant centers. We\noutperform state-of-the-art generic object detectors and methods fine-tuned for\npupil tracking."},
{"paper": "University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization", "abstract": "We consider the problem of cross-view geo-localization. The primary challenge of this task is to learn the robust feature against large viewpoint changes.Existing benchmarks can help, but are limited in the number of viewpoints. Image pairs, containing two viewpoints, e.g., satellite and ground, are usually provided, which may compromise the feature learning. Besides phone cameras and satellites, in this paper, we argue that drones could serve as the third platform to deal with the geo-localization problem. In contrast to the traditional ground-view images, drone-view images meet fewer obstacles, e.g., trees, and could provide a comprehensive view when flying around the target place. To verify the effectiveness of the drone platform, we introduce a new multi-view multi-source benchmark for drone-based geo-localization, named University-1652. University-1652 contains data from three platforms, i.e., synthetic drones, satellites and ground cameras of 1,652 university buildings around the world. To our knowledge, University-1652 is the first drone-based geo-localization dataset and enables two new tasks, i.e., drone-view target localization and drone navigation. As the name implies, drone-view target localization intends to predict the location of the target place via drone-view images. On the other hand, given a satellite-view query image, drone navigation is to drive the drone to the area of interest in the query. We use this dataset to analyze a variety of off-the-shelf CNN features and propose a strong CNN baseline on this challenging dataset. The experiments show that University-1652 helps the model to learn the viewpoint-invariant features and also has good generalization ability in the real-world scenario."},
{"paper": "Attention-based Extraction of Structured Information from Street View Imagery", "abstract": "We present a neural network model - based on CNNs, RNNs and a novel attention\nmechanism - which achieves 84.2% accuracy on the challenging French Street Name\nSigns (FSNS) dataset, significantly outperforming the previous state of the art\n(Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler\nand more general than the previous approach.To demonstrate the generality of\nour model, we show that it also performs well on an even more challenging\ndataset derived from Google Street View, in which the goal is to extract\nbusiness names from store fronts. Finally, we study the speed/accuracy tradeoff\nthat results from using CNN feature extractors of different depths. Surprisingly, we find that deeper is not always better (in terms of accuracy,\nas well as speed). Our resulting model is simple, accurate and fast, allowing\nit to be used at scale on a variety of challenging real-world text extraction\nproblems."},
{"paper": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection", "abstract": "Accurate detection of objects in 3D point clouds is a central problem in many\napplications, such as autonomous navigation, housekeeping robots, and\naugmented/virtual reality. To interface a highly sparse LiDAR point cloud with\na region proposal network (RPN), most existing efforts have focused on\nhand-crafted feature representations, for example, a bird's eye view\nprojection.In this work, we remove the need of manual feature engineering for\n3D point clouds and propose VoxelNet, a generic 3D detection network that\nunifies feature extraction and bounding box prediction into a single stage,\nend-to-end trainable deep network. Specifically, VoxelNet divides a point cloud\ninto equally spaced 3D voxels and transforms a group of points within each\nvoxel into a unified feature representation through the newly introduced voxel\nfeature encoding (VFE) layer. In this way, the point cloud is encoded as a\ndescriptive volumetric representation, which is then connected to a RPN to\ngenerate detections. Experiments on the KITTI car detection benchmark show that\nVoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a\nlarge margin. Furthermore, our network learns an effective discriminative\nrepresentation of objects with various geometries, leading to encouraging\nresults in 3D detection of pedestrians and cyclists, based on only LiDAR."},
{"task": "Photo-To-Caricature Translation", "papers": {}},
{"task": "Fundus to Angiography Generation", "papers": {"0": "Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks"}},
{"task": "Bird View Synthesis", "papers": {}},
{"paper": "Frustum PointNets for 3D Object Detection from RGB-D Data", "abstract": "In this work, we study 3D object detection from RGB-D data in both indoor and\noutdoor scenes. While previous methods focus on images or 3D voxels, often\nobscuring natural 3D patterns and invariances of 3D data, we directly operate\non raw point clouds by popping up RGB-D scans.However, a key challenge of this\napproach is how to efficiently localize objects in point clouds of large-scale\nscenes (region proposal). Instead of solely relying on 3D proposals, our method\nleverages both mature 2D object detectors and advanced 3D deep learning for\nobject localization, achieving efficiency as well as high recall for even small\nobjects. Benefited from learning directly in raw point clouds, our method is\nalso able to precisely estimate 3D bounding boxes even under strong occlusion\nor with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection\nbenchmarks, our method outperforms the state of the art by remarkable margins\nwhile having real-time capability."},
{"task": "Cross-View Image-to-Image Translation", "papers": {"0": "Unified Generative Adversarial Networks for Controllable Image-to-Image Translation", "1": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation", "2": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation", "3": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation", "4": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation", "5": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation", "6": "A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View"}},
{"task": "Noisy Speech Recognition", "papers": {"0": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin"}},
{"task": "Robust Speech Recognition", "papers": {}},
{"paper": "Effective Handwritten Digit Recognition using Deep Convolution Neural Network", "abstract": "This  paper  proposed  a  simple  neural  network  approach towards  handwritten  digit  recognition  using  convolution. With  machine  learning  algorithms  like  KNN, SVM/SOM, recognizing digits is considered as one of the unsolvable tasks due to its distinctiveness in the style of writing.In this paper, Convolution  Neural  Networks  are  implemented  with an MNIST  dataset  of  70000  digits  with  250  distinct forms of writings. The proposed method achieved 98.51% accuracy for real-world handwritten digit prediction with less than 0.1 % loss on training with 60000 digits while 10000 under validation."},
{"task": "Multimodal Unsupervised Image-To-Image Translation", "papers": {"0": "Multimodal Unsupervised Image-to-Image Translation", "1": "Multimodal Unsupervised Image-to-Image Translation", "2": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "3": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "4": "Multimodal Unsupervised Image-to-Image Translation", "5": "In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks"}},
{"task": "Synthetic-to-Real Translation", "papers": {"0": "Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation", "1": "Discriminative Adversarial Domain Adaptation"}},
{"task": "Semanticity prediction", "papers": {"0": "PhyAAt: Physiology of Auditory Attention to Speech Dataset"}},
{"paper": "AKHCRNet: Bengali Handwritten Character Recognition Using Deep Learning", "abstract": "I propose a state of the art deep neural architectural solution for handwritten character recognition for Bengali alphabets, compound characters as well as numerical digits that achieves state-of-the-art accuracy 96.8% in just 11 epochs. Similar work has been done before by Chatterjee, Swagato, et al. but they achieved 96.12% accuracy in about 47 epochs.The deep neural architecture used in that paper was fairly large considering the inclusion of the weights of the ResNet 50 model which is a 50 layer Residual Network. This proposed model achieves higher accuracy as compared to any previous work & in a little number of epochs. ResNet50 is a good model trained on the ImageNet dataset, but I propose an HCR network that is trained from the scratch on Bengali characters without the \"Ensemble Learning\" that can outperform previous architectures."},
{"task": "Sequence-To-Sequence Speech Recognition", "papers": {}},
{"task": "Unsupervised Image-To-Image Translation", "papers": {"0": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "1": "In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks"}},
{"task": "Speech Recognition", "papers": {"0": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition", "1": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition", "2": "English Conversational Telephone Speech Recognition by Humans and Machines", "3": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "4": "English Conversational Telephone Speech Recognition by Humans and Machines", "5": "Purely sequence-trained neural networks for ASR based on lattice-free MMI", "6": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "7": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "8": "Jasper: An End-to-End Convolutional Neural Acoustic Model", "9": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "10": "End-to-end speech recognition using lattice-free MMI", "11": "Espresso: A Fast End-to-end Neural Speech Recognition Toolkit", "12": "Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription"}},
{"task": "Distant Speech Recognition", "papers": {"0": "The PyTorch-Kaldi Speech Recognition Toolkit"}},
{"task": "Visual Speech Recognition", "papers": {}},
{"task": "Noise Level Prediction", "papers": {"0": "PhyAAt: Physiology of Auditory Attention to Speech Dataset"}},
{"task": "Attention Score Prediction", "papers": {"0": "PhyAAt: Physiology of Auditory Attention to Speech Dataset"}},
{"task": "EEG Artifact Removal", "papers": {"0": "Intracerebral EEG Artifact Identification Using Convolutional Neural Networks"}},
{"paper": "Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks", "abstract": "Fluorescein Angiography (FA) is a technique that employs the designated camera for Fundus photography incorporating excitation and barrier filters. FA also requires fluorescein dye that is injected intravenously, which might cause adverse effects ranging from nausea, vomiting to even fatal anaphylaxis.Currently, no other fast and non-invasive technique exists that can generate FA without coupling with Fundus photography. To eradicate the need for an invasive FA extraction procedure, we introduce an Attention-based Generative network that can synthesize Fluorescein Angiography from Fundus images. The proposed gan incorporates multiple attention based skip connections in generators and comprises novel residual blocks for both generators and discriminators. It utilizes reconstruction, feature-matching, and perceptual loss along with adversarial training to produces realistic Angiograms that is hard for experts to distinguish from real ones. Our experiments confirm that the proposed architecture surpasses recent state-of-the-art generative networks for fundus-to-angio translation task."},
{"task": "Predicting Drug-Induced Laboratory Test Effects", "papers": {}},
{"paper": "A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View", "abstract": "Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge.Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360{\\deg} BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV"},
{"task": "EEG", "papers": {"0": "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition", "1": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks"}},
{"task": "CBC TEST", "papers": {}},
{"paper": "In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks", "abstract": "In unsupervised image-to-image translation, the goal is to learn the mapping\nbetween an input image and an output image using a set of unpaired training\nimages. In this paper, we propose an extension of the unsupervised\nimage-to-image translation problem to multiple input setting.Given a set of\npaired images from multiple modalities, a transformation is learned to\ntranslate the input into a specified domain. For this purpose, we introduce a\nGenerative Adversarial Network (GAN) based framework along with a multi-modal\ngenerator structure and a new loss term, latent consistency loss. Through\nvarious experiments we show that leveraging multiple inputs generally improves\nthe visual quality of the translated images. Moreover, we show that the\nproposed method outperforms current state-of-the-art unsupervised\nimage-to-image translation methods."},
{"paper": "Discriminative Adversarial Domain Adaptation", "abstract": "Given labeled instances on a source domain and unlabeled ones on a target domain, unsupervised domain adaptation aims to learn a task classifier that can well classify target instances. Recent advances rely on domain-adversarial training of deep networks to learn domain-invariant features.However, due to an issue of mode collapse induced by the separate design of task and domain classifiers, these methods are limited in aligning the joint distributions of feature and category across domains. To overcome it, we propose a novel adversarial learning method termed Discriminative Adversarial Domain Adaptation (DADA). Based on an integrated category and domain classifier, DADA has a novel adversarial objective that encourages a mutually inhibitory relation between category and domain predictions for any input instance. We show that under practical conditions, it defines a minimax game that can promote the joint distribution alignment. Except for the traditional closed set domain adaptation, we also extend DADA for extremely challenging problem settings of partial and open set domain adaptation. Experiments show the efficacy of our proposed methods and we achieve the new state of the art for all the three settings on benchmark datasets."},
{"paper": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation", "abstract": "Cross-view image translation is challenging because it involves images with\ndrastically different views and severe deformation. In this paper, we propose a\nnovel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that\nmakes it possible to generate images of natural scenes in arbitrary viewpoints,\nbased on an image of the scene and a novel semantic map.The proposed\nSelectionGAN explicitly utilizes the semantic information and consists of two\nstages. In the first stage, the condition image and the target semantic map are\nfed into a cycled semantic-guided generation network to produce initial coarse\nresults. In the second stage, we refine the initial results by using a\nmulti-channel attention selection mechanism. Moreover, uncertainty maps\nautomatically learned from attentions are used to guide the pixel loss for\nbetter network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top\ndatasets show that our model is able to generate significantly better results\nthan the state-of-the-art methods. The source code, data and trained models are\navailable at https://github.com/Ha0Tang/SelectionGAN."},
{"paper": "Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation", "abstract": "Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples.However, the different data distributions in the two domains, or \\emph{domain shift/discrepancy}, inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains, the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper, we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation, which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First, the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then, we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart, respectively. Finally, we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5$\\rightarrow $Cityscapes and SYNTHIA$\\rightarrow $Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at \\url{https://github.com/RogerZhangzz/CAG_UDA}."},
{"paper": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains.We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2."},
{"paper": "Multimodal Unsupervised Image-to-Image Translation", "abstract": "Unsupervised image-to-image translation is an important and challenging\nproblem in computer vision. Given an image in the source domain, the goal is to\nlearn the conditional distribution of corresponding images in the target\ndomain, without seeing any pairs of corresponding images.While this\nconditional distribution is inherently multimodal, existing approaches make an\noverly simplified assumption, modeling it as a deterministic one-to-one\nmapping. As a result, they fail to generate diverse outputs from a given source\ndomain image. To address this limitation, we propose a Multimodal Unsupervised\nImage-to-image Translation (MUNIT) framework. We assume that the image\nrepresentation can be decomposed into a content code that is domain-invariant,\nand a style code that captures domain-specific properties. To translate an\nimage to another domain, we recombine its content code with a random style code\nsampled from the style space of the target domain. We analyze the proposed\nframework and establish several theoretical results. Extensive experiments with\ncomparisons to the state-of-the-art approaches further demonstrates the\nadvantage of the proposed framework. Moreover, our framework allows users to\ncontrol the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT"},
{"task": "Blood Cell Count", "papers": {}},
{"paper": "Unified Generative Adversarial Networks for Controllable Image-to-Image Translation", "abstract": "We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps.The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fr\\'echet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN."},
{"paper": "End-to-end speech recognition using lattice-free MMI", "abstract": "We present our work on end-to-end training of acoustic models\r\nusing the lattice-free maximum mutual information (LF-MMI)\r\nobjective function in the context of hidden Markov models. By end-to-end training, we mean flat-start training of a single\r\nDNN in one stage without using any previously trained models,\r\nforced alignments, or building state-tying decision trees.We\r\nuse full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can\r\nachieve comparable results to regular LF-MMI on well-known\r\nlarge vocabulary tasks. We also compare with other end-to-end\r\nmethods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models."},
{"paper": "Purely sequence-trained neural networks for ASR based on lattice-free MMI", "abstract": "In this paper we describe a method to perform sequence-discriminative training of neural network acoustic models without the need for frame-level cross-entropy pre-training. We use the lattice-free version of the maximum mutual information\r\n(MMI) criterion: LF-MMI.To make its computation feasible we use a phone n-gram language model, in place of the word language model. To further reduce its space and time complexity we compute the objective function using neural network outputs at one third the standard frame rate. These changes enable us to perform the computation for the forward-backward algorithm on GPUs. Further the reduced output frame-rate also provides a significant speed-up during decoding. We present results on 5 different LVCSR tasks with training data ranging from 100 to 2100 hours. Models trained with LFMMI provide a relative word error rate reduction of \u223c11.5%, over those trained with cross-entropy objective function, and \u223c8%, over those trained with cross-entropy and sMBR objective functions. A further reduction of \u223c2.5%, relative, can be obtained by fine tuning these models with the word-lattice based sMBR objective function."},
{"paper": "The PyTorch-Kaldi Speech Recognition Toolkit", "abstract": "The availability of open-source software is playing a remarkable role in the\npopularization of speech recognition and deep learning. Kaldi, for instance, is\nnowadays an established framework used to develop state-of-the-art speech\nrecognizers.PyTorch is used to build neural networks with the Python language\nand has recently spawn tremendous interest within the machine learning\ncommunity thanks to its simplicity and flexibility. The PyTorch-Kaldi project aims to bridge the gap between these popular\ntoolkits, trying to inherit the efficiency of Kaldi and the flexibility of\nPyTorch. PyTorch-Kaldi is not only a simple interface between these software,\nbut it embeds several useful features for developing modern speech recognizers. For instance, the code is specifically designed to naturally plug-in\nuser-defined acoustic models. As an alternative, users can exploit several\npre-implemented neural networks that can be customized using intuitive\nconfiguration files. PyTorch-Kaldi supports multiple feature and label streams\nas well as combinations of neural networks, enabling the use of complex neural\narchitectures. The toolkit is publicly-released along with a rich documentation\nand is designed to properly work locally or on HPC clusters. Experiments, that are conducted on several datasets and tasks, show that\nPyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech\nrecognizers."},
{"paper": "Intracerebral EEG Artifact Identification Using Convolutional Neural Networks", "abstract": "Manual and semi-automatic identification of artifacts and unwanted physiological signals in large intracerebral electroencephalographic (iEEG) recordings is time consuming and inaccurate. To date, unsupervised methods to accurately detect iEEG artifacts are not available.This study introduces a novel machine-learning approach for detection of artifacts in iEEG signals in clinically controlled conditions using convolutional neural networks (CNN) and benchmarks the method\u2019s performance against expert annotations. The method was trained and tested on data obtained from St Anne\u2019s University Hospital (Brno, Czech Republic) and validated on data from Mayo Clinic (Rochester, Minnesota, U.S.A). We show that the proposed technique can be used as a generalized model for iEEG artifact detection. Moreover, a transfer learning process might be used for retraining of the generalized version to form a data-specific model. The generalized model can be efficiently retrained for use with different EEG acquisition systems and noise environments. The generalized and specialized model F1 scores on the testing dataset were 0.81 and 0.96, respectively. The CNN model provides faster, more objective, and more reproducible iEEG artifact detection compared to manual approaches."},
{"paper": "Jasper: An End-to-End Convolutional Neural Acoustic Model", "abstract": "In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections.To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets."},
{"paper": "Espresso: A Fast End-to-end Neural Speech Recognition Toolkit", "abstract": "We present Espresso, an open-source, modular, extensible end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. Espresso supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented.Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard data sets among other end-to-end systems without data augmentation, and is 4--11x faster for decoding than similar systems (e.g. ESPnet)."},
{"paper": "Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription", "abstract": "While end-to-end ASR systems have proven competitive with the conventional hybrid approach, they are prone to accuracy degradation when it comes to noisy and low-resource conditions. In this paper, we argue that, even in such difficult cases, some end-to-end approaches show performance close to the hybrid baseline.To demonstrate this, we use the CHiME-6 Challenge data as an example of challenging environments and noisy conditions of everyday speech. We experimentally compare and analyze CTC-Attention versus RNN-Transducer approaches along with RNN versus Transformer architectures. We also provide a comparison of acoustic features and speech enhancements. Besides, we evaluate the effectiveness of neural network language models for hypothesis re-scoring in low-resource conditions. Our best end-to-end model based on RNN-Transducer, together with improved beam search, reaches quality by only 3.8% WER abs. worse than the LF-MMI TDNN-F CHiME-6 Challenge baseline. With the Guided Source Separation based training data augmentation, this approach outperforms the hybrid baseline system by 2.7% WER abs. and the end-to-end system best known before by 25.7% WER abs."},
{"paper": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks", "abstract": "To investigate critical frequency bands and channels, this paper introduces deep belief networks (DBNs) to constructing EEG-based emotion recognition models for three emotions: positive, neutral and negative. We develop an EEG dataset acquired from 15 subjects.Each subject performs the experiments twice at the interval of a few days. DBNs are trained with differential entropy features extracted from multichannel EEG data. We examine the weights of the trained DBNs and investigate the critical frequency bands and channels. Four different profiles of 4, 6, 9, and 12 channels are selected. The recognition accuracies of these four profiles are relatively stable with the best accuracy of 86.65%, which is even better than that of the original 62 channels. The critical frequency bands and channels determined by using the weights of trained DBNs are consistent with the existing observations. In addition, our experiment results show that neural signatures associated with different emotions do exist and they share commonality across sessions and individuals. We compare the performance of deep models with shallow models. The average accuracies of DBN, SVM, LR, and KNN are 86.08%, 83.99%, 82.70%, and 72.60%, respectively."},
{"paper": "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition", "abstract": "The neuroscience study has revealed the discrepancy of emotion expression between left and right hemispheres of human brain. Inspired by this study, in this paper, we propose a novel bi-hemispheric discrepancy model (BiHDM) to learn the asymmetric differences between two hemispheres for electroencephalograph (EEG) emotion recognition.Concretely, we first employ four directed recurrent neural networks (RNNs) based on two spatial orientations to traverse electrode signals on two separate brain regions, which enables the model to obtain the deep representations of all the EEG electrodes' signals while keeping the intrinsic spatial dependence. Then we design a pairwise subnetwork to capture the discrepancy information between two hemispheres and extract higher-level features for final classification. Besides, in order to reduce the domain shift between training and testing data, we use a domain discriminator that adversarially induces the overall feature learning module to generate emotion-related but domain-invariant feature, which can further promote EEG emotion recognition. We conduct experiments on three public EEG emotional datasets, and the experiments show that the new state-of-the-art results can be achieved."},
{"paper": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition", "abstract": "We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training.By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%."},
{"task": "Thoracic Disease Classification", "papers": {}},
{"paper": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain\ninvariant representations, but are difficult to visualize and sometimes fail to\ncapture pixel-level and low-level domain shifts.Recent work has shown that\ngenerative adversarial networks combined with cycle-consistency constraints are\nsurprisingly effective at mapping images between domains, even without the use\nof aligned image pairs. We propose a novel discriminatively-trained\nCycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts\nrepresentations at both the pixel-level and feature-level, enforces\ncycle-consistency while leveraging a task loss, and does not require aligned\npairs. Our model can be applied in a variety of visual recognition and\nprediction settings. We show new state-of-the-art results across multiple\nadaptation tasks, including digit classification and semantic segmentation of\nroad scenes demonstrating transfer from synthetic to real world domains."},
{"paper": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned.Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data."},
{"paper": "English Conversational Telephone Speech Recognition by Humans and Machines", "abstract": "One of the most difficult speech recognition tasks is accurate recognition of\nhuman to human communication. Advances in deep learning over the last few years\nhave produced major speech recognition improvements on the representative\nSwitchboard conversational corpus.Word error rates that just a few years ago\nwere 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now\nbelieved to be within striking range of human performance. This then raises two\nissues - what IS human performance, and how far down can we still drive speech\nrecognition error rates? A recent paper by Microsoft suggests that we have\nalready achieved human performance. In trying to verify this statement, we\nperformed an independent set of human performance measurements on two\nconversational tasks and found that human performance may be considerably\nbetter than what was earlier reported, giving the community a significantly\nharder goal to achieve. We also report on our own efforts in this area,\npresenting a set of acoustic and language modeling techniques that lowered the\nword error rate of our own English conversational telephone LVCSR system to the\nlevel of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000\nevaluation, which - at least at the writing of this paper - is a new\nperformance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with\nmultiple feature inputs, a second LSTM trained with speaker-adversarial\nmulti-task learning and a third residual net (ResNet) with 25 convolutional\nlayers and time-dilated convolutions. On the language modeling side, we use\nword and character LSTMs and convolutional WaveNet-style language models."},
{"task": "Medical Diagnosis", "papers": {}},
{"task": "Road Segementation", "papers": {}},
{"task": "UNET Segmentation", "papers": {}},
{"task": "Retinal OCT Disease Classification", "papers": {"0": "Optic-Net: A Novel Convolutional Neural Network for Diagnosis of Retinal Diseases from Optical Tomography Images", "1": "Optic-Net: A Novel Convolutional Neural Network for Diagnosis of Retinal Diseases from Optical Tomography Images"}},
{"task": "Attentive segmentation networks", "papers": {}},
{"task": "Joint Entity and Relation Extraction", "papers": {"0": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training", "1": "Joint Entity and Relation Extraction with Set Prediction Networks", "2": "Joint Entity and Relation Extraction with Set Prediction Networks"}},
{"task": "Event Extraction", "papers": {"0": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "1": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "2": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "3": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "4": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "5": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "6": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts"}},
{"task": "Histopathological Segmentation", "papers": {}},
{"task": "Room Layout Estimation", "papers": {}},
{"task": "Spinal Cord Gray Matter - Segmentation", "papers": {}},
{"task": "4D Spatio Temporal Semantic Segmentation", "papers": {}},
{"task": "Unsupervised Semantic Segmentation", "papers": {"0": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "1": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "2": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "3": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "4": "SegSort: Segmentation by Discriminative Sorting of Segments"}},
{"task": "indoor scene understanding", "papers": {}},
{"task": "One-Shot Segmentation", "papers": {"0": "One-Shot Segmentation in Clutter"}},
{"task": "Semi-Supervised Semantic Segmentation", "papers": {"0": "Semi-supervised semantic segmentation needs strong, varied perturbations", "1": "Semi-supervised semantic segmentation needs strong, varied perturbations", "2": "Semi-supervised semantic segmentation needs strong, varied perturbations", "3": "Semi-supervised semantic segmentation needs strong, varied perturbations", "4": "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning", "5": "Semi-supervised semantic segmentation needs strong, varied perturbations", "6": "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning", "7": "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning", "8": "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning"}},
{"task": "3D Part Segmentation", "papers": {"0": "KPConv: Flexible and Deformable Convolution for Point Clouds"}},
{"task": "3D Semantic Segmentation", "papers": {"0": "Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution"}},
{"task": "Panoptic Segmentation", "papers": {"0": "EfficientPS: Efficient Panoptic Segmentation", "1": "DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution", "2": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation", "3": "EfficientPS: Efficient Panoptic Segmentation", "4": "ResNeSt: Split-Attention Networks", "5": "EfficientPS: Efficient Panoptic Segmentation", "6": "EfficientPS: Efficient Panoptic Segmentation"}},
{"task": "Hand Keypoint Localization", "papers": {}},
{"task": "Weakly-Supervised Semantic Segmentation", "papers": {"0": "Saliency Guided Self-attention Network for Weakly and Semi-supervised Semantic Segmentation", "1": "Saliency Guided Self-attention Network for Weakly and Semi-supervised Semantic Segmentation"}},
{"task": "Real-Time Semantic Segmentation", "papers": {"0": "HarDNet: A Low Memory Traffic Network", "1": "BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation", "2": "Light-Weight RefineNet for Real-Time Semantic Segmentation", "3": "BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation", "4": "LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation"}},
{"task": "Gesture-to-Gesture Translation", "papers": {"0": "GestureGAN for Hand Gesture-to-Gesture Translation in the Wild", "1": "GestureGAN for Hand Gesture-to-Gesture Translation in the Wild"}},
{"task": "Scene Segmentation", "papers": {"0": "Index Network", "1": "3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation", "2": "Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation"}},
{"task": "Tumor Segmentation", "papers": {"0": "Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images"}},
{"task": "Semantic Segmentation", "papers": {"0": "Hierarchical Multi-Scale Attention for Semantic Segmentation", "1": "Rethinking Pre-training and Self-training", "2": "ResNeSt: Split-Attention Networks", "3": "ResNeSt: Split-Attention Networks", "4": "ResNeSt: Split-Attention Networks", "5": "ResNeSt: Split-Attention Networks", "6": "Rethinking Pre-training and Self-training", "7": "Virtual Multi-view Fusion for 3D Semantic Segmentation", "8": "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds", "9": "Improving Semantic Segmentation via Video Propagation and Label Relaxation", "10": "Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning", "11": "Scene Segmentation with Dual Relation-aware Attention Network", "12": "MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning", "13": "Self-Correction for Human Parsing", "14": "SkyScapes Fine-Grained Semantic Understanding of Aerial Scenes", "15": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation", "16": "Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning", "17": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "18": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "19": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "20": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "21": "Cloud-Net+: A Cloud Segmentation CNN for Landsat 8 Remote Sensing Imagery Optimized with Filtered Jaccard Loss Function", "22": "Global Aggregation then Local Distribution in Fully Convolutional Networks", "23": "SkyScapes Fine-Grained Semantic Understanding of Aerial Scenes", "24": "Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation", "25": "Improving Semantic Segmentation via Video Propagation and Label Relaxation", "26": "FasterSeg: Searching for Faster Real-time Semantic Segmentation", "27": "A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View", "28": "Inter-Region Affinity Distillation for Road Marking Segmentation", "29": "Plugin Networks for Inference under Partial Evidence", "30": "What's There in the Dark", "31": "What's There in the Dark"}},
{"paper": "DeepEventMine: end-to-end neural nested event extraction from biomedical texts", "abstract": "Motivation\r\nRecent neural approaches on event extraction from text mainly focus on flat events in general domain, while there are less attempts to detect nested and overlapping events. These existing systems are built on given entities and they depend on external syntactic tools.Results\r\nWe propose an end-to-end neural nested event extraction model named DeepEventMine that extracts multiple overlapping directed acyclic graph structures from a raw sentence. On the top of the bidirectional encoder representations from transformers model, our model detects nested entities and triggers, roles, nested events and their modifications in an end-to-end manner without any syntactic tools. Our DeepEventMine model achieves the new state-of-the-art performance on seven biomedical nested event extraction tasks. Even when gold entities are unavailable, our model can detect events from raw text with promising performance. Availability and implementation\r\nOur codes and models to reproduce the results are available at: https://github.com/aistairc/DeepEventMine."},
{"paper": "Joint Entity and Relation Extraction with Set Prediction Networks", "abstract": "The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered.However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at http://github.com/DianboWork/SPN4RE."},
{"paper": "Optic-Net: A Novel Convolutional Neural Network for Diagnosis of Retinal Diseases from Optical Tomography Images", "abstract": "Diagnosing different retinal diseases from Spectral Domain Optical Coherence Tomography (SD-OCT) images is a challenging task. Different automated approaches such as image processing, machine learning and deep learning algorithms have been used for early detection and diagnosis of retinal diseases.Unfortunately, these are prone to error and computational inefficiency, which requires further intervention from human experts. In this paper, we propose a novel convolution neural network architecture to successfully distinguish between different degeneration of retinal layers and their underlying causes. The proposed novel architecture outperforms other classification models while addressing the issue of gradient explosion. Our approach reaches near perfect accuracy of 99.8% and 100% for two separately available Retinal SD-OCT data-set respectively. Additionally, our architecture predicts retinal diseases in real time while outperforming human diagnosticians."},
{"paper": "Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution", "abstract": "Self-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling.To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3%, ranking 1st on the competitive SemanticKITTI leaderboard. It also achieves 8x computation reduction and 3x measured speedup over MinkowskiNet with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI."},
{"task": "Hand Segmentation", "papers": {}},
{"paper": "Semi-supervised semantic segmentation needs strong, varied perturbations", "abstract": "Consistency regularization describes a class of approaches that have yielded ground breaking results in semi-supervised classification problems. Prior work has established the cluster assumption - under which the data distribution consists of uniform class clusters of samples separated by low density regions - as important to its success.We analyze the problem of semantic segmentation and find that its' distribution does not exhibit low density regions separating classes and offer this as an explanation for why semi-supervised segmentation is a challenging problem, with only a few reports of success. We then identify choice of augmentation as key to obtaining reliable performance without such low-density regions. We find that adapted variants of the recently proposed CutOut and CutMix augmentation techniques yield state-of-the-art semi-supervised semantic segmentation results in standard datasets. Furthermore, given its challenging nature we propose that semantic segmentation acts as an effective acid test for evaluating semi-supervised regularizers. Implementation at: https://github.com/Britefury/cutmix-semisup-seg."},
{"paper": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training", "abstract": "We introduce SpERT, an attention model for span-based joint entity and relation extraction. Our key contribution is a light-weight reasoning on BERT embeddings, which features entity recognition and filtering, as well as relation classification with a localized, marker-free context representation.The model is trained using strong within-sentence negative samples, which are efficiently extracted in a single BERT pass. These aspects facilitate a search over all spans in the sentence. In ablation studies, we demonstrate the benefits of pre-training, strong negative sampling and localized context. Our model outperforms prior work by up to 2.6% F1 score on several datasets for joint entity and relation extraction."},
{"paper": "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning", "abstract": "The state of the art in semantic segmentation is steadily increasing in performance, resulting in more precise and reliable segmentations in many different applications. However, progress is limited by the cost of generating labels for training, which sometimes requires hours of manual labor for a single image.Because of this, semi-supervised methods have been applied to this task, with varying degrees of success. A key challenge is that common augmentations used in semi-supervised classification are less effective for semantic segmentation. We propose a novel data augmentation mechanism called ClassMix, which generates augmentations by mixing unlabelled samples, by leveraging on the network's predictions for respecting object boundaries. We evaluate this augmentation technique on two common semi-supervised semantic segmentation benchmarks, showing that it attains state-of-the-art results. Lastly, we also provide extensive ablation studies comparing different design decisions and training regimes."},
{"task": "Hand-Gesture Recognition", "papers": {"0": "Efficient Neural Vision Systems Based on Convolutional Image Acquisition"}},
{"paper": "KPConv: Flexible and Deformable Convolution for Point Clouds", "abstract": "We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them.Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv."},
{"paper": "EfficientPS: Efficient Panoptic Segmentation", "abstract": "Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task.In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date."},
{"paper": "Saliency Guided Self-attention Network for Weakly and Semi-supervised Semantic Segmentation", "abstract": "Weakly supervised semantic segmentation (WSSS) using only image-level labels can greatly reduce the annotation cost and therefore has attracted considerable research interest. However, its performance is still inferior to the fully supervised counterparts.To mitigate the performance gap, we propose a saliency guided self-attention network (SGAN) to address the WSSS problem. The introduced self-attention mechanism is able to capture rich and extensive contextual information but may mis-spread attentions to unexpected regions. In order to enable this mechanism to work effectively under weak supervision, we integrate class-agnostic saliency priors into the self-attention mechanism and utilize class-specific attention cues as an additional supervision for SGAN. Our SGAN is able to produce dense and accurate localization cues so that the segmentation performance is boosted. Moreover, by simply replacing the additional supervisions with partially labeled ground-truth, SGAN works effectively for semi-supervised semantic segmentation as well. Experiments on the PASCAL VOC 2012 and COCO datasets show that our approach outperforms all other state-of-the-art methods in both weakly and semi-supervised settings."},
{"paper": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation", "abstract": "Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions.Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes."},
{"paper": "What's There in the Dark", "abstract": "Scene  Parsing  is  an  important  cog  for  modern  autonomousdriving systems. Most of the works in semantic segmenta-tion pertains to day-time scenes with favourable weather andillumination  conditions.In  this  paper,  we  propose  a  noveldeep architecture, NiSeNet, that performs semantic segmen-tation of night scenes using a domain mapping approach ofsynthetic  to  real  data. It  is  a  dual-channel  network,  wherewe designed a Real channel using DeepLabV3+ coupled withan MSE loss to preserve the spatial information. In addition,we used an Adaptive channel reducing the domain gap be-tween  synthetic  and  real  night  images,  which  also  comple-ments  the  failures  of  Real  channel  output. Apart  from  thedual channel, we introduced a novel fusion scheme to fuse theoutputs of two channels. In addition to that, we compiled anew dataset Urban Night Driving Dataset (UNDD); it consistsof7125unlabelled day and night images; additionally, it has75night images with pixel-level annotations having classesequivalent to Cityscapes dataset. We evaluated our approachon  the  Berkley  Deep  Drive  dataset,  the  challenging  Mapil-lary dataset and UNDD dataset to exhibit that the proposedmethod outperforms the state-of-the-art techniques in termsof accuracy and visual quality"},
{"paper": "One-Shot Segmentation in Clutter", "abstract": "We tackle the problem of one-shot segmentation: finding and segmenting a\npreviously unseen object in a cluttered scene based on a single instruction\nexample. We propose a novel dataset, which we call $\\textit{cluttered\nOmniglot}$.Using a baseline architecture combining a Siamese embedding for\ndetection with a U-net for segmentation we show that increasing levels of\nclutter make the task progressively harder. Using oracle models with access to\nvarious amounts of ground-truth information, we evaluate different aspects of\nthe problem and show that in this kind of visual search task, detection and\nsegmentation are two intertwined problems, the solution to each of which helps\nsolving the other. We therefore introduce $\\textit{MaskNet}$, an improved model\nthat attends to multiple candidate locations, generates segmentation proposals\nto mask out background clutter and selects among the segmented objects. Our\nfindings suggest that such image recognition models based on an iterative\nrefinement of object detection and foreground segmentation may provide a way to\ndeal with highly cluttered scenes."},
{"paper": "Plugin Networks for Inference under Partial Evidence", "abstract": "In this paper, we propose a novel method to incorporate partial evidence in the inference of deep convolutional neural networks. Contrary to the existing, top performing methods, which either iteratively modify the input of the network or exploit external label taxonomy to take the partial evidence into account, we add separate network modules (\"Plugin Networks\") to the intermediate layers of a pre-trained convolutional network.The goal of these modules is to incorporate additional signal, ie information about known labels, into the inference procedure and adjust the predicted output accordingly. Since the attached plugins have a simple structure, consisting of only fully connected layers, we drastically reduced the computational cost of training and inference. At the same time, the proposed architecture allows to propagate information about known labels directly to the intermediate layers to improve the final representation. Extensive evaluation of the proposed method confirms that our Plugin Networks outperform the state-of-the-art in a variety of tasks, including scene categorization, multi-label image annotation, and semantic segmentation."},
{"paper": "Inter-Region Affinity Distillation for Road Marking Segmentation", "abstract": "We study the problem of distilling knowledge from a large deep teacher network to a much smaller student network for the task of road marking segmentation. In this work, we explore a novel knowledge distillation (KD) approach that can transfer 'knowledge' on scene structure more effectively from a teacher to a student model.Our method is known as Inter-Region Affinity KD (IntRA-KD). It decomposes a given road scene image into different regions and represents each region as a node in a graph. An inter-region affinity graph is then formed by establishing pairwise relationships between nodes based on their similarity in feature distribution. To learn structural knowledge from the teacher network, the student is required to match the graph generated by the teacher. The proposed method shows promising results on three large-scale road marking segmentation benchmarks, i.e., ApolloScape, CULane and LLAMAS, by taking various lightweight models as students and ResNet-101 as the teacher. IntRA-KD consistently brings higher performance gains on all lightweight models, compared to previous distillation methods. Our code is available at https://github.com/cardwing/Codes-for-IntRA-KD."},
{"paper": "DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution", "abstract": "Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice. In this paper, we explore this mechanism in the backbone design for object detection.At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level, we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results using switch functions. Combining them results in DetectoRS, which significantly improves the performances of object detection. On COCO test-dev, DetectoRS achieves state-of-the-art 54.7% box AP for object detection, 47.1% mask AP for instance segmentation, and 49.6% PQ for panoptic segmentation. The code is made publicly available."},
{"paper": "Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation", "abstract": "This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data.Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes -> Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks."},
{"paper": "Global Aggregation then Local Distribution in Fully Convolutional Networks", "abstract": "It has been widely proven that modelling long-range dependencies in fully convolutional networks (FCNs) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection. However, global aggregation is often dominated by features of large patterns and tends to oversmooth regions that contain small patterns (e.g., boundaries and small objects).To resolve this problem, we propose to first use \\emph{Global Aggregation} and then \\emph{Local Distribution}, which is called GALD, where long-range dependencies are more confidently used inside large pattern regions and vice versa. The size of each pattern at each position is estimated in the network as a per-channel mask map. GALD is end-to-end trainable and can be easily plugged into existing FCNs with various global aggregation modules for a wide range of vision tasks, and consistently improves the performance of state-of-the-art object detection and instance segmentation approaches. In particular, GALD used in semantic segmentation achieves new state-of-the-art performance on Cityscapes test set with mIoU 83.3\\%. Code is available at: \\url{https://github.com/lxtGH/GALD-Net}"},
{"paper": "Cloud-Net+: A Cloud Segmentation CNN for Landsat 8 Remote Sensing Imagery Optimized with Filtered Jaccard Loss Function", "abstract": "Cloud Segmentation is one of the fundamental steps in optical remote sensing image analysis. Current methods for identification of cloud regions in aerial or satellite images are not accurate enough especially in the presence of snow and haze.This paper presents a deep learning-based framework to address the problem of cloud detection in Landsat 8 imagery. The proposed method benefits from a convolutional neural network (Cloud-Net+) with multiple blocks, which is trained with a novel loss function (Filtered Jaccard loss). The proposed loss function is more sensitive to the absence of cloud pixels in an image and penalizes/rewards the predicted mask more accurately. The combination of Cloud-Net+ and Filtered Jaccard loss function delivers superior results over four public cloud detection datasets. Our experiments on one of the most common public datasets in computer vision (Pascal VOC dataset) show that the proposed network/loss function could be used in other segmentation tasks for more accurate performance/evaluation."},
{"paper": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "abstract": "Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the real-world. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions.Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modality-specific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities, we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a larger effective receptive field with more than 10x fewer parameters, complemented with a strong decoder with a multi-resolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on several benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance."},
{"paper": "Efficient Neural Vision Systems Based on Convolutional Image Acquisition", "abstract": "Despite the substantial progress made in deep learning in recent years, advanced approaches remain computationally intensive. The trade-off between accuracy and computation time and energy limits their use in real-time applications on low power and other resource-constrained systems.In this paper, we tackle this fundamental challenge by introducing a hybrid optical-digital implementation of a convolutional neural network (CNN) based on engineering of the point spread function (PSF) of an optical imaging system. This is done by coding an imaging aperture such that its PSF replicates a large convolution kernel of the first layer of a pre-trained CNN. As the convolution takes place in the optical domain, it has zero cost in terms of energy consumption and has zero latency independent of the kernel size. Experimental results on two datasets demonstrate that our approach yields more than two orders of magnitude reduction in the computational cost while achieving near-state-of-the-art accuracy, or equivalently, better accuracy at the same computational cost."},
{"paper": "FasterSeg: Searching for Faster Real-time Semantic Segmentation", "abstract": "We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models.To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to \"collapsing\" to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model's accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy."},
{"paper": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation", "abstract": "We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results.Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance."},
{"paper": "ResNeSt: Split-Attention Networks", "abstract": "While image classification models have recently continued to advance, most downstream applications such as object detection and semantic segmentation still employ ResNet variants as the backbone network due to their simple and modular structure. We present a simple and modular Split-Attention block that enables attention across feature-map groups.By stacking these Split-Attention blocks ResNet-style, we obtain a new ResNet variant which we call ResNeSt. Our network preserves the overall ResNet structure to be used in downstream tasks straightforwardly without introducing additional computational costs. ResNeSt models outperform other networks with similar model complexities. For example, ResNeSt-50 achieves 81.13% top-1 accuracy on ImageNet using a single crop-size of 224x224, outperforming previous best ResNet variant by more than 1% accuracy. This improvement also helps downstream tasks including object detection, instance segmentation and semantic segmentation. For example, by simply replace the ResNet-50 backbone with ResNeSt-50, we improve the mAP of Faster-RCNN on MS-COCO from 39.3% to 42.3% and the mIoU for DeeplabV3 on ADE20K from 42.1% to 45.1%."},
{"paper": "SkyScapes Fine-Grained Semantic Understanding of Aerial Scenes", "abstract": "Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring, and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications.To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the baselines in region outlines and level of detail on both tasks."},
{"paper": "Self-Correction for Human Parsing", "abstract": "Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g. human parsing, remains a challenging task. The ambiguous boundary between different semantic parts and those categories with similar appearance usually are confusing, leading to unexpected noises in ground truth masks.To tackle the problem of learning with label noises, this work introduces a purification strategy, called Self-Correction for Human Parsing (SCHP), to progressively promote the reliability of the supervised labels as well as the learned models. In particular, starting from a model trained with inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudo-masks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those correspondingly corrected labels can in turn to further boost the model performance. In this way, the models and the labels will reciprocally become more robust and accurate during the self-correction learning cycles. Benefiting from the superiority of SCHP, we achieve the best performance on two popular single-person human parsing benchmarks, including LIP and Pascal-Person-Part datasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is available at https://github.com/PeikeLi/Self-Correction-Human-Parsing."},
{"paper": "Scene Segmentation with Dual Relation-aware Attention Network", "abstract": "In this article, we propose a Dual Relation-aware Attention Network (DRANet) to handle the task of scene segmentation. How to efficiently exploit context is essential for pixel-level recognition.To address the issue, we adaptively capture contextual information based on the relation-aware attention mechanism. Especially, we append two types of attention modules on the top of the dilated fully convolutional network (FCN), which model the contextual dependencies in spatial and channel dimensions, respectively. In the attention modules, we adopt a self-attention mechanism to model semantic associations between any two pixels or channels. Each pixel or channel can adaptively aggregate context from all pixels or channels according to their correlations. To reduce the high cost of computation and memory caused by the abovementioned pairwise association computation, we further design two types of compact attention modules. In the compact attention modules, each pixel or channel is built into association only with a few numbers of gathering centers and obtains corresponding context aggregation over these gathering centers. Meanwhile, we add a cross-level gating decoder to selectively enhance spatial details that boost the performance of the network. We conduct extensive experiments to validate the effectiveness of our network and achieve new state-of-the-art segmentation performance on four challenging scene segmentation data sets, i.e., Cityscapes, ADE20K, PASCAL Context, and COCO Stuff data sets. In particular, a Mean IoU score of 82.9% on the Cityscapes test set is achieved without using extra coarse annotated data."},
{"paper": "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds", "abstract": "We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds.In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI."},
{"paper": "Improving Semantic Segmentation via Video Propagation and Label Relaxation", "abstract": "Semantic segmentation requires large amounts of pixel-wise annotations to learn accurate models. In this paper, we present a video prediction-based methodology to scale up training sets by synthesizing new training samples in order to improve the accuracy of semantic segmentation networks.We exploit video prediction models' ability to predict future frames in order to also predict future labels. A joint propagation strategy is also proposed to alleviate mis-alignments in synthesized samples. We demonstrate that training segmentation models on datasets augmented by the synthesized samples leads to significant improvements in accuracy. Furthermore, we introduce a novel boundary label relaxation technique that makes training robust to annotation noise and propagation artifacts along object boundaries. Our proposed methods achieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our single model, without model ensembles, achieves 72.8% mIoU on the KITTI semantic segmentation test set, which surpasses the winning entry of the ROB challenge 2018. Our code and videos can be found at https://nv-adlr.github.io/publication/2018-Segmentation."},
{"paper": "Virtual Multi-view Fusion for 3D Semantic Segmentation", "abstract": "Semantic segmentation of 3D meshes is an important problem for 3D scene understanding. In this paper we revisit the classic multiview representation of 3D meshes and study several techniques that make them effective for 3D semantic segmentation of meshes.Given a 3D mesh reconstructed from RGBD sensors, our method effectively chooses different virtual views of the 3D mesh and renders multiple 2D channels for training an effective 2D semantic segmentation model. Features from multiple per view predictions are finally fused on 3D mesh vertices to predict mesh semantic segmentation labels. Using the large scale indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual views enable more effective training of 2D semantic segmentation networks than previous multiview approaches. When the 2D per pixel predictions are aggregated on 3D surfaces, our virtual multiview fusion method is able to achieve significantly better 3D semantic segmentation results compared to all prior multiview approaches and competitive with recent 3D convolution approaches."},
{"paper": "Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning", "abstract": "We propose a new supervized learning framework for oversegmenting 3D point\nclouds into superpoints. We cast this problem as learning deep embeddings of\nthe local geometry and radiometry of 3D points, such that the border of objects\npresents high contrasts.The embeddings are computed using a lightweight neural\nnetwork operating on the points' local neighborhood. Finally, we formulate\npoint cloud oversegmentation as a graph partition problem with respect to the\nlearned embeddings. This new approach allows us to set a new state-of-the-art in point cloud\noversegmentation by a significant margin, on a dense indoor dataset (S3DIS) and\na sparse outdoor one (vKITTI). Our best solution requires over five times fewer\nsuperpoints to reach similar performance than previously published methods on\nS3DIS. Furthermore, we show that our framework can be used to improve\nsuperpoint-based semantic segmentation algorithms, setting a new\nstate-of-the-art for this task as well."},
{"paper": "MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning", "abstract": "In this paper, we argue about the importance of considering task interactions at multiple scales when distilling task information in a multi-task learning setup. In contrast to common belief, we show that tasks with high affinity at a certain scale are not guaranteed to retain this behaviour at other scales, and vice versa.We propose a novel architecture, namely MTI-Net, that builds upon this finding in three ways. First, it explicitly models task interactions at every scale via a multi-scale multi-modal distillation unit. Second, it propagates distilled task information from lower to higher scales via a feature propagation module. Third, it aggregates the refined task features from all scales via a feature aggregation unit to produce the final per-task predictions. Extensive experiments on two multi-task dense labeling datasets show that, unlike prior work, our multi-task model delivers on the full potential of multi-task learning, that is, smaller memory footprint, reduced number of calculations, and better performance w.r.t. single-task learning. The code is made publicly available: https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch."},
{"paper": "Hierarchical Multi-Scale Attention for Semantic Segmentation", "abstract": "Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling.In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test)."},
{"paper": "Rethinking Pre-training and Self-training", "abstract": "Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models.He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art result by DeepLabv3+."},
{"paper": "3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation", "abstract": "We present 3DMV, a novel method for 3D semantic scene segmentation of RGB-D\nscans in indoor environments using a joint 3D-multi-view prediction network. In\ncontrast to existing methods that either use geometry or RGB data as input for\nthis task, we combine both data modalities in a joint, end-to-end network\narchitecture.Rather than simply projecting color data into a volumetric grid\nand operating solely in 3D -- which would result in insufficient detail -- we\nfirst extract feature maps from associated RGB images. These features are then\nmapped into the volumetric feature grid of a 3D network using a differentiable\nbackprojection layer. Since our target is 3D scanning scenarios with possibly\nmany frames, we use a multi-view pooling approach in order to handle a varying\nnumber of RGB input views. This learned combination of RGB and geometric\nfeatures with our joint 2D-3D architecture achieves significantly better\nresults than existing baselines. For instance, our final result on the ScanNet\n3D segmentation benchmark increases from 52.8\\% to 75\\% accuracy compared to\nexisting volumetric architectures."},
{"paper": "Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation", "abstract": "Semantic Segmentation using deep convolutional neural network pose more\ncomplex challenge for any GPU intensive task. As it has to compute million of\nparameters, it results to huge memory consumption.Moreover, extracting finer\nfeatures and conducting supervised training tends to increase the complexity. With the introduction of Fully Convolutional Neural Network, which uses finer\nstrides and utilizes deconvolutional layers for upsampling, it has been a go to\nfor any image segmentation task. In this paper, we propose two segmentation\narchitecture which not only needs one-third the parameters to compute but also\ngives better accuracy than the similar architectures. The model weights were\ntransferred from the popular neural net like VGG19 and VGG16 which were trained\non Imagenet classification data-set. Then we transform all the fully connected\nlayers to convolutional layers and use dilated convolution for decreasing the\nparameters. Lastly, we add finer strides and attach four skip architectures\nwhich are element-wise summed with the deconvolutional layers in steps. We\ntrain and test on different sparse and fine data-sets like Pascal VOC2012,\nPascal-Context and NYUDv2 and show how better our model performs in this tasks. On the other hand our model has a faster inference time and consumes less\nmemory for training and testing on NVIDIA Pascal GPUs, making it more efficient\nand less memory consuming architecture for pixel-wise segmentation."},
{"paper": "Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images", "abstract": "Incorporating human domain knowledge for breast tumor diagnosis is challenging, since shape, boundary, curvature, intensity, or other common medical priors vary significantly across patients and cannot be employed. This work proposes a new approach for integrating visual saliency into a deep learning model for breast tumor segmentation in ultrasound images.Visual saliency refers to image maps containing regions that are more likely to attract radiologists visual attention. The proposed approach introduces attention blocks into a U-Net architecture, and learns feature representations that prioritize spatial regions with high saliency levels. The validation results demonstrate increased accuracy for tumor segmentation relative to models without salient attention layers. The approach achieved a Dice similarity coefficient of 90.5 percent on a dataset of 510 images. The salient attention model has potential to enhance accuracy and robustness in processing medical images of other organs, by providing a means to incorporate task-specific knowledge into deep learning architectures."},
{"paper": "Index Network", "abstract": "We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation.By viewing the indices as a function of the feature map, we introduce the concept of \"learning to index\", and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the downsampling and upsampling stages, without extra training supervision. At the core of this framework is a new learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned on the feature map itself. IndexNet can be used as a plug-in applying to almost all off-the-shelf convolutional networks that have coupled downsampling and upsampling stages, giving the networks the ability to dynamically capture variations of local patterns. In particular, we instantiate and investigate five families of IndexNet and demonstrate their effectiveness on four dense prediction tasks, including image denoising, image matting, semantic segmentation, and monocular depth estimation. Code and models have been made available at: https://tinyurl.com/IndexNetV1"},
{"paper": "GestureGAN for Hand Gesture-to-Gesture Translation in the Wild", "abstract": "Hand gesture-to-gesture translation in the wild is a challenging task since hand gestures can have arbitrary poses, sizes, locations and self-occlusions. Therefore, this task requires a high-level understanding of the mapping between the input source gesture and the output target gesture.To tackle this problem, we propose a novel hand Gesture Generative Adversarial Network (GestureGAN). GestureGAN consists of a single generator $G$ and a discriminator $D$, which takes as input a conditional hand image and a target hand skeleton image. GestureGAN utilizes the hand skeleton information explicitly, and learns the gesture-to-gesture mapping through two novel losses, the color loss and the cycle-consistency loss. The proposed color loss handles the issue of \"channel pollution\" while back-propagating the gradients. In addition, we present the Fr\\'echet ResNet Distance (FRD) to evaluate the quality of generated images. Extensive experiments on two widely used benchmark datasets demonstrate that the proposed GestureGAN achieves state-of-the-art performance on the unconstrained hand gesture-to-gesture translation task. Meanwhile, the generated images are in high-quality and are photo-realistic, allowing them to be used as data augmentation to improve the performance of a hand gesture classifier. Our model and code are available at https://github.com/Ha0Tang/GestureGAN."},
{"paper": "Light-Weight RefineNet for Real-Time Semantic Segmentation", "abstract": "We consider an important task of effective and efficient semantic image\nsegmentation. In particular, we adapt a powerful semantic segmentation\narchitecture, called RefineNet, into the more compact one, suitable even for\ntasks requiring real-time performance on high-resolution inputs.To this end,\nwe identify computationally expensive blocks in the original setup, and propose\ntwo modifications aimed to decrease the number of parameters and floating point\noperations. By doing that, we achieve more than twofold model reduction, while\nkeeping the performance levels almost intact. Our fastest model undergoes a\nsignificant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on\n512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL\nVOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7%\nmean iou on the same dataset. Alternatively, we showcase that our approach is\neasily mixable with light-weight classification networks: we attain 79.2% mean\niou on PASCAL VOC using a model that contains only 3.3M parameters and performs\nonly 9.3B floating point operations."},
{"paper": "SegSort: Segmentation by Discriminative Sorting of Segments", "abstract": "Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition.This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set. As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving $76\\%$ performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments."},
{"paper": "BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation", "abstract": "The low-level details and high-level semantics are both essential to the semantic segmentation task. However, to speed up the model inference, current approaches almost always sacrifice the low-level details, which leads to a considerable accuracy decrease.We propose to treat these spatial details and categorical semantics separately to achieve high accuracy and high efficiency for realtime semantic segmentation. To this end, we propose an efficient and effective architecture with a good trade-off between speed and accuracy, termed Bilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a Detail Branch, with wide channels and shallow layers to capture low-level details and generate high-resolution feature representation; (ii) a Semantic Branch, with narrow channels and deep layers to obtain high-level semantic context. The Semantic Branch is lightweight due to reducing the channel capacity and a fast-downsampling strategy. Furthermore, we design a Guided Aggregation Layer to enhance mutual connections and fuse both types of feature representation. Besides, a booster training strategy is designed to improve the segmentation performance without any extra inference cost. Extensive quantitative and qualitative evaluations demonstrate that the proposed architecture performs favourably against a few state-of-the-art real-time semantic segmentation approaches. Specifically, for a 2,048x1,024 input, we achieve 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than existing methods, yet we achieve better segmentation accuracy."},
{"paper": "LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation", "abstract": "Semantic image segmentation plays a pivotal role in many vision applications including autonomous driving and medical image analysis. Most of the former approaches move towards enhancing the performance in terms of accuracy with a little awareness of computational efficiency.In this paper, we introduce LiteSeg, a lightweight architecture for semantic image segmentation. In this work, we explore a new deeper version of Atrous Spatial Pyramid Pooling module (ASPP) and apply short and long residual connections, and depthwise separable convolution, resulting in a faster and efficient model. LiteSeg architecture is introduced and tested with multiple backbone networks as Darknet19, MobileNet, and ShuffleNet to provide multiple trade-offs between accuracy and computational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone network, achieves an accuracy of 67.81% mean intersection over union at 161 frames per second with $640 \\times 360$ resolution on the Cityscapes dataset."},
{"paper": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "abstract": "We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation.These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC"},
{"paper": "HarDNet: A Low Memory Traffic Network", "abstract": "State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time.We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge."},
{"task": "Emotion-Cause Pair Extraction", "papers": {"0": "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction", "1": "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction"}},
{"task": "Denoising Of Radar Micro-Doppler Signatures", "papers": {}},
{"task": "Emotion Recognition in Conversation", "papers": {"0": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations", "1": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations", "2": "NELEC at SemEval-2019 Task 3: Think Twice Before Going Deep", "3": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation", "4": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations", "5": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations", "6": "Hierarchical Transformer Network for Utterance-level Emotion Recognition"}},
{"task": "Speech Emotion Recognition", "papers": {"0": "Multimodal Speech Emotion Recognition and Ambiguity Resolution"}},
{"task": "Emotion Recognition", "papers": {"0": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks", "1": "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition"}},
{"task": "Hand Pose Estimation", "papers": {"0": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map", "1": "Pixel-wise Regression: 3D Hand Pose Estimation via Spatial-form Representation and Differentiable Decoder", "2": "Dense 3D Regression for Hand Pose Estimation", "3": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image", "4": "HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation", "5": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"}},
{"task": "Building change detection for remote sensing images", "papers": {"0": "Looking for change? Roll the Dice and demand Attention"}},
{"task": "Multimodal Emotion Recognition", "papers": {"0": "Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling", "1": "Context-Dependent Sentiment Analysis in User-Generated Videos", "2": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning"}},
{"task": "Lake Detection", "papers": {}},
{"task": "Hand Gesture Recognition", "papers": {"0": "An Efficient PointLSTM for Point Clouds Based Gesture Recognition", "1": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition", "2": "Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks", "3": "Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training", "4": "An Efficient PointLSTM for Point Clouds Based Gesture Recognition", "5": "An Efficient PointLSTM for Point Clouds Based Gesture Recognition", "6": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition", "7": "Fast and Robust Dynamic Hand Gesture Recognition via Key Frames Extraction and Feature Fusion", "8": "Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention", "9": "Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention", "10": "Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition", "11": "Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition", "12": "Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition", "13": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition", "14": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition", "15": "Fast and Robust Dynamic Hand Gesture Recognition via Key Frames Extraction and Feature Fusion"}},
{"task": "Change detection for remote sensing images", "papers": {}},
{"task": "The Semantic Segmentation Of Remote Sensing Imagery", "papers": {}},
{"task": "Lake Ice Monitoring", "papers": {}},
{"task": "Segmentation Of Remote Sensing Imagery", "papers": {}},
{"task": "3D", "papers": {}},
{"task": "Remote Sensing Image Classification", "papers": {}},
{"paper": "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document. Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs.However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well. In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction. It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking. Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document."},
{"task": "Multi-View 3D Shape Retrieval", "papers": {}},
{"task": "Underwater 3D Scene Reconstruction", "papers": {}},
{"paper": "Multimodal Speech Emotion Recognition and Ambiguity Resolution", "abstract": "Identifying emotion from speech is a non-trivial task pertaining to the\nambiguous definition of emotion itself. In this work, we adopt a\nfeature-engineering based approach to tackle the task of speech emotion\nrecognition.Formalizing our problem as a multi-class classification problem,\nwe compare the performance of two categories of models. For both, we extract\neight hand-crafted features from the audio signal. In the first approach, the\nextracted features are used to train six traditional machine learning\nclassifiers, whereas the second approach is based on deep learning wherein a\nbaseline feed-forward neural network and an LSTM-based classifier are trained\nover the same features. In order to resolve ambiguity in communication, we also\ninclude features from the text domain. We report accuracy, f-score, precision,\nand recall for the different experiment settings we evaluated our models in. Overall, we show that lighter machine learning based models trained over a few\nhand-crafted features are able to achieve performance comparable to the current\ndeep learning based state-of-the-art method for emotion recognition."},
{"task": "Point Set Upsampling", "papers": {}},
{"task": "3D Surface Generation", "papers": {}},
{"paper": "HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation", "abstract": "Hand pose estimation from 3D depth images, has been explored widely using various kinds of techniques in the field of computer vision. Though, deep learning based method improve the performance greatly recently, however, this problem still remains unsolved due to lack of large datasets, like ImageNet or effective data synthesis methods.In this paper, we propose HandAugment, a method to synthesize image data to augment the training process of the neural networks. Our method has two main parts: First, We propose a scheme of two-stage neural networks. This scheme can make the neural networks focus on the hand regions and thus to improve the performance. Second, we introduce a simple and effective method to synthesize data by combining real and synthetic image together in the image space. Finally, we show that our method achieves the first place in the task of depth-based 3D hand pose estimation in HANDS 2019 challenge."},
{"paper": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning", "abstract": "Emotion recognition has become an important field of research in Human Computer Interactions as we improve upon the techniques for modelling the various aspects of behaviour. With the advancement of technology our understanding of emotions are advancing, there is a growing need for automatic emotion recognition systems.One of the directions the research is heading is the use of Neural Networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of Neural networks to enable us to perform multimodal Emotion recognition on IEMOCAP dataset using data from Speech, Text, and Motion capture data from face expressions, rotation and hand movements. Prior research has concentrated on Emotion detection from Speech on the IEMOCAP dataset, but our approach is the first that uses the multiple modes of data offered by IEMOCAP for a more robust and accurate emotion detection."},
{"paper": "Looking for change? Roll the Dice and demand Attention", "abstract": "Change detection, i.e. identification per pixel of changes for some classes of interest from a set of bi-temporal co-registered images, is a fundamental task in the field of remote sensing. It remains challenging due to unrelated forms of change that appear at different times in input images.These are changes due to to different environmental conditions or simply changes of objects that are not of interest. Here, we propose a reliable deep learning framework for the task of semantic change detection in very high-resolution aerial images. Our framework consists of a new loss function, new attention modules, new feature extraction building blocks, and a new backbone architecture that is tailored for the task of semantic change detection. Specifically, we define a new form of set similarity, that is based on an iterative evaluation of a variant of the Dice coefficient. We use this similarity metric to define a new loss function as well as a new spatial and channel convolution Attention layer (the FracTAL). The new attention layer, designed specifically for vision tasks, is memory efficient, thus suitable for use in all levels of deep convolutional networks. Based on these, we introduce two new efficient self-contained feature extraction convolution units. We term these units CEECNet and FracTAL ResNet units. We validate the performance of these feature extraction building blocks on the CIFAR10 reference data and compare the results with standard ResNet modules. Further, we introduce a new encoder/decoder scheme, a network macro-topology, that is tailored for the task of change detection. We validate our approach by showing excellent performance and achieving state of the art score (F1 and Intersection over Union - hereafter IoU) on two building change detection datasets, namely, the LEVIRCD (F1: 0.918, IoU: 0.848) and the WHU (F1: 0.938, IoU: 0.882) datasets."},
{"task": "3D Object Detection From Monocular Images", "papers": {}},
{"paper": "Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition", "abstract": "Gesture recognition is a challenging problem in the field of biometrics. In\nthis paper, we integrate Fisher criterion into Bidirectional Long-Short Term\nMemory (BLSTM) network and Bidirectional Gated Recurrent Unit (BGRU),thus\nleading to two new deep models termed as F-BLSTM and F-BGRU.BothFisher\ndiscriminative deep models can effectively classify the gesture based on\nanalyzing the acceleration and angular velocity data of the human gestures. Moreover, we collect a large Mobile Gesture Database (MGD) based on the\naccelerations and angular velocities containing 5547 sequences of 12 gestures. Extensive experiments are conducted to validate the superior performance of the\nproposed networks as compared to the state-of-the-art BLSTM and BGRU on MGD\ndatabase and two benchmark databases (i.e. BUAA mobile gesture and SmartWatch\ngesture)."},
{"paper": "Context-Dependent Sentiment Analysis in User-Generated Videos", "abstract": "Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video.In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10{\\%} performance improvement over the state of the art and high robustness to generalizability."},
{"paper": "Fast and Robust Dynamic Hand Gesture Recognition via Key Frames Extraction and Feature Fusion", "abstract": "Gesture recognition is a hot topic in computer vision and pattern\nrecognition, which plays a vitally important role in natural human-computer\ninterface. Although great progress has been made recently, fast and robust hand\ngesture recognition remains an open problem, since the existing methods have\nnot well balanced the performance and the efficiency simultaneously.To bridge\nit, this work combines image entropy and density clustering to exploit the key\nframes from hand gesture video for further feature extraction, which can\nimprove the efficiency of recognition. Moreover, a feature fusion strategy is\nalso proposed to further improve feature representation, which elevates the\nperformance of recognition. To validate our approach in a \"wild\" environment,\nwe also introduce two new datasets called HandGesture and Action3D datasets. Experiments consistently demonstrate that our strategy achieves competitive\nresults on Northwestern University, Cambridge, HandGesture and Action3D hand\ngesture datasets. Our code and datasets will release at\nhttps://github.com/Ha0Tang/HandGestureRecognition."},
{"paper": "Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention", "abstract": "We propose a Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) method for hand gesture recognition. The key idea is to first construct a fully-connected graph from a hand skeleton, where the node features and edges are then automatically learned via a self-attention mechanism that performs in both spatial and temporal domains.We further propose to leverage the spatial-temporal cues of joint positions to guarantee robust recognition in challenging conditions. In addition, a novel spatial-temporal mask is applied to significantly cut down the computational cost by 99%. We carry out extensive experiments on benchmarks (DHG-14/28 and SHREC'17) and prove the superior performance of our method compared with the state-of-the-art methods. The source code can be found at https://github.com/yuxiaochen1103/DG-STA."},
{"paper": "Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training", "abstract": "We present an efficient approach for leveraging the knowledge from multiple modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for the task of dynamic hand gesture recognition. Instead of explicitly combining multimodal information, which is commonplace in many state-of-the-art methods, we propose a different framework in which we embed the knowledge of multiple modalities in individual networks so that each unimodal network can achieve an improved performance.In particular, we dedicate separate networks per available modality and enforce them to collaborate and learn to develop networks with common semantics and better representations. We introduce a \"spatiotemporal semantic alignment\" loss (SSA) to align the content of the features from different networks. In addition, we regularize this loss with our proposed \"focal regularization parameter\" to avoid negative knowledge transfer. Experimental results show that our framework improves the test time recognition accuracy of unimodal networks, and provides the state-of-the-art performance on various dynamic hand gesture recognition datasets."},
{"paper": "An Efficient PointLSTM for Point Clouds Based Gesture Recognition", "abstract": "Point clouds contain rich spatial information, which provides complementary cues for gesture recognition. In this paper, we formulate gesture recognition as an irregular sequence recognition problem and aim to capture long-term spatial correlations across point cloud sequences.A novel and effective PointLSTM is proposed to propagate information from past to future while preserving the spatial structure. The proposed PointLSTM combines state information from neighboring points in the past with current features to update the current states by a weight-shared LSTM layer. This method can be integrated into many other sequence learning approaches. In the task of gesture recognition, the proposed PointLSTM achieves state-of-the-art results on two challenging datasets (NVGesture and SHREC'17) and outperforms previous skeleton-based methods. To show its advantages in generalization, we evaluate our method on MSR Action3D dataset, and it produces competitive results with previous skeleton-based methods."},
{"paper": "Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling", "abstract": "Multimodal sentiment analysis is a very actively growing field of research. A\npromising area of opportunity in this field is to improve the multimodal fusion\nmechanism.We present a novel feature fusion strategy that proceeds in a\nhierarchical fashion, first fusing the modalities two in two and only then\nfusing all three modalities. On multimodal sentiment analysis of individual\nutterances, our strategy outperforms conventional concatenation of features by\n1%, which amounts to 5% reduction in error rate. On utterance-level multimodal\nsentiment analysis of multi-utterance video clips, for which current\nstate-of-the-art techniques incorporate contextual information from other\nutterances of the same clip, our hierarchical fusion gives up to 2.4% (almost\n10% error rate reduction) over currently used concatenation. The implementation\nof our method is publicly available in the form of open-source code."},
{"paper": "Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks", "abstract": "Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach.The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04% and 83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available."},
{"paper": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition", "abstract": "Acquiring spatio-temporal states of an action is the most crucial step for\naction classification. In this paper, we propose a data level fusion strategy,\nMotion Fused Frames (MFFs), designed to fuse motion information into static\nimages as better representatives of spatio-temporal states of an action.MFFs\ncan be used as input to any deep learning architecture with very little\nmodification on the network. We evaluate MFFs on hand gesture recognition tasks\nusing three video datasets - Jester, ChaLearn LAP IsoGD and NVIDIA Dynamic Hand\nGesture Datasets - which require capturing long-term temporal relations of hand\nmovements. Our approach obtains very competitive performance on Jester and\nChaLearn benchmarks with the classification accuracies of 96.28% and 57.4%,\nrespectively, while achieving state-of-the-art performance with 84.7% accuracy\non NVIDIA benchmark."},
{"paper": "Dense 3D Regression for Hand Pose Estimation", "abstract": "We present a simple and effective method for 3D hand pose estimation from a\nsingle depth frame. As opposed to previous state-of-the-art methods based on\nholistic 3D regression, our method works on dense pixel-wise estimation.This\nis achieved by careful design choices in pose parameterization, which leverages\nboth 2D and 3D properties of depth map. Specifically, we decompose the pose\nparameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat\nmaps and unit 3D directional vector fields. The 2D/3D joint heat maps and 3D\njoint offsets are estimated via multi-task network cascades, which is trained\nend-to-end. The pixel-wise estimations can be directly translated into a vote\ncasting scheme. A variant of mean shift is then used to aggregate local votes\nwhile enforcing consensus between the the estimated 3D pose and the pixel-wise\n2D and 3D estimations by design. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous\nstate-of-the-art approaches by a large margin. On the ICVL hand dataset, our\nmethod achieves similar accuracy compared to the currently proposed nearly\nsaturated result and outperforms various other proposed methods. Code is\navailable $\\href{\"https://github.com/melonwan/denseReg\"}{\\text{online}}$."},
{"paper": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image", "abstract": "For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints.They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet-50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU."},
{"paper": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map", "abstract": "Most of the existing deep learning-based methods for 3D hand and human pose\nestimation from a single depth map are based on a common framework that takes a\n2D depth map and directly regresses the 3D coordinates of keypoints, such as\nhand or human body joints, via 2D convolutional neural networks (CNNs). The\nfirst weakness of this approach is the presence of perspective distortion in\nthe 2D depth map.While the depth map is intrinsically 3D data, many previous\nmethods treat depth maps as 2D images that can distort the shape of the actual\nobject through projection from 3D to 2D space. This compels the network to\nperform perspective distortion-invariant estimation. The second weakness of the\nconventional approach is that directly regressing 3D coordinates from a 2D\nimage is a highly non-linear mapping, which causes difficulty in the learning\nprocedure. To overcome these weaknesses, we firstly cast the 3D hand and human\npose estimation problem from a single depth map into a voxel-to-voxel\nprediction that uses a 3D voxelized grid and estimates the per-voxel likelihood\nfor each keypoint. We design our model as a 3D CNN that provides accurate\nestimates while running in real-time. Our system outperforms previous methods\nin almost all publicly available 3D hand and human pose estimation datasets and\nplaced first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in https://github.com/mks0601/V2V-PoseNet_RELEASE."},
{"paper": "Pixel-wise Regression: 3D Hand Pose Estimation via Spatial-form Representation and Differentiable Decoder", "abstract": "3D Hand pose estimation from a single depth image is an essential topic in computer vision and human-computer interaction. Although the rising of deep learning method boosts the accuracy a lot, the problem is still hard to solve due to the complex structure of the human hand.Existing methods with deep learning either lose spatial information of hand structure or lack a direct supervision of joint coordinates. In this paper, we propose a novel Pixel-wise Regression method, which use spatial-form representation (SFR) and differentiable decoder (DD) to solve the two problems. To use our method, we build a model, in which we design a particular SFR and its correlative DD which divided the 3D joint coordinates into two parts, plane coordinates and depth coordinates and use two modules named Plane Regression (PR) and Depth Regression (DR) to deal with them respectively. We conduct an ablation experiment to show the method we proposed achieve better results than the former methods. We also make an exploration on how different training strategies influence the learned SFRs and results. The experiment on three public datasets demonstrates that our model is comparable with the existing state-of-the-art models and in one of them our model can reduce mean 3D joint error by 25%."},
{"paper": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks", "abstract": "Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels.In this paper, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition."},
{"paper": "Hierarchical Transformer Network for Utterance-level Emotion Recognition", "abstract": "While there have been significant advances in de-tecting emotions in text, in the field of utter-ance-level emotion recognition (ULER), there are still many problems to be solved. In this paper, we address some challenges in ULER in dialog sys-tems.(1) The same utterance can deliver different emotions when it is in different contexts or from different speakers. (2) Long-range contextual in-formation is hard to effectively capture. (3) Unlike the traditional text classification problem, this task is supported by a limited number of datasets, among which most contain inadequate conversa-tions or speech. To address these problems, we propose a hierarchical transformer framework (apart from the description of other studies, the \"transformer\" in this paper usually refers to the encoder part of the transformer) with a lower-level transformer to model the word-level input and an upper-level transformer to capture the context of utterance-level embeddings. We use a pretrained language model bidirectional encoder representa-tions from transformers (BERT) as the lower-level transformer, which is equivalent to introducing external data into the model and solve the problem of data shortage to some extent. In addition, we add speaker embeddings to the model for the first time, which enables our model to capture the in-teraction between speakers. Experiments on three dialog emotion datasets, Friends, EmotionPush, and EmoryNLP, demonstrate that our proposed hierarchical transformer network models achieve 1.98%, 2.83%, and 3.94% improvement, respec-tively, over the state-of-the-art methods on each dataset in terms of macro-F1."},
{"paper": "NELEC at SemEval-2019 Task 3: Think Twice Before Going Deep", "abstract": "Existing Machine Learning techniques yield close to human performance on\ntext-based classification tasks. However, the presence of multi-modal noise in\nchat data such as emoticons, slang, spelling mistakes, code-mixed data, etc.makes existing deep-learning solutions perform poorly. The inability of\ndeep-learning systems to robustly capture these covariates puts a cap on their\nperformance. We propose NELEC: Neural and Lexical Combiner, a system which\nelegantly combines textual and deep-learning based methods for sentiment\nclassification. We evaluate our system as part of the third task of 'Contextual\nEmotion Detection in Text' as part of SemEval-2019. Our system performs\nsignificantly better than the baseline, as well as our deep-learning model\nbenchmarks. It achieved a micro-averaged F1 score of 0.7765, ranking 3rd on the\ntest-set leader-board. Our code is available at\nhttps://github.com/iamgroot42/nelec"},
{"paper": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations", "abstract": "In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn interactions between interlocutors participating in a conversation.Current state-of-the-art methods often encounter difficulties in context propagation, emotion shift detection, and differentiating between related emotion classes. By learning distinct commonsense representations, COSMIC addresses these challenges and achieves new state-of-the-art results for emotion recognition on four different benchmark conversational datasets. Our code is available at https://github.com/declare-lab/conv-emotion."},
{"paper": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation", "abstract": "Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC.We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets."},
{"task": "3D Geometry Perception", "papers": {}},
{"paper": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations", "abstract": "Messages in human conversations inherently convey emotions. The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks.However, enabling machines to analyze emotions in conversations is challenging, partly because humans often rely on the context and commonsense knowledge to express emotions. In this paper, we address these challenges by proposing a Knowledge-Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a context-aware affective graph attention mechanism. Experiments on multiple textual conversation datasets demonstrate that both context and commonsense knowledge are consistently beneficial to the emotion detection performance. In addition, the experimental results show that our KET model outperforms the state-of-the-art models on most of the tested datasets in F1 score."},
{"task": "3D Feature Matching", "papers": {"0": "Fully Convolutional Geometric Features"}},
{"subfield": "Dimensionality Reduction", "tasks": {"0": "Program Repair", "1": "Fault localization", "2": "Variable misuse", "3": "Function-docstring mismatch", "4": "Wrong binary operator", "5": "Swapped operands", "6": "Exception type"}},
{"task": "Generating 3D Point Clouds", "papers": {}},
{"task": "3D Plane Detection", "papers": {}},
{"task": "Classify 3D Point Clouds", "papers": {}},
{"task": "3D Volumetric Reconstruction", "papers": {}},
{"task": "Learning Representation Of Multi-View Data", "papers": {}},
{"task": "Learning Network Representations", "papers": {}},
{"task": "3D Shape Recognition", "papers": {}},
{"task": "Learning Representation On Graph", "papers": {}},
{"task": "3D Shape Reconstruction From A Single 2D Image", "papers": {}},
{"task": "Sentence Embeddings For Biomedical Texts", "papers": {"0": "BioSentVec: creating sentence embeddings for biomedical texts", "1": "BioSentVec: creating sentence embeddings for biomedical texts"}},
{"task": "Learning Semantic Representations", "papers": {}},
{"paper": "Fully Convolutional Geometric Features", "abstract": "Extracting geometric features from 3D scans or point clouds is the first step in applications such as registration, reconstruction, and tracking. State-of-the-art methods require computing low-level features as input or extracting patch-based features with limited receptive field.In this work, we present fully-convolutional geometric features, computed in a single pass by a 3D fully-convolutional network. We also present new metric learning losses that dramatically improve performance. Fully-convolutional geometric features are compact, capture broad spatial context, and scale to large scenes. We experimentally validate our approach on both indoor and outdoor datasets. Fully-convolutional geometric features achieve state-of-the-art accuracy without requiring prepossessing, are compact (32 dimensions), and are 600 times faster than the most accurate prior method."},
{"task": "Document Embedding", "papers": {}},
{"task": "Learning Word Embeddings", "papers": {}},
{"task": "Swapped operands", "papers": {}},
{"task": "Multilingual Word Embeddings", "papers": {}},
{"task": "Exception type", "papers": {}},
{"task": "Sentence Embedding", "papers": {}},
{"task": "Knowledge Graph Embeddings", "papers": {}},
{"task": "Wrong binary operator", "papers": {}},
{"task": "Function-docstring mismatch", "papers": {}},
{"task": "Variable misuse", "papers": {}},
{"task": "Fault localization", "papers": {}},
{"task": "Program Repair", "papers": {"0": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"}},
{"paper": "BioSentVec: creating sentence embeddings for biomedical texts", "abstract": "Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedical texts to date.In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMIC-III Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different text genres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings. BioSentVec is publicly available at https://github.com/ncbi-nlp/BioSentVec"},
{"task": "Graph Representation Learning", "papers": {"0": "$\u03a0-$nets: Deep Polynomial Neural Networks"}},
{"task": "Knowledge Graph Embedding", "papers": {}},
{"task": "Unsupervised Anomaly Detection In Sound", "papers": {}},
{"task": "Graph Embedding", "papers": {}},
{"task": "Unsupervised Representation Learning", "papers": {}},
{"task": "Sentence Embeddings", "papers": {}},
{"task": "Network Embedding", "papers": {}},
{"task": "Representation Learning", "papers": {"0": "Dense Morphological Network: An Universal Function Approximator", "1": "Rumor Detection on Twitter with Tree-structured Recursive Neural Networks"}},
{"task": "Group Anomaly Detection", "papers": {}},
{"task": "Abnormal Event Detection In Video", "papers": {"0": "Abnormal Event Detection in Videos using Generative Adversarial Nets"}},
{"task": "Word Embeddings", "papers": {}},
{"task": "Anomaly Detection in Edge Streams", "papers": {"0": "MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams"}},
{"task": "Anomaly Detection", "papers": {"0": "Unsupervised real-time anomaly detection for streaming data", "1": "CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances", "2": "CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances", "3": "CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances", "4": "CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances", "5": "Deep Anomaly Detection with Deviation Networks", "6": "Deep Anomaly Detection with Deviation Networks", "7": "Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm", "8": "Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows"}},
{"task": "Anomaly Detection In Surveillance Videos", "papers": {}},
{"task": "Unsupervised Anomaly Detection", "papers": {"0": "Learning Representations from Healthcare Time Series Data for Unsupervised Anomaly Detection", "1": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "2": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "3": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "4": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection"}},
{"task": "Video Games", "papers": {}},
{"task": "League of Legends", "papers": {}},
{"task": "SNES Games", "papers": {"0": "Playing SNES in the Retro Learning Environment", "1": "Playing SNES in the Retro Learning Environment", "2": "Playing SNES in the Retro Learning Environment", "3": "Playing SNES in the Retro Learning Environment", "4": "Playing SNES in the Retro Learning Environment"}},
{"paper": "$\u03a0-$nets: Deep Polynomial Neural Networks", "abstract": "Deep Convolutional Neural Networks (DCNNs) is currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few).In this paper, we propose $\\Pi$-Nets, a new class of DCNNs. $\\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. $\\Pi$-Nets can be implemented using special kind of skip connections and their parameters can be represented via high-order tensors. We empirically demonstrate that $\\Pi$-Nets have better representation power than standard DCNNs and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\\Pi$-Nets produce state-of-the-art results in challenging tasks, such as image generation. Lastly, our framework elucidates why recent generative models, such as StyleGAN, improve upon their predecessors, e.g., ProGAN."},
{"paper": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback", "abstract": "We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback.Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best), and 48.4% synthesis success rate on SPoC (+3.7% over the prior best)."},
{"task": "Dota 2", "papers": {}},
{"task": "Real-Time Strategy Games", "papers": {}},
{"task": "Game of Doom", "papers": {"0": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning"}},
{"task": "Starcraft II", "papers": {"0": "StarCraft II: A New Challenge for Reinforcement Learning", "1": "StarCraft II: A New Challenge for Reinforcement Learning"}},
{"paper": "Abnormal Event Detection in Videos using Generative Adversarial Nets", "abstract": "In this paper we address the abnormality detection problem in crowded scenes. We propose to use Generative Adversarial Nets (GANs), which are trained using\nnormal frames and corresponding optical-flow images in order to learn an\ninternal representation of the scene normality.Since our GANs are trained with\nonly normal data, they are not able to generate abnormal events. At testing\ntime the real data are compared with both the appearance and the motion\nrepresentations reconstructed by our GANs and abnormal areas are detected by\ncomputing local differences. Experimental results on challenging abnormality\ndetection datasets show the superiority of the proposed method compared to the\nstate of the art in both frame-level and pixel-level abnormality detection\ntasks."},
{"paper": "MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams", "abstract": "Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? Existing approaches aim to detect individually surprising edges.In this work, we propose MIDAS, which focuses on detecting microcluster anomalies, or suddenly arriving groups of suspiciously similar edges, such as lockstep behavior, including denial of service attacks in network traffic data. MIDAS has the following properties: (a) it detects microcluster anomalies while providing theoretical guarantees about its false positive probability; (b) it is online, thus processing each edge in constant time and constant memory, and also processes the data 162-644 times faster than state-of-the-art approaches; (c) it provides 42%-48% higher accuracy (in terms of AUC) than state-of-the-art approaches."},
{"paper": "Rumor Detection on Twitter with Tree-structured Recursive Neural Networks", "abstract": "Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors.We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage."},
{"task": "3D Object Retrieval", "papers": {}},
{"task": "Face Reenactment", "papers": {}},
{"paper": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace.It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall."},
{"paper": "Learning Representations from Healthcare Time Series Data for Unsupervised Anomaly Detection", "abstract": "The amount of time series data generated in Healthcare is growing very fast and so is the need for methods that can analyse these data, detect anomalies and provide meaningful insights. However, most of the data available is unlabelled and, therefore, anomaly detection in this scenario has been a great challenge for researchers and practitioners.Recently, unsupervised representation learning with deep generative models has been applied to find representations of data, without the need for big labelled datasets. Motivated by their success, we propose an unsupervised framework for anomaly detection in time series data. In our method, both representation learning and anomaly detection are fully unsupervised. In addition, the training data may contain anomalous data. We first learn representations of time series using a Variational Recurrent Autoencoder. Afterwards, based on those representations, we detect anomalous time series using Clustering and the Wasserstein distance. Our results on the publicly available ECG5000 electrocardiogram dataset show the ability of the proposed approach to detect anomalous heartbeats in a fully unsupervised fashion, while providing structured and expressive data representations. Furthermore, our approach outperforms previous supervised and unsupervised methods on this dataset."},
{"paper": "Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm", "abstract": "A popular method for anomaly detection is to use the generator of an adversarial network to formulate anomaly scores over reconstruction loss of input. Due to the rare occurrence of anomalies, optimizing such networks can be a cumbersome task.Another possible approach is to use both generator and discriminator for anomaly detection. However, attributed to the involvement of adversarial training, this model is often unstable in a way that the performance fluctuates drastically with each training step. In this study, we propose a framework that effectively generates stable results across a wide range of training steps and allows us to use both the generator and the discriminator of an adversarial model for efficient and robust anomaly detection. Our approach transforms the fundamental role of a discriminator from identifying real and fake data to distinguishing between good and bad quality reconstructions. To this end, we prepare training examples for the good quality reconstruction by employing the current generator, whereas poor quality examples are obtained by utilizing an old state of the same generator. This way, the discriminator learns to detect subtle distortions that often appear in reconstructions of the anomaly inputs. Extensive experiments performed on Caltech-256 and MNIST image datasets for novelty detection show superior results. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our model achieves a frame-level AUC of 98.1%, surpassing recent state-of-the-art methods."},
{"paper": "Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows", "abstract": "The detection of manufacturing errors is crucial in fabrication processes to ensure product quality and safety standards. Since many defects occur very rarely and their characteristics are mostly unknown a priori, their detection is still an open research question.To this end, we propose DifferNet: It leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using normalizing flows. Normalizing flows are well-suited to deal with low dimensional data distributions. However, they struggle with the high dimensionality of images. Therefore, we employ a multi-scale feature extractor which enables the normalizing flow to assign meaningful likelihoods to the images. Based on these likelihoods we develop a scoring function that indicates defects. Moreover, propagating the score back to the image enables pixel-wise localization. To achieve a high robustness and performance we exploit multiple transformations in training and evaluation. In contrast to most other methods, ours does not require a large number of training samples and performs well with as low as 16 images. We demonstrate the superior performance over existing approaches on the challenging and newly proposed MVTec AD and Magnetic Tile Defects datasets."},
{"paper": "Deep Anomaly Detection with Deviation Networks", "abstract": "Although deep learning has been applied to successfully address many data mining problems, relatively limited work has been done on deep learning for anomaly detection. Existing deep anomaly detection methods, which focus on learning new feature representations to enable downstream anomaly detection methods, perform indirect optimization of anomaly scores, leading to data-inefficient learning and suboptimal anomaly scoring.Also, they are typically designed as unsupervised learning due to the lack of large-scale labeled anomaly data. As a result, they are difficult to leverage prior knowledge (e.g., a few labeled anomalies) when such information is available as in many real-world anomaly detection applications. This paper introduces a novel anomaly detection framework and its instantiation to address these problems. Instead of representation learning, our method fulfills an end-to-end learning of anomaly scores by a neural deviation learning, in which we leverage a few (e.g., multiple to dozens) labeled anomalies and a prior probability to enforce statistically significant deviations of the anomaly scores of anomalies from that of normal data objects in the upper tail. Extensive results show that our method can be trained substantially more data-efficiently and achieves significantly better anomaly scoring than state-of-the-art competing methods."},
{"task": "Atari Games", "papers": {"0": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning", "1": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "2": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "3": "Agent57: Outperforming the Atari Human Benchmark", "4": "Agent57: Outperforming the Atari Human Benchmark", "5": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "6": "Agent57: Outperforming the Atari Human Benchmark", "7": "Agent57: Outperforming the Atari Human Benchmark", "8": "First return then explore", "9": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "10": "Agent57: Outperforming the Atari Human Benchmark", "11": "Recurrent Experience Replay in Distributed Reinforcement Learning", "12": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "13": "Agent57: Outperforming the Atari Human Benchmark", "14": "Dueling Network Architectures for Deep Reinforcement Learning", "15": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "16": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "17": "Agent57: Outperforming the Atari Human Benchmark", "18": "A Distributional Perspective on Reinforcement Learning", "19": "First return then explore", "20": "Recurrent Experience Replay in Distributed Reinforcement Learning", "21": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "22": "Agent57: Outperforming the Atari Human Benchmark", "23": "Agent57: Outperforming the Atari Human Benchmark", "24": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "25": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "26": "First return then explore", "27": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "28": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "29": "Agent57: Outperforming the Atari Human Benchmark", "30": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "31": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "32": "Self-Imitation Learning", "33": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "34": "Recurrent Experience Replay in Distributed Reinforcement Learning", "35": "Agent57: Outperforming the Atari Human Benchmark", "36": "Recurrent Experience Replay in Distributed Reinforcement Learning", "37": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "38": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "39": "Agent57: Outperforming the Atari Human Benchmark", "40": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "41": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "42": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "43": "Agent57: Outperforming the Atari Human Benchmark", "44": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "45": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "46": "Distributed Prioritized Experience Replay", "47": "Recurrent Experience Replay in Distributed Reinforcement Learning", "48": "First return then explore", "49": "Go-Explore: a New Approach for Hard-Exploration Problems", "50": "Evolving simple programs for playing Atari games", "51": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "52": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "53": "Agent57: Outperforming the Atari Human Benchmark", "54": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "55": "Agent57: Outperforming the Atari Human Benchmark", "56": "Noisy Networks for Exploration", "57": "Increasing the Action Gap: New Operators for Reinforcement Learning", "58": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "59": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "60": "The Arcade Learning Environment: An Evaluation Platform for General Agents"}},
{"paper": "CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances", "abstract": "Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation.In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets. Code and pre-trained models are available at https://github.com/alinlab/CSI."},
{"paper": "Unsupervised real-time anomaly detection for streaming data", "abstract": "We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities.One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics."},
{"paper": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these\nattributes may be acquired naturally by human players, teaching them to a\ncomputer program is a far more challenging task.In recent years, extensive\nresearch was carried out in the field of reinforcement learning and numerous\nalgorithms were introduced, aiming to learn how to perform human tasks such as\nplaying video games. As a result, the Arcade Learning Environment (ALE)\n(Bellemare et al., 2013) has become a commonly used benchmark environment\nallowing algorithms to train on various Atari 2600 games. In many games the\nstate-of-the-art algorithms outperform humans. In this paper we introduce a new\nlearning environment, the Retro Learning Environment --- RLE, that can run\ngames on the Super Nintendo Entertainment System (SNES), Sega Genesis and\nseveral other gaming consoles. The environment is expandable, allowing for more\nvideo games and consoles to be easily added to the environment, while\nmaintaining the same interface as ALE. Moreover, RLE is compatible with Python\nand Torch. SNES games pose a significant challenge to current algorithms due to\ntheir higher level of complexity and versatility."},
{"paper": "StarCraft II: A New Challenge for Reinforcement Learning", "abstract": "This paper introduces SC2LE (StarCraft II Learning Environment), a\nreinforcement learning environment based on the StarCraft II game. This domain\nposes a new grand challenge for reinforcement learning, representing a more\ndifficult class of problems than considered in most prior work.It is a\nmulti-agent problem with multiple players interacting; there is imperfect\ninformation due to a partially observed map; it has a large action space\ninvolving the selection and control of hundreds of units; it has a large state\nspace that must be observed solely from raw input feature planes; and it has\ndelayed credit assignment requiring long-term strategies over thousands of\nsteps. We describe the observation, action, and reward specification for the\nStarCraft II domain and provide an open source Python-based interface for\ncommunicating with the game engine. In addition to the main game maps, we\nprovide a suite of mini-games focusing on different elements of StarCraft II\ngameplay. For the main game maps, we also provide an accompanying dataset of\ngame replay data from human expert players. We give initial baseline results\nfor neural networks trained from this data to predict game outcomes and player\nactions. Finally, we present initial baseline results for canonical deep\nreinforcement learning agents applied to the StarCraft II domain. On the\nmini-games, these agents learn to achieve a level of play that is comparable to\na novice player. However, when trained on the main game, these agents are\nunable to make significant progress. Thus, SC2LE offers a new and challenging\nenvironment for exploring deep reinforcement learning algorithms and\narchitectures."},
{"paper": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning", "abstract": "The recent advances in deep neural networks have led to effective\nvision-based reinforcement learning methods that have been employed to obtain\nhuman-level controllers in Atari 2600 games from pixel data. Atari 2600 games,\nhowever, do not resemble real-world tasks since they involve non-realistic 2D\nenvironments and the third-person perspective.Here, we propose a novel\ntest-bed platform for reinforcement learning research from raw visual\ninformation which employs the first-person perspective in a semi-realistic 3D\nworld. The software, called ViZDoom, is based on the classical first-person\nshooter video game, Doom. It allows developing bots that play the game using\nthe screen buffer. ViZDoom is lightweight, fast, and highly customizable via a\nconvenient mechanism of user scenarios. In the experimental part, we test the\nenvironment by trying to learn bots for two scenarios: a basic move-and-shoot\ntask and a more complex maze-navigation problem. Using convolutional deep\nneural networks with Q-learning and experience replay, for both scenarios, we\nwere able to train competent bots, which exhibit human-like behaviors. The\nresults confirm the utility of ViZDoom as an AI research platform and imply\nthat visual reinforcement learning in 3D realistic first-person perspective\nenvironments is feasible."},
{"paper": "Dense Morphological Network: An Universal Function Approximator", "abstract": "Artificial neural networks are built on the basic operation of linear combination and non-linear activation function. Theoretically this structure can approximate any continuous function with three layer architecture.But in practice learning the parameters of such network can be hard. Also the choice of activation function can greatly impact the performance of the network. In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function. To this end we are proposing the use of elementary morphological operations (dilation and erosion) as the basic operation in neurons. We show that these networks (Denoted as DenMo-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks. The results show that our network perform favorably when compared with similar structured network."},
{"task": "FPS Games", "papers": {}},
{"task": "3D Scene Reconstruction", "papers": {}},
{"task": "3D Point Cloud Matching", "papers": {"0": "3D-CODED : 3D Correspondences by Deep Deformation"}},
{"task": "3D Shape Generation", "papers": {}},
{"task": "3D Depth Estimation", "papers": {}},
{"task": "3D FACE MODELING", "papers": {}},
{"task": "Neural Rendering", "papers": {}},
{"task": "3D Shape Classification", "papers": {"0": "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling"}},
{"paper": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning", "abstract": "Count-based exploration algorithms are known to perform near-optimally when\nused in conjunction with tabular reinforcement learning (RL) methods for\nsolving small discrete Markov decision processes (MDPs). It is generally\nthought that count-based methods cannot be applied in high-dimensional state\nspaces, since most states will only occur once.Recent deep RL exploration\nstrategies are able to deal with high-dimensional continuous state spaces\nthrough complex heuristics, often relying on optimism in the face of\nuncertainty or intrinsic motivation. In this work, we describe a surprising\nfinding: a simple generalization of the classic count-based approach can reach\nnear state-of-the-art performance on various high-dimensional and/or continuous\ndeep RL benchmarks. States are mapped to hash codes, which allows to count\ntheir occurrences with a hash table. These counts are then used to compute a\nreward bonus according to the classic count-based exploration theory. We find\nthat simple hash functions can achieve surprisingly good results on many\nchallenging tasks. Furthermore, we show that a domain-dependent learned hash\ncode may further improve these results. Detailed analysis reveals important\naspects of a good hash function: 1) having appropriate granularity and 2)\nencoding information relevant to solving the MDP. This exploration strategy\nachieves near state-of-the-art performance on both continuous control tasks and\nAtari 2600 games, hence providing a simple yet powerful baseline for solving\nMDPs that require considerable exploration."},
{"task": "3D Object Classification", "papers": {"0": "3D Point Capsule Networks", "1": "Orientation-boosted Voxel Nets for 3D Object Recognition"}},
{"paper": "Go-Explore: a New Approach for Hard-Exploration Problems", "abstract": "A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall.On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of \"superhuman\" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics)."},
{"task": "No-Reference Image Quality Assessment", "papers": {"0": "Music Data Analysis: A State-of-the-art Survey"}},
{"task": "Talking Head Generation", "papers": {"0": "Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars", "1": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "2": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "3": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "4": "Pose Manipulation with Identity Preservation", "5": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "6": "0-Step Capturability, Motion Decomposition and Global Feedback Control of the 3D Variable Height-Inverted Pendulum"}},
{"paper": "3D-CODED : 3D Correspondences by Deep Deformation", "abstract": "We present a new deep learning approach for matching deformable shapes by\nintroducing {\\it Shape Deformation Networks} which jointly encode 3D shapes and\ncorrespondences. This is achieved by factoring the surface representation into\n(i) a template, that parameterizes the surface, and (ii) a learnt global\nfeature vector that parameterizes the transformation of the template into the\ninput surface.By predicting this feature for a new shape, we implicitly\npredict correspondences between this shape and the template. We show that these\ncorrespondences can be improved by an additional step which improves the shape\nfeature by minimizing the Chamfer distance between the input and transformed\ntemplate. We demonstrate that our simple approach improves on state-of-the-art\nresults on the difficult FAUST-inter challenge, with an average correspondence\nerror of 2.88cm. We show, on the TOSCA dataset, that our method is robust to\nmany types of perturbations, and generalizes to non-human shapes. This\nrobustness allows it to perform well on real unclean, meshes from the the SCAPE\ndataset."},
{"paper": "Evolving simple programs for playing Atari games", "abstract": "Cartesian Genetic Programming (CGP) has previously shown capabilities in\nimage processing tasks by evolving programs with a function set specialized for\ncomputer vision. A similar approach can be applied to Atari playing.Programs\nare evolved using mixed type CGP with a function set suited for matrix\noperations, including image processing, but allowing for controller behavior to\nemerge. While the programs are relatively small, many controllers are\ncompetitive with state of the art methods for the Atari benchmark set and\nrequire less training time. By evaluating the programs of the best evolved\nindividuals, simple but effective strategies can be found."},
{"paper": "Noisy Networks for Exploration", "abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights.NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance."},
{"paper": "Self-Imitation Learning", "abstract": "This paper proposes Self-Imitation Learning (SIL), a simple off-policy\nactor-critic algorithm that learns to reproduce the agent's past good\ndecisions. This algorithm is designed to verify our hypothesis that exploiting\npast good experiences can indirectly drive deep exploration.Our empirical\nresults show that SIL significantly improves advantage actor-critic (A2C) on\nseveral hard exploration Atari games and is competitive to the state-of-the-art\ncount-based exploration methods. We also show that SIL improves proximal policy\noptimization (PPO) on MuJoCo tasks."},
{"paper": "Distributed Prioritized Experience Replay", "abstract": "We propose a distributed architecture for deep reinforcement learning at\nscale, that enables agents to learn effectively from orders of magnitude more\ndata than previously possible. The algorithm decouples acting from learning:\nthe actors interact with their own instances of the environment by selecting\nactions according to a shared neural network, and accumulate the resulting\nexperience in a shared experience replay memory; the learner replays samples of\nexperience and updates the neural network.The architecture relies on\nprioritized experience replay to focus only on the most significant data\ngenerated by the actors. Our architecture substantially improves the state of\nthe art on the Arcade Learning Environment, achieving better final performance\nin a fraction of the wall-clock training time."},
{"paper": "Increasing the Action Gap: New Operators for Reinforcement Learning", "abstract": "This paper introduces new optimality-preserving operators on Q-functions. We\nfirst describe an operator for tabular representations, the consistent Bellman\noperator, which incorporates a notion of local policy consistency.We show that\nthis local consistency leads to an increase in the action gap at each state;\nincreasing this gap, we argue, mitigates the undesirable effects of\napproximation and estimation errors on the induced greedy policies. This\noperator can also be applied to discretized continuous space and time problems,\nand we provide empirical results evidencing superior performance in this\ncontext. Extending the idea of a locally consistent operator, we then derive\nsufficient conditions for an operator to preserve optimality, leading to a\nfamily of operators which includes our consistent Bellman operator. As\ncorollaries we provide a proof of optimality for Baird's advantage learning\nalgorithm and derive other gap-increasing operators with interesting\nproperties. We conclude with an empirical study on 60 Atari 2600 games\nillustrating the strong potential of these new operators."},
{"paper": "A Distributional Perspective on Reinforcement Learning", "abstract": "In this paper we argue for the fundamental importance of the value\ndistribution: the distribution of the random return received by a reinforcement\nlearning agent. This is in contrast to the common approach to reinforcement\nlearning which models the expectation of this return, or value.Although there\nis an established body of literature studying the value distribution, thus far\nit has always been used for a specific purpose such as implementing risk-aware\nbehaviour. We begin with theoretical results in both the policy evaluation and\ncontrol settings, exposing a significant distributional instability in the\nlatter. We then use the distributional perspective to design a new algorithm\nwhich applies Bellman's equation to the learning of approximate value\ndistributions. We evaluate our algorithm using the suite of games from the\nArcade Learning Environment. We obtain both state-of-the-art results and\nanecdotal evidence demonstrating the importance of the value distribution in\napproximate reinforcement learning. Finally, we combine theoretical and\nempirical evidence to highlight the ways in which the value distribution\nimpacts learning in the approximate setting."},
{"paper": "First return then explore", "abstract": "The promise of reinforcement learning is to solve complex sequential decision problems by specifying a high-level reward function only. However, RL algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback.Avoiding these pitfalls requires thoroughly exploring the environment, but despite substantial investments by the community, creating algorithms that can do so remains one of the central challenges of the field. We hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (\"detachment\") and from failing to first return to a state before exploring from it (\"derailment\"). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before exploring. Go-Explore solves all heretofore unsolved Atari games (those for which algorithms could not previously outperform humans when evaluated following current community standards) and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a challenging and extremely sparse-reward robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The striking contrast between the substantial performance gains from Go-Explore and the simplicity of its mechanisms suggests that remembering promising states, returning to them, and exploring from them is a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents."},
{"paper": "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling", "abstract": "We study 3D shape modeling from a single image and make contributions to it\nin three aspects. First, we present Pix3D, a large-scale benchmark of diverse\nimage-shape pairs with pixel-level 2D-3D alignment.Pix3D has wide applications\nin shape-related tasks including reconstruction, retrieval, viewpoint\nestimation, etc. Building such a large-scale dataset, however, is highly\nchallenging; existing datasets either contain only synthetic data, or lack\nprecise alignment between 2D images and 3D shapes, or only have a small number\nof images. Second, we calibrate the evaluation criteria for 3D shape\nreconstruction through behavioral studies, and use them to objectively and\nsystematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction\nand pose estimation; our multi-task learning approach achieves state-of-the-art\nperformance on both tasks."},
{"paper": "Recurrent Experience Replay in Distributed Reinforcement Learning", "abstract": "Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy.Using a single network architecture and fixed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and surpasses the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the\n57 Atari games."},
{"paper": "Music Data Analysis: A State-of-the-art Survey", "abstract": "Music accounts for a significant chunk of interest among various online\nactivities. This is reflected by wide array of alternatives offered in music\nrelated web/mobile apps, information portals, featuring millions of artists,\nsongs and events attracting user activity at similar scale.Availability of\nlarge scale structured and unstructured data has attracted similar level of\nattention by data science community. This paper attempts to offer current\nstate-of-the-art in music related analysis. Various approaches involving\nmachine learning, information theory, social network analysis, semantic web and\nlinked open data are represented in the form of taxonomy along with data\nsources and use cases addressed by the research community."},
{"paper": "Orientation-boosted Voxel Nets for 3D Object Recognition", "abstract": "Recent work has shown good recognition results in 3D object recognition using\n3D convolutional networks. In this paper, we show that the object orientation\nplays an important role in 3D recognition.More specifically, we argue that\nobjects induce different features in the network under rotation. Thus, we\napproach the category-level classification task as a multi-task problem, in\nwhich the network is trained to predict the pose of the object in addition to\nthe class label as a parallel task. We show that this yields significant\nimprovements in the classification results. We test our suggested architecture\non several datasets representing various 3D data sources: LiDAR data, CAD\nmodels, and RGB-D images. We report state-of-the-art results on classification\nas well as significant improvements in precision and speed over the baseline on\n3D detection."},
{"paper": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "abstract": "In this article we introduce the Arcade Learning Environment (ALE): both a\nchallenge problem and a platform and methodology for evaluating the development\nof general, domain-independent AI technology. ALE provides an interface to\nhundreds of Atari 2600 game environments, each one different, interesting, and\ndesigned to be a challenge for human players.ALE presents significant research\nchallenges for reinforcement learning, model learning, model-based planning,\nimitation learning, transfer learning, and intrinsic motivation. Most\nimportantly, it provides a rigorous testbed for evaluating and comparing\napproaches to these problems. We illustrate the promise of ALE by developing\nand benchmarking domain-independent agents designed using well-established AI\ntechniques for both reinforcement learning and planning. In doing so, we also\npropose an evaluation methodology made possible by ALE, reporting empirical\nresults on over 55 different games. All of the software, including the\nbenchmark agents, is publicly available."},
{"paper": "Pose Manipulation with Identity Preservation", "abstract": "This paper describes a new model which generates images in novel poses e.g. by altering face expression and orientation, from just a few instances of a human subject. Unlike previous approaches which require large datasets of a specific person for training, our approach may start from a scarce set of images, even from a single image.To this end, we introduce Character Adaptive Identity Normalization GAN (CainGAN) which uses spatial characteristic features extracted by an embedder and combined across source images. The identity information is propagated throughout the network by applying conditional normalization. After extensive adversarial training, CainGAN receives figures of faces from a certain individual and produces new ones while preserving the person's identity. Experimental results show that the quality of generated images scales with the size of the input set used during inference. Furthermore, quantitative measurements indicate that CainGAN performs better compared to other methods when training data is limited."},
{"paper": "0-Step Capturability, Motion Decomposition and Global Feedback Control of the 3D Variable Height-Inverted Pendulum", "abstract": "One common method for stabilizing robots after a push is the Instantaneous Capture Point, however, this has the fundamental limitation of assuming constant height. Although there are several works for balancing bipedal robots including height variations in 2D, the amount of literature on 3D models is limited.There are optimization methods using variable Center of Pressure (CoP) and reaction force to the ground, although they do not provide the physical region where a robot can step and require a precomputation for the analysis. This work provides the necessary and sufficient conditions to maintain balance of the 3D Variable Height Inverted Pendulum (VHIP) with both, fixed and variable CoP. We also prove that the 3D VHIP with Fixed CoP is the same as its 2D version, and we generalize controllers working on the 2D VHIP to the 3D VHIP. We also show the generalization of the Divergent Component of Motion to the 3D VHIP and we provide an alternative motion decomposition for the analysis of height and CoP strategies independently. This allow us to generalize previous global feedback controllers done in the 2D VHIP to the 3D VHIP with a Variable CoP."},
{"paper": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "abstract": "Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person.However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings."},
{"paper": "Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars", "abstract": "We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person's appearance by decomposing it into two layers.The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system."},
{"paper": "3D Point Capsule Networks", "abstract": "In this paper, we propose 3D point-capsule networks, an auto-encoder designed to process sparse 3D point clouds while preserving spatial arrangements of the input data. 3D capsule networks arise as a direct consequence of our novel unified 3D auto-encoder formulation.Their dynamic routing scheme and the peculiar 2D latent space deployed by our approach bring in improvements for several common point cloud-related tasks, such as object classification, object reconstruction and part segmentation as substantiated by our extensive evaluations. Moreover, it enables new applications such as part interpolation and replacement."},
{"paper": "Agent57: Outperforming the Atari Human Benchmark", "abstract": "Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms.Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning."},
{"subfield": "Activity Recognition", "tasks": {"0": "Action Recognition", "1": "Activity Recognition", "2": "Multimodal Activity Recognition", "3": "Egocentric Activity Recognition", "4": "Group Activity Recognition", "5": "Human action generation", "6": "RF-based Action Recognition", "7": "Cross-Domain Activity Recognition", "8": "Concurrent Activity Recognition", "9": "Recognizing And Localizing Human Actions"}},
{"paper": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available.However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules."},
{"task": "3D Shape Representation", "papers": {}},
{"subfield": "Semantic Segmentation", "tasks": {"0": "Semantic Segmentation", "1": "Tumor Segmentation", "2": "Real-Time Semantic Segmentation", "3": "Scene Segmentation", "4": "3D Semantic Segmentation", "5": "Panoptic Segmentation", "6": "Weakly-Supervised Semantic Segmentation", "7": "3D Part Segmentation", "8": "Semi-Supervised Semantic Segmentation", "9": "One-Shot Segmentation", "10": "indoor scene understanding", "11": "Unsupervised Semantic Segmentation", "12": "4D Spatio Temporal Semantic Segmentation", "13": "Room Layout Estimation", "14": "Spinal Cord Gray Matter - Segmentation", "15": "Attentive segmentation networks", "16": "UNET Segmentation", "17": "Histopathological Segmentation", "18": "Road Segementation"}},
{"task": "3D Shape Reconstruction", "papers": {"0": "AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation"}},
{"subfield": "Style Transfer", "tasks": {"0": "Style Transfer", "1": "Image Stylization", "2": "Music Genre Transfer", "3": "Style Generalization", "4": "Font Style Transfer", "5": "Face Transfer", "6": "Music Style Transfer", "7": "Reverse Style Transfer", "8": "Serial Style Transfer"}},
{"subfield": "Speech Recognition", "tasks": {"0": "Speech Recognition", "1": "Visual Speech Recognition", "2": "Distant Speech Recognition", "3": "Sequence-To-Sequence Speech Recognition", "4": "Robust Speech Recognition", "5": "Noisy Speech Recognition", "6": "Accented Speech Recognition", "7": "English Conversational Speech Recognition"}},
{"task": "3D Pose Estimation", "papers": {"0": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"}},
{"subfield": "Image Retrieval", "tasks": {"0": "Visual Object Tracking", "1": "Multi-Object Tracking", "2": "Multiple Object Tracking", "3": "Online Multi-Object Tracking", "4": "Thermal Infrared Object Tracking", "5": "Video Object Tracking"}},
{"subfield": "Dimensionality Reduction", "tasks": {"0": "Emotion Recognition", "1": "Multimodal Emotion Recognition", "2": "Speech Emotion Recognition", "3": "Emotion Recognition in Conversation", "4": "Emotion-Cause Pair Extraction", "5": "Emotion Cause Extraction", "6": "Video Emotion Recognition", "7": "Emotion Recognition in Context"}},
{"task": "3D Reconstruction", "papers": {"0": "Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images", "1": "Scan2CAD: Learning CAD Model Alignment in RGB-D Scans", "2": "What Do Single-view 3D Reconstruction Networks Learn?", "3": "Atlas: End-to-End 3D Scene Reconstruction from Posed Images"}},
{"subfield": "Video Classification", "tasks": {"0": "Style Transfer", "1": "Image Stylization", "2": "Music Genre Transfer", "3": "Style Generalization", "4": "Font Style Transfer", "5": "Face Transfer", "6": "Music Style Transfer", "7": "Reverse Style Transfer", "8": "Serial Style Transfer"}},
{"subfield": "Anomaly Detection", "tasks": {"0": "Person Re-Identification", "1": "Unsupervised Person Re-Identification", "2": "Video-Based Person Re-Identification", "3": "Large-Scale Person Re-Identification", "4": "Generalizable Person Re-identification", "5": "Image-To-Video Person Re-Identification", "6": "Semi-Supervised Person Re-Identification", "7": "Cross-Modal  Person Re-Identification"}},
{"subfield": "Zero-Shot Learning", "tasks": {"0": "Scene Understanding", "1": "Scene Parsing", "2": "Scene Text Recognition", "3": "Scene Recognition", "4": "Scene Graph Generation", "5": "Street Scene Parsing", "6": "Scene Labeling", "7": "Indoor Scene Synthesis", "8": "Indoor Scene Reconstruction"}},
{"subfield": "Few-Shot Learning", "tasks": {"0": "3D Reconstruction", "1": "3D Pose Estimation", "2": "3D Shape Reconstruction", "3": "3D Shape Representation", "4": "No-Reference Image Quality Assessment", "5": "3D Object Classification", "6": "3D Shape Classification", "7": "Neural Rendering", "8": "3D FACE MODELING", "9": "3D Shape Generation", "10": "3D Scene Reconstruction", "11": "3D Point Cloud Matching", "12": "FPS Games", "13": "3D Depth Estimation", "14": "Talking Head Generation", "15": "Face Reenactment", "16": "3D Object Retrieval", "17": "3D Shape Recognition", "18": "3D Shape Reconstruction From A Single 2D Image", "19": "3D Volumetric Reconstruction", "20": "Classify 3D Point Clouds", "21": "Generating 3D Point Clouds", "22": "3D Feature Matching", "23": "3D Geometry Perception", "24": "3D Plane Detection", "25": "3D Surface Generation", "26": "Point Set Upsampling", "27": "3D Object Detection From Monocular Images", "28": "Underwater 3D Scene Reconstruction", "29": "Multi-View 3D Shape Retrieval", "30": "3D"}},
{"subfield": "Action Recognition", "tasks": {"0": "Instance Segmentation", "1": "3D Instance Segmentation", "2": "Real-time Instance Segmentation", "3": "3D Semantic Instance Segmentation", "4": "Human Instance Segmentation", "5": "One-Shot Instance Segmentation"}},
{"task": "Recognizing And Localizing Human Actions", "papers": {}},
{"task": "RF-based Action Recognition", "papers": {}},
{"task": "Concurrent Activity Recognition", "papers": {}},
{"task": "Cross-Domain Activity Recognition", "papers": {}},
{"task": "Human action generation", "papers": {"0": "Structure-Aware Human-Action Generation", "1": "Structure-Aware Human-Action Generation"}},
{"task": "Serial Style Transfer", "papers": {}},
{"task": "Reverse Style Transfer", "papers": {}},
{"task": "Music Style Transfer", "papers": {}},
{"task": "Face Transfer", "papers": {}},
{"task": "Font Style Transfer", "papers": {}},
{"paper": "Atlas: End-to-End 3D Scene Reconstruction from Posed Images", "abstract": "We present an end-to-end 3D reconstruction method for a scene by directly regressing a truncated signed distance function (TSDF) from a set of posed RGB images. Traditional approaches to 3D reconstruction rely on an intermediate representation of depth maps prior to estimating a full 3D model of a scene.We hypothesize that a direct regression to 3D is more effective. A 2D CNN extracts features from each image independently which are then back-projected and accumulated into a voxel volume using the camera intrinsics and extrinsics. After accumulation, a 3D CNN refines the accumulated features and predicts the TSDF values. Additionally, semantic segmentation of the 3D model is obtained without significant computation. This approach is evaluated on the Scannet dataset where we significantly outperform state-of-the-art baselines (deep multiview stereo followed by traditional TSDF fusion) both quantitatively and qualitatively. We compare our 3D semantic segmentation to prior methods that use a depth sensor since no previous work attempts the problem with only RGB input."},
{"task": "Cross-Modal  Person Re-Identification", "papers": {}},
{"paper": "Dueling Network Architectures for Deep Reinforcement Learning", "abstract": "In recent years there have been many successes of using deep representations\nin reinforcement learning. Still, many of these applications use conventional\narchitectures, such as convolutional networks, LSTMs, or auto-encoders.In this\npaper, we present a new neural network architecture for model-free\nreinforcement learning. Our dueling network represents two separate estimators:\none for the state value function and one for the state-dependent action\nadvantage function. The main benefit of this factoring is to generalize\nlearning across actions without imposing any change to the underlying\nreinforcement learning algorithm. Our results show that this architecture leads\nto better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the\nstate-of-the-art on the Atari 2600 domain."},
{"task": "Indoor Scene Reconstruction", "papers": {}},
{"task": "Video Object Tracking", "papers": {}},
{"task": "Indoor Scene Synthesis", "papers": {}},
{"task": "One-Shot Instance Segmentation", "papers": {"0": "One-Shot Instance Segmentation"}},
{"task": "Human Instance Segmentation", "papers": {"0": "Pose2Seg: Detection Free Human Instance Segmentation"}},
{"task": "3D Semantic Instance Segmentation", "papers": {"0": "3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation", "1": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation"}},
{"paper": "Structure-Aware Human-Action Generation", "abstract": "Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions.Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, well-capturing the structure information in action sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods."},
{"task": "Real-time Instance Segmentation", "papers": {"0": "YOLACT: Real-time Instance Segmentation"}},
{"task": "Scene Labeling", "papers": {}},
{"task": "Instance Segmentation", "papers": {"0": "DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution", "1": "ResNeSt: Split-Attention Networks", "2": "PolyTransform: Deep Polygon Transformer for Instance Segmentation", "3": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation"}},
{"task": "3D Instance Segmentation", "papers": {"0": "OccuSeg: Occupancy-aware 3D Instance Segmentation", "1": "Learning Gaussian Instance Segmentation in Point Clouds", "2": "OccuSeg: Occupancy-aware 3D Instance Segmentation", "3": "MASC: Multi-scale Affinity with Sparse Convolution for 3D Instance Segmentation"}},
{"task": "Street Scene Parsing", "papers": {}},
{"task": "Scene Graph Generation", "papers": {"0": "Unbiased Scene Graph Generation from Biased Training", "1": "Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation"}},
{"task": "Semi-Supervised Person Re-Identification", "papers": {}},
{"task": "Image-To-Video Person Re-Identification", "papers": {}},
{"task": "Scene Parsing", "papers": {"0": "Variational Context-Deformable ConvNets for Indoor Scene Parsing"}},
{"task": "Scene Recognition", "papers": {"0": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "1": "When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition"}},
{"task": "Scene Text Recognition", "papers": {}},
{"paper": "One-Shot Instance Segmentation", "abstract": "We tackle the problem of one-shot instance segmentation: Given an example image of a novel, previously unknown object category, find and segment all objects of this category within a complex scene. To address this challenging new task, we propose Siamese Mask R-CNN.It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We demonstrate empirical results on MS Coco highlighting challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories works very well, targeting the detection network towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research into more powerful and flexible scene analysis algorithms. Code is available at: https://github.com/bethgelab/siamese-mask-rcnn"},
{"task": "Scene Understanding", "papers": {"0": "Context Prior for Scene Segmentation"}},
{"task": "Generalizable Person Re-identification", "papers": {"0": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "1": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "2": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "3": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "4": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "5": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting"}},
{"paper": "Pose2Seg: Detection Free Human Instance Segmentation", "abstract": "The standard approach to image instance segmentation is to perform the object\ndetection first, and then segment the object from the detection bounding-box. More recently, deep learning methods like Mask R-CNN perform them jointly.However, little research takes into account the uniqueness of the \"human\"\ncategory, which can be well defined by the pose skeleton. Moreover, the human\npose skeleton can be used to better distinguish instances with heavy occlusion\nthan using bounding-boxes. In this paper, we present a brand new pose-based\ninstance segmentation framework for humans which separates instances based on\nhuman pose, rather than proposal region detection. We demonstrate that our\npose-based framework can achieve better accuracy than the state-of-art\ndetection-based approach on the human instance segmentation problem, and can\nmoreover better handle occlusion. Furthermore, there are few public datasets\ncontaining many heavily occluded humans along with comprehensive annotations,\nwhich makes this a challenging problem seldom noticed by researchers. Therefore, in this paper we introduce a new benchmark \"Occluded Human\n(OCHuman)\", which focuses on occluded humans with comprehensive annotations\nincluding bounding-box, human pose and instance masks. This dataset contains\n8110 detailed annotated human instances within 4731 images. With an average\n0.67 MaxIoU for each person, OCHuman is the most complex and challenging\ndataset related to human instance segmentation. Through this dataset, we want\nto emphasize occlusion as a challenging problem for researchers to study."},
{"paper": "3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation", "abstract": "We present 3D-MPA, a method for instance segmentation on 3D point clouds. Given an input point cloud, we propose an object-centric approach where each point votes for its object center.We sample object proposals from the predicted object centers. Then, we learn proposal features from grouped point features that voted for the same object center. A graph convolutional network introduces inter-proposal relations, providing higher-level feature learning in addition to the lower-level point features. Each proposal comprises a semantic label, a set of associated points over which we define a foreground-background mask, an objectness score and aggregation features. Previous works usually perform non-maximum-suppression (NMS) over proposals to obtain the final object detections or semantic instances. However, NMS can discard potentially correct predictions. Instead, our approach keeps all proposals and groups them together based on the learned aggregation features. We show that grouping proposals improves over NMS and outperforms previous state-of-the-art methods on the tasks of 3D object detection and semantic instance segmentation on the ScanNetV2 benchmark and the S3DIS dataset."},
{"task": "Large-Scale Person Re-Identification", "papers": {}},
{"paper": "MASC: Multi-scale Affinity with Sparse Convolution for 3D Instance Segmentation", "abstract": "We propose a new approach for 3D instance segmentation based on sparse\nconvolution and point affinity prediction, which indicates the likelihood of\ntwo points belonging to the same instance. The proposed network, built upon\nsubmanifold sparse convolution [3], processes a voxelized point cloud and\npredicts semantic scores for each occupied voxel as well as the affinity\nbetween neighboring voxels at different scales.A simple yet effective\nclustering algorithm segments points into instances based on the predicted\naffinity and the mesh topology. The semantic for each instance is determined by\nthe semantic prediction. Experiments show that our method outperforms the\nstate-of-the-art instance segmentation methods by a large margin on the widely\nused ScanNet benchmark [2]. We share our code publicly at\nhttps://github.com/art-programmer/MASC."},
{"paper": "Learning Gaussian Instance Segmentation in Point Clouds", "abstract": "This paper presents a novel method for instance segmentation of 3D point clouds. The proposed method is called Gaussian Instance Center Network (GICN), which can approximate the distributions of instance centers scattered in the whole scene as Gaussian center heatmaps.Based on the predicted heatmaps, a small number of center candidates can be easily selected for the subsequent predictions with efficiency, including i) predicting the instance size of each center to decide a range for extracting features, ii) generating bounding boxes for centers, and iii) producing the final instance masks. GICN is a single-stage, anchor-free, and end-to-end architecture that is easy to train and efficient to perform inference. Benefited from the center-dictated mechanism with adaptive instance size selection, our method achieves state-of-the-art performance in the task of 3D instance segmentation on ScanNet and S3DIS datasets."},
{"paper": "Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation", "abstract": "Generating scene graph to describe all the relations inside an image gains\nincreasing interests these years. However, most of the previous methods use\ncomplicated structures with slow inference speed or rely on the external data,\nwhich limits the usage of the model in real-life scenarios.To improve the\nefficiency of scene graph generation, we propose a subgraph-based connection\ngraph to concisely represent the scene graph during the inference. A bottom-up\nclustering method is first used to factorize the entire scene graph into\nsubgraphs, where each subgraph contains several objects and a subset of their\nrelationships. By replacing the numerous relationship representations of the\nscene graph with fewer subgraph and object features, the computation in the\nintermediate stage is significantly reduced. In addition, spatial information\nis maintained by the subgraph features, which is leveraged by our proposed\nSpatial-weighted Message Passing~(SMP) structure and Spatial-sensitive Relation\nInference~(SRI) module to facilitate the relationship recognition. On the\nrecent Visual Relationship Detection and Visual Genome datasets, our method\noutperforms the state-of-the-art method in both accuracy and speed."},
{"paper": "YOLACT: Real-time Instance Segmentation", "abstract": "We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU.We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty."},
{"task": "Video-Based Person Re-Identification", "papers": {}},
{"task": "Unsupervised Person Re-Identification", "papers": {"0": "Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification", "1": "Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification", "2": "Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification", "3": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification", "4": "Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification", "5": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification", "6": "Learning Generalisable Omni-Scale Representations for Person Re-Identification", "7": "Learning Generalisable Omni-Scale Representations for Person Re-Identification"}},
{"paper": "Unbiased Scene Graph Generation from Biased Training", "abstract": "Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse \"human walk on / sit on / lay on beach\" into \"human on beach\". Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects.However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., \"person read book\" rather than \"eat\") and bad long-tailed bias (e.g., \"near\" dominating \"behind / in front of\"). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect (TDE) as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods."},
{"paper": "PolyTransform: Deep Polygon Transformer for Instance Segmentation", "abstract": "In this paper, we propose PolyTransform, a novel instance segmentation algorithm that produces precise, geometry-preserving masks by combining the strengths of prevailing segmentation approaches and modern polygon-based methods. In particular, we first exploit a segmentation network to generate instance masks.We then convert the masks into a set of polygons that are then fed to a deforming network that transforms the polygons such that they better fit the object boundaries. Our experiments on the challenging Cityscapes dataset show that our PolyTransform significantly improves the performance of the backbone instance segmentation network and ranks 1st on the Cityscapes test-set leaderboard. We also show impressive gains in the interactive annotation setting. We release the code at https://github.com/uber-research/PolyTransform."},
{"paper": "OccuSeg: Occupancy-aware 3D Instance Segmentation", "abstract": "3D instance segmentation, with a variety of applications in robotics and augmented reality, is in large demands these days. Unlike 2D images that are projective observations of the environment, 3D models provide metric reconstruction of the scenes without occlusion or scale ambiguity.In this paper, we define \"3D occupancy size\", as the number of voxels occupied by each instance. It owns advantages of robustness in prediction, on which basis, OccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our multi-task learning produces both occupancy signal and embedding representations, where the training of spatial and feature embeddings varies with their difference in scale-aware. Our clustering scheme benefits from the reliable comparison between the predicted occupancy size and the clustered occupancy size, which encourages hard samples being correctly clustered and avoids over segmentation. The proposed approach achieves state-of-the-art performance on 3 real-world datasets, i.e. ScanNetV2, S3DIS and SceneNN, while maintaining high efficiency."},
{"paper": "Variational Context-Deformable ConvNets for Indoor Scene Parsing", "abstract": "Context information is critical for image semantic segmentation. Especially in indoor scenes, the large variation of object scales makes spatial-context an important factor for improving the segmentation performance.Thus, in this paper, we propose a novel variational context-deformable (VCD) module to learn adaptive receptive-field in a structured fashion. Different from standard ConvNets, which share fixed-size spatial context for all pixels, the VCD module learns a deformable spatial-context with the guidance of depth information: depth information provides clues for identifying real local neighborhoods. Specifically, adaptive Gaussian kernels are learned with the guidance of multimodal information. By multiplying the learned Gaussian kernel with standard convolution filters, the VCD module can aggregate flexible spatial context for each pixel during convolution. The main contributions of this work are as follows: 1) a novel VCD module is proposed, which exploits learnable Gaussian kernels to enable feature learning with structured adaptive-context; 2) variational Bayesian probabilistic modeling is introduced for the training of VCD module, which can make it continuous and more stable; 3) a perspective-aware guidance module is designed to take advantage of multi-modal information for RGB-D segmentation. We evaluate the proposed approach on three widely-used datasets, and the performance improvement has shown the effectiveness of the proposed method."},
{"task": "Person Re-Identification", "papers": {"0": "Spatial-Temporal Person Re-identification", "1": "Spatial-Temporal Person Re-identification", "2": "Adaptive L2 Regularization in Person Re-Identification", "3": "Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification", "4": "Video Person Re-ID: Fantastic Techniques and Where to Find Them", "5": "Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification", "6": "AlignedReID: Surpassing Human-Level Performance in Person Re-Identification", "7": "Video Person Re-ID: Fantastic Techniques and Where to Find Them", "8": "AlignedReID: Surpassing Human-Level Performance in Person Re-Identification", "9": "Adaptive Graph Representation Learning for Video Person Re-identification", "10": "Joint Discriminative and Generative Learning for Person Re-identification", "11": "Unsupervised Tracklet Person Re-Identification", "12": "Joint Discriminative and Generative Learning for Person Re-identification", "13": "Pedestrian Alignment Network for Large-scale Person Re-identification"}},
{"paper": "Scan2CAD: Learning CAD Model Alignment in RGB-D Scans", "abstract": "We present Scan2CAD, a novel data-driven method that learns to align clean 3D\nCAD models from a shape database to the noisy and incomplete geometry of a\ncommodity RGB-D scan. For a 3D reconstruction of an indoor scene, our method\ntakes as input a set of CAD models, and predicts a 9DoF pose that aligns each\nmodel to the underlying scan geometry.To tackle this problem, we create a new\nscan-to-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated\nkeypoint pairs between 14225 CAD models from ShapeNet and their counterpart\nobjects in the scans. Our method selects a set of representative keypoints in a\n3D scan for which we find correspondences to the CAD geometry. To this end, we\ndesign a novel 3D CNN architecture that learns a joint embedding between real\nand synthetic objects, and from this predicts a correspondence heatmap. Based\non these correspondence heatmaps, we formulate a variational energy\nminimization that aligns a given set of CAD models to the reconstruction. We\nevaluate our approach on our newly introduced Scan2CAD benchmark where we\noutperform both handcrafted feature descriptor as well as state-of-the-art CNN\nbased methods by 21.39%."},
{"paper": "Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images", "abstract": "Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially.However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method."},
{"paper": "When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition", "abstract": "Recognizing objects and scenes are two challenging but essential tasks in image understanding. In particular, the use of RGB-D sensors in handling these tasks has emerged as an important area of focus for better visual understanding.Meanwhile, deep neural networks, specifically convolutional neural networks (CNNs), have become widespread and have been applied to many visual tasks by replacing hand-crafted features with effective deep features. However, it is an open problem how to exploit deep features from a multi-layer CNN model effectively. In this paper, we propose a novel two-stage framework that extracts discriminative feature representations from multi-modal RGB-D images for object and scene recognition tasks. In the first stage, a pretrained CNN model has been employed as a backbone to extract visual features at multiple levels. The second stage maps these features into high level representations with a fully randomized structure of recursive neural networks (RNNs) efficiently. In order to cope with the high dimensionality of CNN activations, a random weighted pooling scheme has been proposed by extending the idea of randomness in RNNs. Multi-modal fusion has been performed through a soft voting approach by computing weights based on individual recognition confidences (i.e. SVM scores) of RGB and depth streams separately. This produces consistent class label estimation in final RGB-D classification performance. Extensive experiments verify that fully randomized structure in RNN stage encodes CNN activations to discriminative solid features successfully. Comparative experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene datasets show that the proposed approach significantly outperforms state-of-the-art methods both in object and scene recognition tasks."},
{"paper": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "abstract": "For person re-identification, existing deep networks often focus on representation learning. However, without transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios.In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and results are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation, the proposed Query-Adaptive Convolution (QAConv) method gains large improvements over popular learning methods (about 10%+ mAP), and achieves comparable results to many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, achieving state-of-the-art results in cross-dataset person re-identification. Code is available at https://github.com/ShengcaiLiao/QAConv."},
{"paper": "What Do Single-view 3D Reconstruction Networks Learn?", "abstract": "Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space.In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research."},
{"paper": "Context Prior for Scene Segmentation", "abstract": "Recent works have widely explored the contextual dependencies to achieve more accurate segmentation results. However, most approaches rarely distinguish different types of contextual dependencies, which may pollute the scene understanding.In this work, we directly supervise the feature aggregation to distinguish the intra-class and inter-class context clearly. Specifically, we develop a Context Prior with the supervision of the Affinity Loss. Given an input image and corresponding ground truth, Affinity Loss constructs an ideal affinity map to supervise the learning of Context Prior. The learned Context Prior extracts the pixels belonging to the same category, while the reversed prior focuses on the pixels of different classes. Embedded into a conventional deep CNN, the proposed Context Prior Layer can selectively capture the intra-class and inter-class contextual dependencies, leading to robust feature representation. To validate the effectiveness, we design an effective Context Prior Network (CPNet). Extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against state-of-the-art semantic segmentation approaches. More specifically, our algorithm achieves 46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on Cityscapes. Code is available at https://git.io/ContextPrior."},
{"task": "Thermal Infrared Object Tracking", "papers": {}},
{"task": "Online Multi-Object Tracking", "papers": {"0": "Tracking without bells and whistles", "1": "Tracking without bells and whistles", "2": "Tracking without bells and whistles", "3": "Online Multi-Object Tracking Framework with the GMPHD Filter and Occlusion Group Management"}},
{"task": "Multiple Object Tracking", "papers": {"0": "Robust Multi-Modality Multi-Object Tracking"}},
{"paper": "Learning Generalisable Omni-Scale Representations for Person Re-Identification", "abstract": "An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges.First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efficient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without requiring any target data for model adaptation. Our code and models are released at \\texttt{https://github.com/KaiyangZhou/deep-person-reid}."},
{"task": "Style Generalization", "papers": {}},
{"task": "Visual Object Tracking", "papers": {"0": "Fast Visual Object Tracking with Rotated Bounding Boxes", "1": "Scale Equivariance Improves Siamese Tracking", "2": "AAA: Adaptive Aggregation of Arbitrary Online Trackers with Theoretical Performance Guarantee", "3": "Learning Discriminative Model Prediction for Tracking", "4": "Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking", "5": "Fast Visual Object Tracking with Rotated Bounding Boxes", "6": "One-Shot Video Object Segmentation", "7": "SiamVGG: Visual Tracking using Deeper Siamese Networks", "8": "Ocean: Object-aware Anchor-free Tracking", "9": "Ocean: Object-aware Anchor-free Tracking", "10": "Ocean: Object-aware Anchor-free Tracking", "11": "AAA: Adaptive Aggregation of Arbitrary Online Trackers with Theoretical Performance Guarantee"}},
{"paper": "Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification", "abstract": "Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the natural similar characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner.Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from global body to local parts) of unlabeled samples to automatically build multiple clusters from different views. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC to Market1501) and 4.4% (Market1501 to DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG ."},
{"paper": "Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification", "abstract": "This paper considers the domain adaptive person re-identification (re-ID)\nproblem: learning a re-ID model from a labeled source domain and an unlabeled\ntarget domain. Conventional methods are mainly to reduce feature distribution\ngap between the source and target domains.However, these studies largely\nneglect the intra-domain variations in the target domain, which contain\ncritical factors influencing the testing performance on the target domain. In\nthis work, we comprehensively investigate into the intra-domain variations of\nthe target domain and propose to generalize the re-ID model w.r.t three types\nof the underlying invariance, i.e., exemplar-invariance, camera-invariance and\nneighborhood-invariance. To achieve this goal, an exemplar memory is introduced\nto store features of the target domain and accommodate the three invariance\nproperties. The memory allows us to enforce the invariance constraints over\nglobal training batch without significantly increasing computation cost. Experiment demonstrates that the three invariance properties and the proposed\nmemory are indispensable towards an effective domain adaptation system. Results\non three re-ID domains show that our domain adaptation accuracy outperforms the\nstate of the art by a large margin. Code is available at:\nhttps://github.com/zhunzhong07/ECN"},
{"paper": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification", "abstract": "Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one.State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks. Code is available at https://github.com/yxgeee/MMT."},
{"paper": "Unsupervised Tracklet Person Re-Identification", "abstract": "Most existing person re-identification (re-id) methods rely on supervised\nmodel learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in a practical re-id deployment, due to the lack\nof exhaustive identity labelling of positive and negative image pairs for every\ncamera-pair.In this work, we present an unsupervised re-id deep learning\napproach. It is capable of incrementally discovering and exploiting the\nunderlying re-id discriminative information from automatically generated person\ntracklet data end-to-end. We formulate an Unsupervised Tracklet Association\nLearning (UTAL) framework. This is by jointly learning within-camera tracklet\ndiscrimination and cross-camera tracklet association in order to maximise the\ndiscovery of tracklet identity matching both within and across camera views. Extensive experiments demonstrate the superiority of the proposed model over\nthe state-of-the-art unsupervised learning and domain adaptation person re-id\nmethods on eight benchmarking datasets."},
{"paper": "Adaptive Graph Representation Learning for Video Person Re-identification", "abstract": "Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations.Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes' information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/weleen/AGRL.pytorch."},
{"paper": "Joint Discriminative and Generative Learning for Person Re-identification", "abstract": "Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes.The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets."},
{"task": "Multi-Object Tracking", "papers": {"0": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking", "1": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking", "2": "Lifted Disjoint Paths with Application in Multiple Object Tracking", "3": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking", "4": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking", "5": "ReMOTS: Self-Supervised Refining Multi-Object Tracking and Segmentation"}},
{"paper": "Video Person Re-ID: Fantastic Techniques and Where to Find Them", "abstract": "The ability to identify the same person from multiple camera views without the explicit use of facial recognition is receiving commercial and academic interest. The current status-quo solutions are based on attention neural models.In this paper, we propose Attention and CL loss, which is a hybrid of center and Online Soft Mining (OSM) loss added to the attention loss on top of a temporal attention-based neural network. The proposed loss function applied with bag-of-tricks for training surpasses the state of the art on the common person Re-ID datasets, MARS and PRID 2011. Our source code is publicly available on github."},
{"paper": "AlignedReID: Surpassing Human-Level Performance in Person Re-Identification", "abstract": "In this paper, we propose a novel method called AlignedReID that extracts a\nglobal feature which is jointly learned with local features. Global feature\nlearning benefits greatly from local feature learning, which performs an\nalignment/matching by calculating the shortest path between two sets of local\nfeatures, without requiring extra supervision.After the joint learning, we\nonly keep the global feature to compute the similarities between images. Our\nmethod achieves rank-1 accuracy of 94.4% on Market1501 and 97.8% on CUHK03,\noutperforming state-of-the-art methods by a large margin. We also evaluate\nhuman-level performance and demonstrate that our method is the first to surpass\nhuman-level performance on Market1501 and CUHK03, two widely used Person ReID\ndatasets."},
{"paper": "Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification", "abstract": "Person Re-Identification is a challenging task that aims to retrieve all instances of a query image across a system of non-overlapping cameras. Due to the various extreme changes of view, it is common that local regions that could be used to match people are suppressed, which leads to a scenario where approaches have to evaluate the similarity of images based on less informative regions.In this work, we introduce the Top-DB-Net, a method based on Top DropBlock that pushes the network to learn to focus on the scene foreground, with special emphasis on the most task-relevant regions and, at the same time, encodes low informative regions to provide high discriminability. The Top-DB-Net is composed of three streams: (i) a global stream encodes rich image information from a backbone, (ii) the Top DropBlock stream encourages the backbone to encode low informative regions with high discriminative features, and (iii) a regularization stream helps to deal with the noise created by the dropping process of the second stream, when testing the first two streams are used. Vast experiments on three challenging datasets show the capabilities of our approach against state-of-the-art methods. Qualitative results demonstrate that our method exhibits better activation maps focusing on reliable parts of the input images."},
{"paper": "Pedestrian Alignment Network for Large-scale Person Re-identification", "abstract": "Person re-identification (person re-ID) is mostly viewed as an image\nretrieval problem. This task aims to search a query person in a large image\npool.In practice, person re-ID usually adopts automatic detectors to obtain\ncropped pedestrian images. However, this process suffers from two types of\ndetector errors: excessive background and part missing. Both errors deteriorate\nthe quality of pedestrian alignment and may compromise pedestrian matching due\nto the position and scale variances. To address the misalignment problem, we\npropose that alignment can be learned from an identification procedure. We\nintroduce the pedestrian alignment network (PAN) which allows discriminative\nembedding learning and pedestrian alignment without extra annotations. Our key\nobservation is that when the convolutional neural network (CNN) learns to\ndiscriminate between different identities, the learned feature maps usually\nexhibit strong activations on the human body rather than the background. The\nproposed network thus takes advantage of this attention mechanism to adaptively\nlocate and align pedestrians within a bounding box. Visual examples show that\npedestrians are better aligned with PAN. Experiments on three large-scale re-ID\ndatasets confirm that PAN improves the discriminative ability of the feature\nembeddings and yields competitive accuracy with the state-of-the-art methods."},
{"paper": "Online Multi-Object Tracking Framework with the GMPHD Filter and Occlusion Group Management", "abstract": "In this paper, we propose an efficient online multi-object tracking framework based on the GMPHD filter and occlusion group management scheme where the GMPHD filter utilizes hierarchical data association to reduce the false negatives caused by miss detection. The hierarchical data association consists of two steps: detection-to-track and track-to-track associations, which can recover the lost tracks and their switched IDs.In addition, the proposed framework is equipped with an object grouping management scheme which handles occlusion problems with two main parts. The first part is \"track merging\" which can merge the false positive tracks caused by false positive detections from occlusions, where the false positive tracks are usually occluded with a measure. The measure is the occlusion ratio between visual objects, sum-of-intersection-over-area (SIOA) we defined instead of the IOU metric. The second part is \"occlusion group energy minimization (OGEM)\" which prevents the occluded true positive tracks from false \"track merging\". We define each group of the occluded objects as an energy function and find an optimal hypothesis which makes the energy minimal. We evaluate the proposed tracker in benchmark datasets such as MOT15 and MOT17 which are built for multi-person tracking. An ablation study in training dataset shows that not only \"track merging\" and \"OGEM\" complement each other but also the proposed tracking method has more robust performance and less sensitive to parameters than baseline methods. Also, SIOA works better than IOU for various sizes of false positives. Experimental results show that the proposed tracker efficiently handles occlusion situations and achieves competitive performance compared to the state-of-the-art methods. Especially, our method shows the best multi-object tracking accuracy among the online and real-time executable methods."},
{"paper": "Tracking without bells and whistles", "abstract": "The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions.We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions."},
{"paper": "Robust Multi-Modality Multi-Object Tracking", "abstract": "Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information.In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT."},
{"paper": "Adaptive L2 Regularization in Person Re-Identification", "abstract": "We introduce an adaptive L2 regularization mechanism in the setting of person re-identification. In the literature, it is common practice to utilize hand-picked regularization factors which remain constant throughout the training procedure.Unlike existing approaches, the regularization factors in our proposed method are updated adaptively through backpropagation. This is achieved by incorporating trainable scalar variables as the regularization factors, which are further fed into a scaled hard sigmoid function. Extensive experiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the effectiveness of our framework. Most notably, we obtain state-of-the-art performance on MSMT17, which is the largest dataset for person re-identification. Source code is publicly available at https://github.com/nixingyang/AdaptiveL2Regularization."},
{"task": "Music Genre Transfer", "papers": {}},
{"paper": "Spatial-Temporal Person Re-identification", "abstract": "Most of current person re-identification (ReID) methods neglect a\nspatial-temporal constraint. Given a query image, conventional methods compute\nthe feature distances between the query image and all the gallery images and\nreturn a similarity ranked table.When the gallery database is very large in\npractice, these approaches fail to obtain a good performance due to appearance\nambiguity across different camera views. In this paper, we propose a novel\ntwo-stream spatial-temporal person ReID (st-ReID) framework that mines both\nvisual semantic information and spatial-temporal information. To this end, a\njoint similarity metric with Logistic Smoothing (LS) is introduced to integrate\ntwo kinds of heterogeneous information into a unified framework. To approximate\na complex spatial-temporal probability distribution, we develop a fast\nHistogram-Parzen (HP) method. With the help of the spatial-temporal constraint,\nthe st-ReID model eliminates lots of irrelevant images and thus narrows the\ngallery database. Without bells and whistles, our st-ReID method achieves\nrank-1 accuracy of 98.1\\% on Market-1501 and 94.4\\% on DukeMTMC-reID, improving\nfrom the baselines 91.2\\% and 83.8\\%, respectively, outperforming all previous\nstate-of-the-art methods by a large margin."},
{"task": "Image Stylization", "papers": {}},
{"paper": "Ocean: Object-aware Anchor-free Tracking", "abstract": "Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We find the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., $IoU \\geq0.6$).This mechanism makes it difficult to refine the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of refining the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware feature can further contribute to the classification of target objects and background. Moreover, we present a novel tracking framework based on the anchor-free model. The experiments show that our anchor-free tracker achieves state-of-the-art performance on five benchmarks, including VOT-2018, VOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at https://github.com/researchmm/TracKit."},
{"paper": "SiamVGG: Visual Tracking using Deeper Siamese Networks", "abstract": "Recently, we have seen a rapid development of Deep Neural Network (DNN) based\nvisual tracking solutions. Some trackers combine the DNN-based solutions with\nDiscriminative Correlation Filters (DCF) to extract semantic features and\nsuccessfully deliver the state-of-the-art tracking accuracy.However, these\nsolutions are highly compute-intensive, which require long processing time,\nresulting unsecured real-time performance. To deliver both high accuracy and\nreliable real-time performance, we propose a novel tracker called SiamVGG. It\ncombines a Convolutional Neural Network (CNN) backbone and a cross-correlation\noperator, and takes advantage of the features from exemplary images for more\naccurate object tracking. The architecture of SiamVGG is customized from VGG-16, with the parameters\nshared by both exemplary images and desired input video frames. We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017\ndatasets with the state-of-the-art accuracy while maintaining a decent\nreal-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve\n2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in\nVOT2017 Challenge."},
{"paper": "Scale Equivariance Improves Siamese Tracking", "abstract": "Siamese trackers turn tracking into similarity estimation between a template and the candidate regions in the frame. Mathematically, one of the key ingredients of success of the similarity function is translation equivariance.Non-translation-equivariant architectures induce a positional bias during training, so the location of the target will be hard to recover from the feature space. In real life scenarios, objects undergoe various transformations other than translation, such as rotation or scaling. Unless the model has an internal mechanism to handle them, the similarity may degrade. In this paper, we focus on scaling and we aim to equip the Siamese network with additional built-in scale equivariance to capture the natural variations of the target a priori. We develop the theory for scale-equivariant Siamese trackers, and provide a simple recipe for how to make a wide range of existing trackers scale-equivariant. We present SE-SiamFC, a scale-equivariant variant of SiamFC built according to the recipe. We conduct experiments on OTB and VOT benchmarks and on the synthetically generated T-MNIST and S-MNIST datasets. We demonstrate that a built-in additional scale equivariance is useful for visual object tracking."},
{"paper": "AAA: Adaptive Aggregation of Arbitrary Online Trackers with Theoretical Performance Guarantee", "abstract": "For visual object tracking, it is difficult to realize an almighty online tracker due to the huge variations of target appearance depending on an image sequence. This paper proposes an online tracking method that adaptively aggregates arbitrary multiple online trackers.The performance of the proposed method is theoretically guaranteed to be comparable to that of the best tracker for any image sequence, although the best expert is unknown during tracking. The experimental study on the large variations of benchmark datasets and aggregated trackers demonstrates that the proposed method can achieve state-of-the-art performance. The code is available at https://github.com/songheony/AAA-journal."},
{"paper": "Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking", "abstract": "We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system.In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters. To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at https://github.com/XU-TIANYANG/GFS-DCF."},
{"paper": "Learning Discriminative Model Prediction for Tracking", "abstract": "The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage.To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking."},
{"paper": "ReMOTS: Self-Supervised Refining Multi-Object Tracking and Segmentation", "abstract": "We aim to improve the performance of Multiple Object Tracking and Segmentation (MOTS) by refinement. However, it remains challenging for refining MOTS results, which could be attributed to that appearance features are not adapted to target videos and it is also difficult to find proper thresholds to discriminate them.To tackle this issue, we propose a self-supervised refining MOTS (i.e., ReMOTS) framework. ReMOTS mainly takes four steps to refine MOTS results from the data association perspective. (1) Training the appearance encoder using predicted masks. (2) Associating observations across adjacent frames to form short-term tracklets. (3) Training the appearance encoder using short-term tracklets as reliable pseudo labels. (4) Merging short-term tracklets to long-term tracklets utilizing adopted appearance features and thresholds that are automatically obtained from statistical information. Using ReMOTS, we reached the $1^{st}$ place on CVPR 2020 MOTS Challenge 1, with an sMOTSA score of $69.9$."},
{"paper": "Fast Visual Object Tracking with Rotated Bounding Boxes", "abstract": "In this paper, we demonstrate a novel algorithm that uses ellipse fitting to estimate the bounding box rotation angle and size with the segmentation(mask) on the target for online and real-time visual object tracking. Our method, SiamMask_E, improves the bounding box fitting procedure of the state-of-the-art object tracking algorithm SiamMask and still retains a fast-tracking frame rate (80 fps) on a system equipped with GPU (GeForce GTX 1080 Ti or higher).We tested our approach on the visual object tracking datasets (VOT2016, VOT2018, and VOT2019) that were labeled with rotated bounding boxes. By comparing with the original SiamMask, we achieved an improved Accuracy of 0.652 and 0.309 EAO on VOT2019, which is 0.056 and 0.026 higher than the original SiamMask. The implementation is available on GitHub: https://github.com/baoxinchen/siammask_e."},
{"paper": "One-Shot Video Object Segmentation", "abstract": "This paper tackles the task of semi-supervised video object segmentation,\ni.e., the separation of an object from the background in a video, given the\nmask of the first frame. We present One-Shot Video Object Segmentation (OSVOS),\nbased on a fully-convolutional neural network architecture that is able to\nsuccessively transfer generic semantic information, learned on ImageNet, to the\ntask of foreground segmentation, and finally to learning the appearance of a\nsingle annotated object of the test sequence (hence one-shot).Although all\nframes are processed independently, the results are temporally coherent and\nstable. We perform experiments on two annotated video segmentation databases,\nwhich show that OSVOS is fast and improves the state of the art by a\nsignificant margin (79.8% vs 68.0%)."},
{"paper": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking", "abstract": "There has been remarkable progress on object detection and re-identification (re-ID) in recent years which are the key components of multi-object tracking. However, little attention has been focused on jointly accomplishing the two tasks in a single network.Our study shows that the previous attempts ended up with degraded accuracy mainly because the re-ID task is not fairly learned which causes many identity switches. The unfairness lies in two-fold: (1) they treat re-ID as a secondary task whose accuracy heavily depends on the primary detection task. So training is largely biased to the detection task but ignores the re-ID task; (2) they use ROI-Align to extract re-ID features which is directly borrowed from object detection. However, this introduces a lot of ambiguity in characterizing objects because many sampling points may belong to disturbing instances or background. To solve the problems, we present a simple approach \\emph{FairMOT} which consists of two homogeneous branches to predict pixel-wise objectness scores and re-ID features. The achieved fairness between the tasks allows \\emph{FairMOT} to obtain high levels of detection and tracking accuracy and outperform previous state-of-the-arts by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT."},
{"task": "Activity Recognition", "papers": {}},
{"task": "Multimodal Activity Recognition", "papers": {"0": "Interpretable 3D Human Action Analysis with Temporal Convolutional Networks", "1": "HAMLET: A Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm", "2": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "3": "Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos", "4": "Adaptive Feature Processing for Robust Human Activity Recognition on a Novel Multi-Modal Dataset", "5": "Can a simple approach identify complex nurse care activity?", "6": "Autonomous Human Activity Classification from Ego-vision Camera and Accelerometer Data", "7": "HAMLET: A Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm", "8": "HAMLET: A Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm"}},
{"paper": "AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation", "abstract": "We introduce a method for learning to generate the surface of 3D shapes. Our\napproach represents a 3D shape as a collection of parametric surface elements\nand, in contrast to methods generating voxel grids or point clouds, naturally\ninfers a surface representation of the shape.Beyond its novelty, our new shape\ngeneration framework, AtlasNet, comes with significant advantages, such as\nimproved precision and generalization capabilities, and the possibility to\ngenerate a shape of arbitrary resolution without memory issues. We demonstrate\nthese benefits and compare to strong baselines on the ShapeNet benchmark for\ntwo applications: (i) auto-encoding shapes, and (ii) single-view reconstruction\nfrom a still image. We also provide results showing its potential for other\napplications, such as morphing, parametrization, super-resolution, matching,\nand co-segmentation."},
{"subfield": "Depth Estimation", "tasks": {"0": "Medical Image Segmentation", "1": "Lesion Segmentation", "2": "Brain Tumor Segmentation", "3": "Brain Segmentation", "4": "Retinal Vessel Segmentation", "5": "3D Medical Imaging Segmentation", "6": "Cell Segmentation", "7": "Cardiac Segmentation", "8": "Liver Segmentation", "9": "Lung Nodule Segmentation", "10": "Brain Image Segmentation", "11": "Pancreas Segmentation", "12": "Iris Segmentation", "13": "COVID-19 Image Segmentation", "14": "Electron Microscopy Image Segmentation", "15": "Nuclear Segmentation", "16": "Volumetric Medical Image Segmentation", "17": "Skin Cancer Segmentation", "18": "Infant Brain Mri Segmentation", "19": "Brain Lesion Segmentation From Mri", "20": "Ischemic Stroke Lesion Segmentation", "21": "Automatic Liver And Tumor Segmentation", "22": "Acute Stroke Lesion Segmentation", "23": "Cerebrovascular Network Segmentation", "24": "Placenta Segmentation", "25": "Pulmorary Vessel Segmentation", "26": "Automated Pancreas Segmentation", "27": "Brain Ventricle Localization And Segmentation In 3D Ultrasound Images", "28": "Semantic Segmentation Of Orthoimagery"}},
{"task": "Egocentric Activity Recognition", "papers": {"0": "Large-scale weakly-supervised pre-training for video action recognition", "1": "Symbiotic Attention with Privileged Information for Egocentric Action Recognition"}},
{"subfield": "Facial Recognition and Modelling", "tasks": {"0": "Few-Shot Learning", "1": "Few-Shot Image Classification", "2": "One-Shot Learning", "3": "Cross-Domain Few-Shot", "4": "Few-Shot Relation Classification", "5": "Few-Shot Imitation Learning", "6": "Few-Shot Camera-Adaptive Color Constancy"}},
{"subfield": "Activity Recognition", "tasks": {"0": "Depth Estimation", "1": "Monocular Depth Estimation", "2": "Stereo Depth Estimation", "3": "Depth And Camera Motion", "4": "3D Depth Estimation", "5": "Indoor Monocular Depth Estimation", "6": "Depth Map Super-Resolution", "7": "Depth Image Upsampling"}},
{"paper": "Lifted Disjoint Paths with Application in Multiple Object Tracking", "abstract": "We present an extension to the disjoint paths problem in which additional \\emph{lifted} edges are introduced to provide path connectivity priors. We call the resulting optimization problem the lifted disjoint paths problem.We show that this problem is NP-hard by reduction from integer multicommodity flow and 3-SAT. To enable practical global optimization, we propose several classes of linear inequalities that produce a high-quality LP-relaxation. Additionally, we propose efficient cutting plane algorithms for separating the proposed linear inequalities. The lifted disjoint path problem is a natural model for multiple object tracking and allows an elegant mathematical formulation for long range temporal interactions. Lifted edges help to prevent id switches and to re-identify persons. Our lifted disjoint paths tracker achieves nearly optimal assignments with respect to input detections. As a consequence, it leads on all three main benchmarks of the MOT challenge, improving significantly over state-of-the-art."},
{"task": "Group Activity Recognition", "papers": {"0": "Learning Actor Relation Graphs for Group Activity Recognition", "1": "Learning Actor Relation Graphs for Group Activity Recognition"}},
{"subfield": "Temporal Action Localization", "tasks": {"0": "Anomaly Detection", "1": "Unsupervised Anomaly Detection", "2": "Anomaly Detection In Surveillance Videos", "3": "Abnormal Event Detection In Video", "4": "Group Anomaly Detection", "5": "Anomaly Detection in Edge Streams", "6": "Unsupervised Anomaly Detection In Sound"}},
{"subfield": "3D Car Instance Understanding", "tasks": {"0": "Image Retrieval", "1": "Content-Based Image Retrieval", "2": "Sketch-Based Image Retrieval", "3": "Text-Image Retrieval", "4": "Medical Image Retrieval", "5": "Multi-Label Image Retrieval", "6": "Image Instance Retrieval", "7": "Face Image Retrieval"}},
{"subfield": "Video", "tasks": {"0": "Action Recognition", "1": "Skeleton Based Action Recognition", "2": "3D Action Recognition", "3": "Weakly Supervised Action Localization", "4": "Weakly-supervised Temporal Action Localization", "5": "Temporal Action Proposal Generation", "6": "Activity Recognition In Videos", "7": "Action Recognition In Still Images"}},
{"subfield": "Denoising", "tasks": {"0": "Face Recognition", "1": "Face Detection", "2": "Face Verification", "3": "Face Alignment", "4": "Facial Expression Recognition", "5": "Face Generation", "6": "Facial Landmark Detection", "7": "3D Face Reconstruction", "8": "Face Reconstruction", "9": "Face Anti-Spoofing", "10": "Face Swapping", "11": "Face Identification", "12": "Age Estimation", "13": "Gender Prediction", "14": "Facial Inpainting", "15": "Robust Face Recognition", "16": "Face Hallucination", "17": "Action Unit Detection", "18": "Facial Action Unit Detection", "19": "Age And Gender Classification", "20": "Face Sketch Synthesis", "21": "Talking Face Generation", "22": "Unsupervised Facial Landmark Detection", "23": "Heterogeneous Face Recognition", "24": "Robust Face Alignment", "25": "Age-Invariant Face Recognition", "26": "Facial Beauty Prediction", "27": "Facial Attribute Classification", "28": "3D Facial Expression Recognition", "29": "Mobile Periocular Recognition", "30": "Smile Recognition", "31": "Face Image Retrieval", "32": "Facial Recognition and Modelling"}},
{"subfield": "Super-Resolution", "tasks": {"0": "Action Recognition", "1": "Activity Recognition", "2": "Multimodal Activity Recognition", "3": "Egocentric Activity Recognition", "4": "Group Activity Recognition", "5": "Human action generation", "6": "RF-based Action Recognition", "7": "Cross-Domain Activity Recognition", "8": "Concurrent Activity Recognition", "9": "Recognizing And Localizing Human Actions"}},
{"subfield": "Pose Estimation", "tasks": {"0": "Autonomous Driving", "1": "Autonomous Vehicles", "2": "Self-Driving Cars", "3": "Simultaneous Localization and Mapping", "4": "Autonomous Navigation", "5": "Pedestrian Detection", "6": "Lane Detection", "7": "Traffic Sign Recognition", "8": "Loop Closure Detection", "9": "Pedestrian Attribute Recognition", "10": "Driver Attention Monitoring", "11": "3D Car Instance Understanding", "12": "Fast Vehicle Detection", "13": "Pedestrian Density Estimation"}},
{"subfield": "Data Augmentation", "tasks": {"0": "Super-Resolution", "1": "Image Super-Resolution", "2": "Video Super-Resolution", "3": "Multi-Frame Super-Resolution", "4": "3D Object Super-Resolution", "5": "Depth Map Super-Resolution"}},
{"subfield": "Autonomous Vehicles", "tasks": {"0": "Object Tracking", "1": "Video Object Segmentation", "2": "Video Prediction", "3": "Action Classification", "4": "Video Classification", "5": "Visual Object Tracking", "6": "Video Understanding", "7": "Video Generation", "8": "Video Retrieval", "9": "Video Recognition", "10": "Video Super-Resolution", "11": "Multiple Object Tracking", "12": "Video Frame Interpolation", "13": "Motion Compensation", "14": "Video Compression", "15": "Video Description", "16": "Video Summarization", "17": "Video Denoising", "18": "Video-to-Video Synthesis", "19": "Anomaly Detection In Surveillance Videos", "20": "Activity Recognition In Videos", "21": "Video Background Subtraction", "22": "Predict Future Video Frames", "23": "Video Object Tracking", "24": "Video Compressive Sensing", "25": "Unsupervised Video Summarization", "26": "Video Similarity", "27": "Supervised Video Summarization", "28": "Video Synchronization", "29": "Action Spotting", "30": "Localization In Video Forgery", "31": "Video Deinterlacing", "32": "Video Story QA", "33": "Object Discovery In Videos", "34": "Deception Detection In Videos", "35": "Pornography Detection In Videos", "36": "Natural Language Moment Retrieval", "37": "Dynamic Region Segmentation", "38": "Video", "39": "Video Interlacing"}},
{"subfield": "Image Generation", "tasks": {"0": "Pose Estimation", "1": "3D Human Pose Estimation", "2": "3D Pose Estimation", "3": "Keypoint Detection", "4": "Hand Pose Estimation", "5": "Multi-Person Pose Estimation", "6": "6D Pose Estimation", "7": "6D Pose Estimation using RGB", "8": "Head Pose Estimation", "9": "6D Pose Estimation using RGBD", "10": "Human Pose Forecasting", "11": "Animal Pose Estimation", "12": "RF-based Pose Estimation", "13": "Hand Joint Reconstruction", "14": "Activeness Detection"}},
{"task": "Action Recognition", "papers": {"0": "Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition", "1": "Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition", "2": "PAN: Towards Fast Action Recognition via Learning Persistence of Appearance", "3": "Mutual Modality Learning for Video Action Classification", "4": "Multi-granularity Generator for Temporal Action Proposal", "5": "VPN: Learning Video-Pose Embedding for Activities of Daily Living", "6": "PAN: Towards Fast Action Recognition via Learning Persistence of Appearance", "7": "Video Classification with Channel-Separated Convolutional Networks", "8": "Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition", "9": "Large-scale weakly-supervised pre-training for video action recognition", "10": "YouTube-8M: A Large-Scale Video Classification Benchmark", "11": "SlowFast Networks for Video Recognition", "12": "SCSampler: Sampling Salient Clips from Video for Efficient Action Recognition", "13": "Learning Actor Relation Graphs for Group Activity Recognition", "14": "Skeleton-based Action Recognition of People Handling Objects", "15": "Skeleton-based Action Recognition of People Handling Objects", "16": "A Hierarchical Context Model for Event Recognition in Surveillance Video", "17": "Extensible Hierarchical Method of Detecting Interactive Actions for Video Understanding", "18": "Action Machine: Rethinking Action Recognition in Trimmed Videos", "19": "Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention", "20": "Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention", "21": "Multi-modal Self-Supervision from Generalized Data Transformations"}},
{"paper": "Symbiotic Attention with Privileged Information for Egocentric Action Recognition", "abstract": "Egocentric video recognition is a natural testbed for diverse interaction reasoning. Due to the large action vocabulary in egocentric video datasets, recent studies usually utilize a two-branch structure for action recognition, ie, one branch for verb classification and the other branch for noun classification.However, correlation studies between the verb and the noun branches have been largely ignored. Besides, the two branches fail to exploit local features due to the absence of a position-aware attention mechanism. In this paper, we propose a novel Symbiotic Attention framework leveraging Privileged information (SAP) for egocentric video recognition. Finer position-aware object detection features can facilitate the understanding of actor's interaction with the object. We introduce these features in action recognition and regard them as privileged information. Our framework enables mutual communication among the verb branch, the noun branch, and the privileged information. This communication process not only injects local details into global features but also exploits implicit guidance about the spatio-temporal position of an on-going action. We introduce novel symbiotic attention (SA) to enable effective communication. It first normalizes the detection guided features on one branch to underline the action-relevant information from the other branch. SA adaptively enhances the interactions among the three sources. To further catalyze this communication, spatial relations are uncovered for the selection of most action-relevant information. It identifies the most valuable and discriminative feature for classification. We validate the effectiveness of our SAP quantitatively and qualitatively. Notably, it achieves the state-of-the-art on two large-scale egocentric video datasets."},
{"paper": "Adaptive Feature Processing for Robust Human Activity Recognition on a Novel Multi-Modal Dataset", "abstract": "Human Activity Recognition (HAR) is a key building block of many emerging\napplications such as intelligent mobility, sports analytics, ambient-assisted\nliving and human-robot interaction. With robust HAR, systems will become more\nhuman-aware, leading towards much safer and empathetic autonomous systems.While human pose detection has made significant progress with the dawn of deep\nconvolutional neural networks (CNNs), the state-of-the-art research has almost\nexclusively focused on a single sensing modality, especially video. However, in\nsafety critical applications it is imperative to utilize multiple sensor\nmodalities for robust operation. To exploit the benefits of state-of-the-art\nmachine learning techniques for HAR, it is extremely important to have\nmultimodal datasets. In this paper, we present a novel, multi-modal sensor\ndataset that encompasses nine indoor activities, performed by 16 participants,\nand captured by four types of sensors that are commonly used in indoor\napplications and autonomous vehicles. This multimodal dataset is the first of\nits kind to be made openly available and can be exploited for many applications\nthat require HAR, including sports analytics, healthcare assistance and indoor\nintelligent mobility. We propose a novel data preprocessing algorithm to enable\nadaptive feature extraction from the dataset to be utilized by different\nmachine learning algorithms. Through rigorous experimental evaluations, this\npaper reviews the performance of machine learning approaches to posture\nrecognition, and analyses the robustness of the algorithms. When performing HAR\nwith the RGB-Depth data from our new dataset, machine learning algorithms such\nas a deep neural network reached a mean accuracy of up to 96.8% for\nclassification across all stationary and dynamic activities"},
{"paper": "Large-scale weakly-supervised pre-training for video action recognition", "abstract": "Current fully-supervised video datasets consist of only a few hundred\nthousand videos and fewer than a thousand domain-specific labels. This hinders\nthe progress towards advanced video architectures.This paper presents an\nin-depth study of using large volumes of web videos for pre-training video\nmodels for the task of action recognition. Our primary empirical finding is\nthat pre-training at a very large scale (over 65 million videos), despite on\nnoisy social-media videos and hashtags, substantially improves the\nstate-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised\nvideo action datasets. First, given that actions involve interactions with\nobjects, how should one construct a verb-object pre-training label space to\nbenefit transfer learning the most? Second, frame-based models perform quite\nwell on action recognition; is pre-training for good image features sufficient\nor is pre-training for spatio-temporal features valuable for optimal transfer\nlearning? Finally, actions are generally less well-localized in long videos vs.\nshort videos; since action labels are provided at a video level, how should one\nchoose video clips for best performance, given some fixed budget of number or\nminutes of videos?"},
{"task": "Depth Image Upsampling", "papers": {}},
{"task": "Indoor Monocular Depth Estimation", "papers": {"0": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network"}},
{"task": "Depth Map Super-Resolution", "papers": {}},
{"task": "Few-Shot Image Classification", "papers": {"0": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "1": "Self-Supervised Learning For Few-Shot Image Classification", "2": "Few-Shot Learning with Global Class Representations", "3": "Meta-Curvature", "4": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "5": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "6": "Meta-Curvature", "7": "Decoder Choice Network for Meta-Learning", "8": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "9": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "10": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "11": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "12": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "13": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "14": "Task Augmentation by Rotating for Meta-Learning", "15": "Task Augmentation by Rotating for Meta-Learning", "16": "Learning Deep Representations of Fine-grained Visual Descriptions", "17": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "18": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "19": "RelationNet2: Deep Comparison Columns for Few-Shot Learning", "20": "RelationNet2: Deep Comparison Columns for Few-Shot Learning", "21": "Prototypical Networks for Few-shot Learning", "22": "Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning", "23": "Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning", "24": "Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning", "25": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "26": "Synthesized Classifiers for Zero-Shot Learning", "27": "Synthesized Classifiers for Zero-Shot Learning", "28": "Learning Deep Representations of Fine-grained Visual Descriptions", "29": "Synthesized Classifiers for Zero-Shot Learning", "30": "Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels", "31": "Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels", "32": "Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning", "33": "Meta-Transfer Learning for Few-Shot Learning", "34": "Delta-encoder: an effective sample synthesis method for few-shot object recognition", "35": "Delta-encoder: an effective sample synthesis method for few-shot object recognition", "36": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "37": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "38": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "39": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "40": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "41": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "42": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "43": "Few-Shot Learning with Global Class Representations", "44": "Negative Margin Matters: Understanding Margin in Few-shot Classification", "45": "Laplacian Regularized Few-Shot Learning", "46": "Laplacian Regularized Few-Shot Learning", "47": "Laplacian Regularized Few-Shot Learning"}},
{"task": "Action Recognition In Still Images", "papers": {}},
{"task": "Activity Recognition In Videos", "papers": {"0": "Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters"}},
{"task": "Style Transfer", "papers": {}},
{"task": "Hand Joint Reconstruction", "papers": {}},
{"task": "Activeness Detection", "papers": {"0": "ActiveNet: A computer-vision based approach to determine lethargy"}},
{"paper": "Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition", "abstract": "In this work, we combine 3D convolution with late temporal modeling for action recognition. For this aim, we replace the conventional Temporal Global Average Pooling (TGAP) layer at the end of 3D convolutional architecture with the Bidirectional Encoder Representations from Transformers (BERT) layer in order to better utilize the temporal information with BERT's attention mechanism.We show that this replacement improves the performances of many popular 3D convolution architectures for action recognition, including ResNeXt, I3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on both HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy, respectively. The code is publicly available."},
{"paper": "Multi-modal Self-Supervision from Generalized Data Transformations", "abstract": "The recent success of self-supervised learning can be largely attributed to content-preserving transformations, which can be used to easily induce invariances. While transformations generate positive sample pairs in contrastive loss training, most recent work focuses on developing new objective formulations, and pays relatively little attention to the transformations themselves.In this paper, we introduce the framework of Generalized Data Transformations to (1) reduce several recent self-supervised learning objectives to a single formulation for ease of comparison, analysis, and extension, (2) allow a choice between being invariant or distinctive to data transformations, obtaining different supervisory signals, and (3) derive the conditions that combinations of transformations must obey in order to lead to well-posed learning objectives. This framework allows both invariance and distinctiveness to be injected into representations simultaneously, and lets us systematically explore novel contrastive objectives. We apply it to study multi-modal self-supervision for audio-visual representation learning from unlabelled videos, improving the state-of-the-art by a large margin, and even surpassing supervised pretraining. We demonstrate results on a variety of downstream video and audio classification and retrieval tasks, on datasets such as HMDB-51, UCF-101, DCASE2014, ESC-50 and VGG-Sound. In particular, we achieve new state-of-the-art accuracies of 72.8% on HMDB-51 and 95.2% on UCF-101."},
{"paper": "Action Machine: Rethinking Action Recognition in Trimmed Videos", "abstract": "Existing methods in video action recognition mostly do not distinguish human\nbody from the environment and easily overfit the scenes and objects. In this\nwork, we present a conceptually simple, general and high-performance framework\nfor action recognition in trimmed videos, aiming at person-centric modeling.The method, called Action Machine, takes as inputs the videos cropped by person\nbounding boxes. It extends the Inflated 3D ConvNet (I3D) by adding a branch for\nhuman pose estimation and a 2D CNN for pose-based action recognition, being\nfast to train and test. Action Machine can benefit from the multi-task training\nof action recognition and pose estimation, the fusion of predictions from RGB\nimages and poses. On NTU RGB-D, Action Machine achieves the state-of-the-art\nperformance with top-1 accuracies of 97.2% and 94.3% on cross-view and\ncross-subject respectively. Action Machine also achieves competitive\nperformance on another three smaller action recognition datasets: Northwestern\nUCLA Multiview Action3D, MSR Daily Activity3D and UTD-MHAD. Code will be made\navailable."},
{"paper": "Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention", "abstract": "Attentive video modeling is essential for action recognition in unconstrained videos due to their rich yet redundant information over space and time. However, introducing attention in a deep neural network for action recognition is challenging for two reasons.First, an effective attention module needs to learn what (objects and their local motion patterns), where (spatially), and when (temporally) to focus on. Second, a video attention module must be efficient because existing action recognition models already suffer from high computational cost. To address both challenges, a novel What-Where-When (W3) video attention module is proposed. Departing from existing alternatives, our W3 module models all three facets of video attention jointly. Crucially, it is extremely efficient by factorizing the high-dimensional video feature data into low-dimensional meaningful spaces (1D channel vector for `what' and 2D spatial tensors for `where'), followed by lightweight temporal attention reasoning. Extensive experiments show that our attention model brings significant improvements to existing action recognition models, achieving new state-of-the-art performance on a number of benchmarks."},
{"paper": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network", "abstract": "Predicting depth is an essential component in understanding the 3D geometry\nof a scene. While for stereo images local correspondence suffices for\nestimation, finding depth relations from a single image is less\nstraightforward, requiring integration of both global and local information\nfrom various cues.Moreover, the task is inherently ambiguous, with a large\nsource of uncertainty coming from the overall scale. In this paper, we present\na new method that addresses this task by employing two deep network stacks: one\nthat makes a coarse global prediction based on the entire image, and another\nthat refines this prediction locally. We also apply a scale-invariant error to\nhelp measure depth relations rather than scale. By leveraging the raw datasets\nas large sources of training data, our method achieves state-of-the-art results\non both NYU Depth and KITTI, and matches detailed depth boundaries without the\nneed for superpixelation."},
{"paper": "Extensible Hierarchical Method of Detecting Interactive Actions for Video Understanding", "abstract": "For video understanding, namely analyzing who did what in a video, actions along with objects are primary elements. Most studies on actions have handled recognition problems for a well\u2010trimmed video and focused on enhancing their classification performance.However, action detection, including localization as well as recognition, is required because, in general, actions intersect in time and space. In addition, most studies have not considered extensibility for a newly added action that has been previously trained. Therefore, proposed in this paper is an extensible hierarchical method for detecting generic actions, which combine object movements and spatial relations between two objects, and inherited actions, which are determined by the related objects through an ontology and rule based methodology. The hierarchical design of the method enables it to detect any interactive actions based on the spatial relations between two objects. The method using object information achieves an F\u2010measure of 90.27%. Moreover, this paper describes the extensibility of the method for a new action contained in a video from a video domain that is different from the dataset used."},
{"paper": "Negative Margin Matters: Understanding Margin in Few-shot Classification", "abstract": "This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles.These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid falsely mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at https://github.com/bl0/negative-margin.few-shot."},
{"task": "Face Image Retrieval", "papers": {}},
{"paper": "Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters", "abstract": "In this paper, we newly introduce the concept of temporal attention filters,\nand describe how they can be used for human activity recognition from videos. Many high-level activities are often composed of multiple temporal parts (e.g.,\nsub-events) with different duration/speed, and our objective is to make the\nmodel explicitly learn such temporal structure using multiple attention filters\nand benefit from them.Our temporal filters are designed to be fully\ndifferentiable, allowing end-of-end training of the temporal filters together\nwith the underlying frame-based or segment-based convolutional neural network\narchitectures. This paper presents an approach of learning a set of optimal\nstatic temporal attention filters to be shared across different videos, and\nextends this approach to dynamically adjust attention filters per testing video\nusing recurrent long short-term memory networks (LSTMs). This allows our\ntemporal attention filters to learn latent sub-events specific to each\nactivity. We experimentally confirm that the proposed concept of temporal\nattention filters benefits the activity recognition, and we visualize the\nlearned latent sub-events."},
{"task": "Facial Recognition and Modelling", "papers": {}},
{"paper": "Laplacian Regularized Few-Shot Learning", "abstract": "We propose a transductive Laplacian-regularized inference for few-shot tasks. Given any feature embedding learned from the base classes, we minimize a quadratic binary-assignment function containing two terms: (1) a unary term assign- ing query samples to the nearest class prototype, and (2) a pairwise Laplacian term encouraging nearby query samples to have consistent label as- signments.Our transductive inference does not re-train the base model, and can be viewed as a graph clustering of the query set, subject to super- vision constraints from the support set. We derive a computationally efficient bound optimizer of a relaxation of our function, which computes inde- pendent (parallel) updates for each query sample, while guaranteeing convergence. Following a sim- ple cross-entropy training on the base classes, and without complex meta-learning strategies, we con- ducted comprehensive experiments over five few- shot learning benchmarks. Our LaplacianShot consistently outperforms state-of-the-art methods by significant margins across different models, settings, and data sets. Furthermore, our trans- ductive inference is very fast, with computational times that are close to inductive inference, and can be used for large-scale few-shot tasks."},
{"task": "Smile Recognition", "papers": {"0": "Deep Learning For Smile Recognition"}},
{"paper": "Delta-encoder: an effective sample synthesis method for few-shot object recognition", "abstract": "Learning to classify new categories based on just one or a few examples is a\nlong-standing challenge in modern computer vision. In this work, we proposes a\nsimple yet effective method for few-shot (and one-shot) object recognition.Our\napproach is based on a modified auto-encoder, denoted Delta-encoder, that\nlearns to synthesize new samples for an unseen category just by seeing few\nexamples from it. The synthesized samples are then used to train a classifier. The proposed approach learns to both extract transferable intra-class\ndeformations, or \"deltas\", between same-class pairs of training examples, and\nto apply those deltas to the few provided examples of a novel class (unseen\nduring training) in order to efficiently synthesize samples from that new\nclass. The proposed method improves over the state-of-the-art in one-shot\nobject-recognition and compares favorably in the few-shot case. Upon acceptance\ncode will be made available."},
{"paper": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module", "abstract": "The ability to generalize quickly from few observations is crucial for\nintelligent systems. In this paper we introduce APL, an algorithm that\napproximates probability distributions by remembering the most surprising\nobservations it has encountered.These past observations are recalled from an\nexternal memory module and processed by a decoder network that can combine\ninformation from different memory slots to generalize beyond direct recall. We\nshow this algorithm can perform as well as state of the art baselines on\nfew-shot classification benchmarks with a smaller memory footprint. In\naddition, its memory compression allows it to scale to thousands of unknown\nlabels. Finally, we introduce a meta-learning reasoning task which is more\nchallenging than direct classification. In this setting, APL is able to\ngeneralize with fewer than one example per class via deductive reasoning."},
{"task": "Mobile Periocular Recognition", "papers": {}},
{"paper": "ActiveNet: A computer-vision based approach to determine lethargy", "abstract": "The outbreak of COVID-19 has forced everyone to stay indoors, fabricating a significant drop in physical activeness. Our work is constructed upon the idea to formulate a backbone mechanism, to detect levels of activeness in real-time, using a single monocular image of a target person.The scope can be generalized under many applications, be it in an interview, online classes, security surveillance, et cetera. We propose a Computer Vision based multi-stage approach, wherein the pose of a person is first detected, encoded with a novel approach, and then assessed by a classical machine learning algorithm to determine the level of activeness. An alerting system is wrapped around the approach to provide a solution to inhibit lethargy by sending notification alerts to individuals involved."},
{"paper": "Meta-Transfer Learning for Few-Shot Learning", "abstract": "Meta-learning has been proposed as a framework to address the challenging\nfew-shot learning setting. The key idea is to leverage a large number of\nsimilar few-shot tasks in order to learn how to adapt a base-learner to a new\ntask for which only a few labeled samples are available.As deep neural\nnetworks (DNNs) tend to overfit using a few samples only, meta-learning\ntypically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer\nlearning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, \"meta\" refers to training multiple tasks, and \"transfer\" is\nachieved by learning scaling and shifting functions of DNN weights for each\ntask. In addition, we introduce the hard task (HT) meta-batch scheme as an\neffective learning curriculum for MTL. We conduct experiments using (5-class,\n1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot\nlearning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons\nto related works validate that our meta-transfer learning approach trained with\nthe proposed HT meta-batch scheme achieves top performance. An ablation study\nalso shows that both components contribute to fast convergence and high\naccuracy."},
{"paper": "Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels", "abstract": "Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old.Following the recognition that meta-learning is implementing learning in a multi-level model, we present a Bayesian treatment for the meta-learning inner loop through the use of deep kernels. As a result we can learn a kernel that transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classification, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy."},
{"paper": "Synthesized Classifiers for Zero-Shot Learning", "abstract": "Given semantic descriptions of object classes, zero-shot learning aims to\naccurately recognize objects of the unseen classes, from which no examples are\navailable at the training stage, by associating them to the seen classes, from\nwhich labeled examples are provided. We propose to tackle this problem from the\nperspective of manifold learning.Our main idea is to align the semantic space\nthat is derived from external information to the model space that concerns\nitself with recognizing visual features. To this end, we introduce a set of\n\"phantom\" object classes whose coordinates live in both the semantic space and\nthe model space. Serving as bases in a dictionary, they can be optimized from\nlabeled data such that the synthesized real object classifiers achieve optimal\ndiscriminative performance. We demonstrate superior accuracy of our approach\nover the state of the art on four benchmark datasets for zero-shot learning,\nincluding the full ImageNet Fall 2011 dataset with more than 20,000 unseen\nclasses."},
{"paper": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "abstract": "Learning good feature embeddings for images often requires substantial\ntraining data. As a consequence, in settings where training data is limited\n(e.g., few-shot and zero-shot learning), we are typically forced to use a\ngeneric feature embedding across various tasks.Ideally, we want to construct\nfeature embeddings that are tuned for the given task. In this work, we propose\nTask-Aware Feature Embedding Networks (TAFE-Nets) to learn how to adapt the\nimage representation to a new task in a meta learning fashion. Our network is\ncomposed of a meta learner and a prediction network. Based on a task input, the\nmeta learner generates parameters for the feature layers in the prediction\nnetwork so that the feature embedding can be accurately adjusted for that task. We show that TAFE-Net is highly effective in generalizing to new tasks or\nconcepts and evaluate the TAFE-Net on a range of benchmarks in zero-shot and\nfew-shot learning. Our model matches or exceeds the state-of-the-art on all\ntasks. In particular, our approach improves the prediction accuracy of unseen\nattribute-object pairs by 4 to 15 points on the challenging visual\nattribute-object composition task."},
{"task": "3D Facial Expression Recognition", "papers": {"0": "ExpNet: Landmark-Free, Deep, 3D Facial Expressions"}},
{"paper": "Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning", "abstract": "Few-shot learning in image classification aims to learn a classifier to\nclassify images when only few training examples are available for each class. Recent work has achieved promising classification performance, where an\nimage-level feature based measure is usually used.In this paper, we argue that\na measure at such a level may not be effective enough in light of the scarcity\nof examples in few-shot learning. Instead, we think a local descriptor based\nimage-to-class measure should be taken, inspired by its surprising success in\nthe heydays of local invariant features. Specifically, building upon the recent\nepisodic training mechanism, we propose a Deep Nearest Neighbor Neural Network\n(DN4 in short) and train it in an end-to-end manner. Its key difference from\nthe literature is the replacement of the image-level feature based measure in\nthe final layer by a local descriptor based image-to-class measure. This\nmeasure is conducted online via a $k$-nearest neighbor search over the deep\nlocal descriptors of convolutional feature maps. The proposed DN4 not only\nlearns the optimal deep local descriptors for the image-to-class measure, but\nalso utilizes the higher efficiency of such a measure in the case of example\nscarcity, thanks to the exchangeability of visual patterns across the images in\nthe same class. Our work leads to a simple, effective, and computationally\nefficient framework for few-shot learning. Experimental study on benchmark\ndatasets consistently shows its superiority over the related state-of-the-art,\nwith the largest absolute improvement of $17\\%$ over the next best. The source\ncode can be available from \\UrlFont{https://github.com/WenbinLee/DN4.git}."},
{"paper": "Learning Deep Representations of Fine-grained Visual Descriptions", "abstract": "State-of-the-art methods for zero-shot visual recognition formulate learning\nas a joint embedding problem of images and side information. In these\nformulations the current best complement to visual features are attributes:\nmanually encoded vectors describing shared characteristics among categories.Despite good performance, attributes have limitations: (1) finer-grained\nrecognition requires commensurately more attributes, and (2) attributes do not\nprovide a natural language interface. We propose to overcome these limitations\nby training neural language models from scratch; i.e. without pre-training and\nonly consuming words and characters. Our proposed models train end-to-end to\nalign with the fine-grained and category-specific content of images. Natural\nlanguage provides a flexible and compact way of encoding only the salient\nvisual aspects for distinguishing categories. By training on raw text, our\nmodel can do inference on raw text as well, providing humans a familiar mode\nboth for annotation and retrieval. Our model achieves strong performance on\nzero-shot text-based image retrieval and significantly outperforms the\nattribute-based state-of-the-art for zero-shot classification on the Caltech\nUCSD Birds 200-2011 dataset."},
{"paper": "Prototypical Networks for Few-shot Learning", "abstract": "We propose prototypical networks for the problem of few-shot classification,\nwhere a classifier must generalize to new classes not seen in the training set,\ngiven only a small number of examples of each new class. Prototypical networks\nlearn a metric space in which classification can be performed by computing\ndistances to prototype representations of each class.Compared to recent\napproaches for few-shot learning, they reflect a simpler inductive bias that is\nbeneficial in this limited-data regime, and achieve excellent results. We\nprovide an analysis showing that some simple design decisions can yield\nsubstantial improvements over recent approaches involving complicated\narchitectural choices and meta-learning. We further extend prototypical\nnetworks to zero-shot learning and achieve state-of-the-art results on the\nCU-Birds dataset."},
{"paper": "RelationNet2: Deep Comparison Columns for Few-Shot Learning", "abstract": "Few-shot deep learning is a topical challenge area for scaling visual recognition to open ended growth of unseen new classes with limited labeled examples. A promising approach is based on metric learning, which trains a deep embedding to support image similarity matching.Our insight is that effective general purpose matching requires non-linear comparison of features at multiple abstraction levels. We thus propose a new deep comparison network comprised of embedding and relation modules that learn multiple non-linear distance metrics based on different levels of features simultaneously. Furthermore, to reduce over-fitting and enable the use of deeper embeddings, we represent images as distributions rather than vectors via learning parameterized Gaussian noise regularization. The resulting network achieves excellent performance on both miniImageNet and tieredImageNet."},
{"paper": "Task Augmentation by Rotating for Meta-Learning", "abstract": "Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. In this paper, we introduce a task augmentation method by rotating, which increases the number of classes by rotating the original images 90, 180 and 270 degrees, different from traditional augmentation methods which increase the number of images.With a larger amount of classes, we can sample more diverse task instances during training. Therefore, task augmentation by rotating allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach is better than the rotation for increasing the number of images and achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. The code is available on \\url{www.github.com/AceChuse/TaskLevelAug}."},
{"paper": "Meta-Curvature", "abstract": "We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the model-agnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task.For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model's parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task specific techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature."},
{"paper": "Decoder Choice Network for Meta-Learning", "abstract": "Meta-learning has been widely used for implementing few-shot learning and fast model adaptation. One kind of meta-learning methods attempt to learn how to control the gradient descent process in order to make the gradient-based learning have high speed and generalization.This work proposes a method that controls the gradient descent process of the model parameters of a neural network by limiting the model parameters in a low-dimensional latent space. The main challenge of this idea is that a decoder with too many parameters is required. This work designs a decoder with typical structure and shares a part of weights in the decoder to reduce the number of the required parameters. Besides, this work has introduced ensemble learning to work with the proposed approach for improving performance. The results show that the proposed approach is witnessed by the superior performance over the Omniglot classification and the miniImageNet classification tasks."},
{"paper": "Few-Shot Learning with Global Class Representations", "abstract": "In this paper, we propose to tackle the challenging few-shot learning (FSL) problem by learning global class representations using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module.This produces a registered global class representation for computing the classification loss using a query set. Though following a similar episodic training pipeline as existing meta learning based approaches, our method differs significantly in that novel class training samples are involved in the training from the beginning. To compensate for the lack of novel class training samples, an effective sample synthesis strategy is developed to avoid overfitting. Importantly, by joint base-novel class training, our approach can be easily extended to a more practical yet challenging FSL setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. Extensive experiments show that our approach is effective for both of the two FSL settings."},
{"paper": "Self-Supervised Learning For Few-Shot Image Classification", "abstract": "Few-shot image classification aims to classify unseen classes with limited labeled samples. Recent works benefit from the meta-learning process with episodic tasks and can fast adapt to class from training to testing.Due to the limited number of samples for each task, the initial embedding network for meta learning becomes an essential component and can largely affects the performance in practice. To this end, many pre-trained methods have been proposed, and most of them are trained in supervised way with limited transfer ability for unseen classes. In this paper, we proposed to train a more generalized embedding network with self-supervised learning (SSL) which can provide slow and robust representation for downstream tasks by learning from the data itself. We evaluate our work by extensive comparisons with previous baseline methods on two few-shot classification datasets ({\\em i.e.,} MiniImageNet and CUB). Based on the evaluation results, the proposed method achieves significantly better performance, i.e., improve 1-shot and 5-shot tasks by nearly \\textbf{3\\%} and \\textbf{4\\%} on MiniImageNet, by nearly \\textbf{9\\%} and \\textbf{3\\%} on CUB. Moreover, the proposed method can gain the improvement of (\\textbf{15\\%}, \\textbf{13\\%}) on MiniImageNet and (\\textbf{15\\%}, \\textbf{8\\%}) on CUB by pretraining using more unlabeled data. Our code will be available at \\hyperref[https://github.com/phecy/SSL-FEW-SHOT.]{https://github.com/phecy/ssl-few-shot.}"},
{"paper": "Deep Learning For Smile Recognition", "abstract": "Inspired by recent successes of deep learning in computer vision, we propose\na novel application of deep convolutional neural networks to facial expression\nrecognition, in particular smile recognition. A smile recognition test accuracy\nof 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action\n(DISFA) database, significantly outperforming existing approaches based on\nhand-crafted features with accuracies ranging from 65.55% to 79.67%.The\nnovelty of this approach includes a comprehensive model selection of the\narchitecture parameters, allowing to find an appropriate architecture for each\nexpression such as smile. This is feasible because all experiments were run on\na Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations\non a CPU."},
{"paper": "A Hierarchical Context Model for Event Recognition in Surveillance Video", "abstract": "Due to great challenges such as tremendous intra-class variations and low image resolution, context information has been playing a more and more important role for accurate and robust event recognition in surveillance videos. The context information can generally be divided into the feature level context, the semantic level context, and the prior level context.These three levels of context provide crucial bottom-up, middle level, and top down information that can benefit the recognition task itself. Unlike existing researches that generally integrate the context information at one of the three levels, we propose a hierarchical context model that simultaneously exploits contexts at all three levels and systematically incorporate them into event recognition. To tackle the learning and inference challenges brought in by the model hierarchy, we develop complete learning and inference algorithms for the proposed hierarchical context model based on variational Bayes method. Experiments on VIRAT 1.0 and 2.0 Ground Datasets demonstrate the effectiveness of the proposed hierarchical context model for improving the event recognition performance even under great challenges like large intra-class variations and low image resolution."},
{"paper": "Leveraging the Feature Distribution in Transfer-based Few-Shot Learning", "abstract": "Few-shot classification is a challenging problem due to the uncertainty caused by using few labelled samples. In the past few years, transfer-based methods have proved to achieve the best performance, thanks to well-thought-out backbone architectures combined with efficient postprocessing steps.Following this vein, in this paper we propose a transfer-based novel method that builds on two steps: 1) preprocessing the feature vectors so that they become closer to Gaussian-like distributions, and 2) leveraging this preprocessing using an optimal-transport inspired algorithm. Using standardized vision benchmarks, we prove the ability of the proposed methodology to achieve state-of-the-art accuracy with various datasets, backbone architectures and few-shot settings."},
{"paper": "Skeleton-based Action Recognition of People Handling Objects", "abstract": "In visual surveillance systems, it is necessary to recognize the behavior of\npeople handling objects such as a phone, a cup, or a plastic bag. In this\npaper, to address this problem, we propose a new framework for recognizing\nobject-related human actions by graph convolutional networks using human and\nobject poses.In this framework, we construct skeletal graphs of reliable human\nposes by selectively sampling the informative frames in a video, which include\nhuman joints with high confidence scores obtained in pose estimation. The\nskeletal graphs generated from the sampled frames represent human poses related\nto the object position in both the spatial and temporal domains, and these\ngraphs are used as inputs to the graph convolutional networks. Through\nexperiments over an open benchmark and our own data sets, we verify the\nvalidity of our framework in that our method outperforms the state-of-the-art\nmethod for skeleton-based action recognition."},
{"paper": "ExpNet: Landmark-Free, Deep, 3D Facial Expressions", "abstract": "We describe a deep learning based method for estimating 3D facial expression\ncoefficients. Unlike previous work, our process does not relay on facial\nlandmark detection methods as a proxy step.Recent methods have shown that a\nCNN can be trained to regress accurate and discriminative 3D morphable model\n(3DMM) representations, directly from image intensities. By foregoing facial\nlandmark detection, these methods were able to estimate shapes for occluded\nfaces appearing in unprecedented in-the-wild viewing conditions. We build on\nthose methods by showing that facial expressions can also be estimated by a\nrobust, deep, landmark-free approach. Our ExpNet CNN is applied directly to the\nintensities of a face image and regresses a 29D vector of 3D expression\ncoefficients. We propose a unique method for collecting data to train this\nnetwork, leveraging on the robustness of deep networks to training label noise. We further offer a novel means of evaluating the accuracy of estimated\nexpression coefficients: by measuring how well they capture facial emotions on\nthe CK+ and EmotiW-17 emotion recognition benchmarks. We show that our ExpNet\nproduces expression coefficients which better discriminate between facial\nemotions than those obtained using state of the art, facial landmark detection\ntechniques. Moreover, this advantage grows as image scales drop, demonstrating\nthat our ExpNet is more robust to scale changes than landmark detection\nmethods. Finally, at the same level of accuracy, our ExpNet is orders of\nmagnitude faster than its alternatives."},
{"paper": "SlowFast Networks for Video Recognition", "abstract": "We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution.The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast"},
{"paper": "YouTube-8M: A Large-Scale Video Classification Benchmark", "abstract": "Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity\nhardware have reduced the barrier of entry for exploring novel approaches at\nscale.It is possible to train models over millions of examples within a few\ndays. Although large-scale datasets exist for image understanding, such as\nImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video\nclassification dataset, composed of ~8 million videos (500K hours of video),\nannotated with a vocabulary of 4800 visual entities. To get the videos and\ntheir labels, we used a YouTube video annotation system, which labels videos\nwith their main topics. While the labels are machine-generated, they have\nhigh-precision and are derived from a variety of human-based signals including\nmetadata and query click signals. We filtered the video labels (Knowledge Graph\nentities) using both automated and manual curation strategies, including asking\nhuman raters if the labels are visually recognizable. Then, we decoded each\nvideo at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to\nextract the hidden representation immediately prior to the classification\nlayer. Finally, we compressed the frame features and make both the features and\nvideo-level labels available for download. We trained various (modest) classification models on the dataset, evaluated\nthem using popular evaluation metrics, and report them as baselines. Despite\nthe size of the dataset, some of our models train to convergence in less than a\nday on a single machine using TensorFlow. We plan to release code for training\na TensorFlow model and for computing metrics."},
{"paper": "Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition", "abstract": "This paper extends the Spatial-Temporal Graph Convolutional Network (ST-GCN) for skeleton-based action recognition by introducing two novel modules, namely, the Graph Vertex Feature Encoder (GVFE) and the Dilated Hierarchical Temporal Convolutional Network (DH-TCN). On the one hand, the GVFE module learns appropriate vertex features for action recognition by encoding raw skeleton data into a new feature space.On the other hand, the DH-TCN module is capable of capturing both short-term and long-term temporal dependencies using a hierarchical dilated convolutional network. Experiments have been conducted on the challenging NTU RGB-D-60 and NTU RGB-D 120 datasets. The obtained results show that our method competes with state-of-the-art approaches while using a smaller number of layers and parameters; thus reducing the required training time and memory."},
{"paper": "VPN: Learning Video-Pose Embedding for Activities of Daily Living", "abstract": "In this paper, we focus on the spatio-temporal aspect of recognizing Activities of Daily Living (ADL). ADL have two specific properties (i) subtle spatio-temporal patterns and (ii) similar visual patterns varying with time.Therefore, ADL may look very similar and often necessitate to look at their fine-grained details to distinguish them. Because the recent spatio-temporal 3D ConvNets are too rigid to capture the subtle visual patterns across an action, we propose a novel Video-Pose Network: VPN. The 2 key components of this VPN are a spatial embedding and an attention network. The spatial embedding projects the 3D poses and RGB cues in a common semantic space. This enables the action recognition framework to learn better spatio-temporal features exploiting both modalities. In order to discriminate similar actions, the attention network provides two functionalities - (i) an end-to-end learnable pose backbone exploiting the topology of human body, and (ii) a coupler to provide joint spatio-temporal attention weights across a video. Experiments show that VPN outperforms the state-of-the-art results for action classification on a large scale human activity dataset: NTU-RGB+D 120, its subset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota Smarthome and a small scale human-object interaction dataset Northwestern UCLA."},
{"paper": "SCSampler: Sampling Salient Clips from Video for Efficient Action Recognition", "abstract": "While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive.Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight \"clip-sampling\" model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly/uniformly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost."},
{"paper": "Video Classification with Channel-Separated Convolutional Networks", "abstract": "Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks.This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture -- Channel-Separated Convolutional Network (CSN) -- which is simple, efficient, yet accurate. On Sports1M, Kinetics, and Something-Something, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient."},
{"paper": "Multi-granularity Generator for Temporal Action Proposal", "abstract": "Temporal action proposal generation is an important task, aiming to localize\nthe video segments containing human actions in an untrimmed video. In this\npaper, we propose a multi-granularity generator (MGG) to perform the temporal\naction proposal from different granularity perspectives, relying on the video\nvisual features equipped with the position embedding information.First, we\npropose to use a bilinear matching model to exploit the rich local information\nwithin the video sequence. Afterwards, two components, namely segment proposal\nproducer (SPP) and frame actionness producer (FAP), are combined to perform the\ntask of temporal action proposal at two distinct granularities. SPP considers\nthe whole video in the form of feature pyramid and generates segment proposals\nfrom one coarse perspective, while FAP carries out a finer actionness\nevaluation for each video frame. Our proposed MGG can be trained in an\nend-to-end fashion. By temporally adjusting the segment proposals with\nfine-grained frame actionness information, MGG achieves the superior\nperformance over state-of-the-art methods on the public THUMOS-14 and\nActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to\nperform the classification of the proposals generated by MGG, leading to\nsignificant improvements compared against the competing methods for the video\ndetection task."},
{"paper": "PAN: Towards Fast Action Recognition via Learning Persistence of Appearance", "abstract": "Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation.Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000x faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch."},
{"paper": "Mutual Modality Learning for Video Action Classification", "abstract": "The construction of models for video action classification progresses rapidly. However, the performance of those models can still be easily improved by ensembling with the same models trained on different modalities (e.g. Optical flow).Unfortunately, it is computationally expensive to use several modalities during inference. Recent works examine the ways to integrate advantages of multi-modality into a single RGB-model. Yet, there is still a room for improvement. In this paper, we explore the various methods to embed the ensemble power into a single model. We show that proper initialization, as well as mutual modality learning, enhances single-modality models. As a result, we achieve state-of-the-art results in the Something-Something-v2 benchmark."},
{"task": "RF-based Pose Estimation", "papers": {"0": "Making the Invisible Visible: Action Recognition Through Walls and Occlusions"}},
{"task": "Animal Pose Estimation", "papers": {"0": "ImageNet performance correlates with pose estimation robustness and generalization on out-of-domain data"}},
{"task": "Human Pose Forecasting", "papers": {"0": "Learning Trajectory Dependencies for Human Motion Prediction"}},
{"task": "6D Pose Estimation using RGBD", "papers": {"0": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation", "1": "MaskedFusion: Mask-based 6D Object Pose Estimation", "2": "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation", "3": "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation", "4": "Implicit 3D Orientation Learning for 6D Object Detection from RGB Images", "5": "SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again"}},
{"task": "6D Pose Estimation using RGB", "papers": {"0": "EfficientPose -- An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach", "1": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes", "2": "se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains", "3": "Real-Time Seamless Single Shot 6D Object Pose Prediction", "4": "Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation"}},
{"task": "Head Pose Estimation", "papers": {"0": "Deep Ordinal Regression with Label Diversity", "1": "FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation From a Single Image", "2": "Nose, eyes and ears: Head pose estimation by locating facial keypoints"}},
{"task": "Video", "papers": {}},
{"task": "6D Pose Estimation", "papers": {"0": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation", "1": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation", "2": "6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints"}},
{"task": "Multi-Person Pose Estimation", "papers": {"0": "RMPE: Regional Multi-person Pose Estimation", "1": "Distribution-Aware Coordinate Representation for Human Pose Estimation", "2": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation", "3": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation", "4": "DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model", "5": "Learning Temporal Pose Estimation from Sparsely-Labeled Videos", "6": "Learning to Refine Human Pose Estimation", "7": "PoseTrack: Joint Multi-Person Pose Estimation and Tracking"}},
{"task": "Dynamic Region Segmentation", "papers": {}},
{"task": "Deception Detection In Videos", "papers": {}},
{"task": "Pose Estimation", "papers": {"0": "Toward fast and accurate human pose estimation via soft-gated skip connections", "1": "Toward fast and accurate human pose estimation via soft-gated skip connections", "2": "Distribution-Aware Coordinate Representation for Human Pose Estimation", "3": "LSTM Pose Machines", "4": "LSTM Pose Machines", "5": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map", "6": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map", "7": "Parsing R-CNN for Instance-Level Human Analysis", "8": "Stacked Hourglass Networks for Human Pose Estimation", "9": "Stacked Hourglass Networks for Human Pose Estimation", "10": "Rethinking on Multi-Stage Networks for Human Pose Estimation", "11": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image", "12": "Learning to Refine Human Pose Estimation"}},
{"task": "Object Discovery In Videos", "papers": {}},
{"task": "Pornography Detection In Videos", "papers": {}},
{"task": "Keypoint Detection", "papers": {"0": "PoseFix: Model-agnostic General Human Pose Refinement Network", "1": "Deep High-Resolution Representation Learning for Human Pose Estimation", "2": "Rethinking on Multi-Stage Networks for Human Pose Estimation", "3": "6-DoF Object Pose from Semantic Keypoints"}},
{"task": "Natural Language Moment Retrieval", "papers": {}},
{"task": "Video Interlacing", "papers": {}},
{"task": "3D Human Pose Estimation", "papers": {"0": "Learnable Triangulation of Human Pose", "1": "3D Human Pose Estimation with 2D Marginal Heatmaps", "2": "3D Human Motion Estimation via Motion Compression and Refinement", "3": "3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training", "4": "Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach", "5": "Beyond Weak Perspective for Monocular 3D Human Pose Estimation", "6": "A simple yet effective baseline for 3d human pose estimation", "7": "Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D Human Pose Estimation", "8": "An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge", "9": "3D Human Pose Estimation from a Single Image via Distance Matrix Regression", "10": "3D Pictorial Structures for Multiple Human Pose Estimation"}},
{"task": "Video Story QA", "papers": {"0": "Progressive Attention Memory Network for Movie Story Question Answering"}},
{"task": "Video Deinterlacing", "papers": {"0": "Real-time Deep Video Deinterlacing"}},
{"paper": "SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again", "abstract": "We present a novel method for detecting 3D model instances and estimating\ntheir 6D poses from RGB data in a single shot. To this end, we extend the\npopular SSD paradigm to cover the full 6D pose space and train on synthetic\nmodel data only.Our approach competes or surpasses current state-of-the-art\nmethods that leverage RGB-D data on multiple challenging datasets. Furthermore,\nour method produces these results at around 10Hz, which is many times faster\nthan the related methods. For the sake of reproducibility, we make our trained\nnetworks and detection code publicly available."},
{"paper": "Nose, eyes and ears: Head pose estimation by locating facial keypoints", "abstract": "Monocular head pose estimation requires learning a model that computes the\nintrinsic Euler angles for pose (yaw, pitch, roll) from an input image of human\nface. Annotating ground truth head pose angles for images in the wild is\ndifficult and requires ad-hoc fitting procedures (which provides only coarse\nand approximate annotations).This highlights the need for approaches which can\ntrain on data captured in controlled environment and generalize on the images\nin the wild (with varying appearance and illumination of the face). Most\npresent day deep learning approaches which learn a regression function directly\non the input images fail to do so. To this end, we propose to use a higher\nlevel representation to regress the head pose while using deep learning\narchitectures. More specifically, we use the uncertainty maps in the form of 2D\nsoft localization heatmap images over five facial keypoints, namely left ear,\nright ear, left eye, right eye and nose, and pass them through an convolutional\nneural network to regress the head-pose. We show head pose estimation results\non two challenging benchmarks BIWI and AFLW and our approach surpasses the\nstate of the art on both the datasets."},
{"paper": "Implicit 3D Orientation Learning for 6D Object Detection from RGB Images", "abstract": "We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization.This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results."},
{"paper": "Rethinking on Multi-Stage Networks for Human Pose Estimation", "abstract": "Existing pose estimation approaches fall into two categories: single-stage and multi-stage methods. While multi-stage methods are seemingly more suited for the task, their performance in current practice is not as good as single-stage methods.This work studies this issue. We argue that the current multi-stage methods' unsatisfactory performance comes from the insufficiency in various design choices. We propose several improvements, including the single-stage module design, cross stage feature aggregation, and coarse-to-fine supervision. The resulting method establishes the new state-of-the-art on both MS COCO and MPII Human Pose dataset, justifying the effectiveness of a multi-stage architecture. The source code is publicly available for further research."},
{"paper": "Stacked Hourglass Networks for Human Pose Estimation", "abstract": "This work introduces a novel convolutional network architecture for the task\nof human pose estimation. Features are processed across all scales and\nconsolidated to best capture the various spatial relationships associated with\nthe body.We show how repeated bottom-up, top-down processing used in\nconjunction with intermediate supervision is critical to improving the\nperformance of the network. We refer to the architecture as a \"stacked\nhourglass\" network based on the successive steps of pooling and upsampling that\nare done to produce a final set of predictions. State-of-the-art results are\nachieved on the FLIC and MPII benchmarks outcompeting all recent methods."},
{"paper": "FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation From a Single Image", "abstract": "This paper proposes a method for head pose estimation from a single image. Previous methods often predict head poses through landmark or depth estimation and would require more computation than necessary.Our method is based on regression and feature aggregation. For having a compact model, we employ the soft stagewise regression scheme. Existing feature aggregation methods treat inputs as a bag of features and thus ignore their spatial relationship in a feature map. We propose to learn a fine-grained structure mapping for spatially grouping features before aggregation. The fine-grained structure provides part-based information and pooled values. By utilizing learnable and non-learnable importance over the spatial location, different model variants can be generated and form a complementary ensemble. Experiments show that our method outperforms the state-of-the-art methods including both the landmark-free ones and the ones based on landmark or depth estimation. With only a single RGB frame as input, our method even outperforms methods utilizing multi-modality information (RGB-D, RGB-Time) on estimating the yaw angle. Furthermore, the memory overhead of our model is 100 times smaller than those of previous methods."},
{"paper": "Learning to Refine Human Pose Estimation", "abstract": "Multi-person pose estimation in images and videos is an important yet\nchallenging task with many applications. Despite the large improvements in\nhuman pose estimation enabled by the development of convolutional neural\nnetworks, there still exist a lot of difficult cases where even the\nstate-of-the-art models fail to correctly localize all body joints.This\nmotivates the need for an additional refinement step that addresses these\nchallenging cases and can be easily applied on top of any existing method. In\nthis work, we introduce a pose refinement network (PoseRefiner) which takes as\ninput both the image and a given pose estimate and learns to directly predict a\nrefined pose by jointly reasoning about the input-output space. In order for\nthe network to learn to refine incorrect body joint predictions, we employ a\nnovel data augmentation scheme for training, where we model \"hard\" human pose\ncases. We evaluate our approach on four popular large-scale pose estimation\nbenchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack\nPose Estimation, and PoseTrack Pose Tracking, and report systematic improvement\nover the state of the art."},
{"paper": "Learning Temporal Pose Estimation from Sparsely-Labeled Videos", "abstract": "Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive.To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper."},
{"paper": "6-DoF Object Pose from Semantic Keypoints", "abstract": "This paper presents a novel approach to estimating the continuous six degree\nof freedom (6-DoF) pose (3D translation and rotation) of an object from a\nsingle RGB image. The approach combines semantic keypoints predicted by a\nconvolutional network (convnet) with a deformable shape model.Unlike prior\nwork, we are agnostic to whether the object is textured or textureless, as the\nconvnet learns the optimal representation from the available training image\ndata. Furthermore, the approach can be applied to instance- and class-based\npose recovery. Empirically, we show that the proposed approach can accurately\nrecover the 6-DoF object pose for both instance- and class-based scenarios with\na cluttered background. For class-based object pose estimation,\nstate-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset."},
{"paper": "LSTM Pose Machines", "abstract": "We observed that recent state-of-the-art results on single image human pose\nestimation were achieved by multi-stage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of\nthese models on videos is not only computationally intensive, it also suffers\nfrom performance degeneration and flicking.Such suboptimal results are mainly\nattributed to the inability of imposing sequential geometric consistency,\nhandling severe image quality degradation (e.g. motion blur and occlusion) as\nwell as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the\nmulti-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and\nresults in significantly faster speed in invoking the network for videos. It\nalso enables the adoption of Long Short-Term Memory (LSTM) units between video\nframes. We found such memory augmented RNN is very effective in imposing\ngeometric consistency among frames. It also well handles input quality\ndegradation in videos while successfully stabilizes the sequential outputs. The\nexperiments showed that our approach significantly outperformed current\nstate-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why\nsuch mechanism would benefit the prediction for video-based pose estimations."},
{"paper": "Parsing R-CNN for Instance-Level Human Analysis", "abstract": "Instance-level human analysis is common in real-life scenarios and has\nmultiple manifestations, such as human part segmentation, dense pose\nestimation, human-object interactions, etc. Models need to distinguish\ndifferent human instances in the image panel and learn rich features to\nrepresent the details of each instance.In this paper, we present an end-to-end\npipeline for solving the instance-level human analysis, named Parsing R-CNN. It\nprocesses a set of human instances simultaneously through comprehensive\nconsidering the characteristics of region-based approach and the appearance of\na human, thus allowing representing the details of instances. Parsing R-CNN is\nvery flexible and efficient, which is applicable to many issues in human\ninstance analysis. Our approach outperforms all state-of-the-art methods on\nCIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and\nDensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st\nplace in the COCO 2018 Challenge DensePose Estimation task. Code and models are\npublic available."},
{"paper": "Deep High-Resolution Representation Learning for Human Pose Estimation", "abstract": "This is an official pytorch implementation of Deep High-Resolution\nRepresentation Learning for Human Pose Estimation. In this work, we are\ninterested in the human pose estimation problem with a focus on learning\nreliable high-resolution representations.Most existing methods recover\nhigh-resolution representations from low-resolution representations produced by\na high-to-low resolution network. Instead, our proposed network maintains\nhigh-resolution representations through the whole process. We start from a\nhigh-resolution subnetwork as the first stage, gradually add high-to-low\nresolution subnetworks one by one to form more stages, and connect the\nmutli-resolution subnetworks in parallel. We conduct repeated multi-scale\nfusions such that each of the high-to-low resolution representations receives\ninformation from other parallel representations over and over, leading to rich\nhigh-resolution representations. As a result, the predicted keypoint heatmap is\npotentially more accurate and spatially more precise. We empirically\ndemonstrate the effectiveness of our network through the superior pose\nestimation results over two benchmark datasets: the COCO keypoint detection\ndataset and the MPII Human Pose dataset. The code and models have been publicly\navailable at\n\\url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}."},
{"paper": "PoseFix: Model-agnostic General Human Pose Refinement Network", "abstract": "Multi-person pose estimation from a 2D image is an essential technique for\nhuman behavior understanding. In this paper, we propose a human pose refinement\nnetwork that estimates a refined pose from a tuple of an input image and input\npose.The pose refinement was performed mainly through an end-to-end trainable\nmulti-stage architecture in previous methods. However, they are highly\ndependent on pose estimation models and require careful model design. By\ncontrast, we propose a model-agnostic pose refinement method. According to a\nrecent study, state-of-the-art 2D human pose estimation methods have similar\nerror distributions. We use this error statistics as prior information to\ngenerate synthetic poses and use the synthesized poses to train our model. In\nthe testing stage, pose estimation results of any other methods can be input to\nthe proposed method. Moreover, the proposed model does not require code or\nknowledge about other methods, which allows it to be easily used in the\npost-processing step. We show that the proposed approach achieves better\nperformance than the conventional multi-stage refinement models and\nconsistently improves the performance of various state-of-the-art pose\nestimation methods on the commonly used benchmark. The code is available in\nthis https URL\\footnote{\\url{https://github.com/mks0601/PoseFix_RELEASE}}."},
{"paper": "Real-time Deep Video Deinterlacing", "abstract": "Interlacing is a widely used technique, for television broadcast and video\nrecording, to double the perceived frame rate without increasing the bandwidth. But it presents annoying visual artifacts, such as flickering and silhouette\n\"serration,\" during the playback.Existing state-of-the-art deinterlacing\nmethods either ignore the temporal information to provide real-time performance\nbut lower visual quality, or estimate the motion for better deinterlacing but\nwith a trade-off of higher computational cost. In this paper, we present the\nfirst and novel deep convolutional neural networks (DCNNs) based method to\ndeinterlace with high visual quality and real-time performance. Unlike existing\nmodels for super-resolution problems which relies on the translation-invariant\nassumption, our proposed DCNN model utilizes the temporal information from both\nthe odd and even half frames to reconstruct only the missing scanlines, and\nretains the given odd and even scanlines for producing the full deinterlaced\nframes. By further introducing a layer-sharable architecture, our system can\nachieve real-time performance on a single GPU. Experiments shows that our\nmethod outperforms all existing methods, in terms of reconstruction accuracy\nand computational performance."},
{"paper": "Progressive Attention Memory Network for Movie Story Question Answering", "abstract": "This paper proposes the progressive attention memory network (PAMN) for movie\nstory question answering (QA). Movie story QA is challenging compared to VQA in\ntwo aspects: (1) pinpointing the temporal parts relevant to answer the question\nis difficult as the movies are typically longer than an hour, (2) it has both\nvideo and subtitle where different questions require different modality to\ninfer the answer.To overcome these challenges, PAMN involves three main\nfeatures: (1) progressive attention mechanism that utilizes cues from both\nquestion and answer to progressively prune out irrelevant temporal parts in\nmemory, (2) dynamic modality fusion that adaptively determines the contribution\nof each modality for answering the current question, and (3) belief correction\nanswering scheme that successively corrects the prediction score on each\ncandidate answer. Experiments on publicly available benchmark datasets, MovieQA\nand TVQA, demonstrate that each feature contributes to our movie story QA\narchitecture, PAMN, and improves performance to achieve the state-of-the-art\nresult. Qualitative analysis by visualizing the inference mechanism of PAMN is\nalso provided."},
{"paper": "Toward fast and accurate human pose estimation via soft-gated skip connections", "abstract": "This paper is on highly accurate and highly efficient human pose estimation. Recent works based on Fully Convolutional Networks (FCNs) have demonstrated excellent results for this difficult problem.While residual connections within FCNs have proved to be quintessential for achieving high accuracy, we re-analyze this design choice in the context of improving both the accuracy and the efficiency over the state-of-the-art. In particular, we make the following contributions: (a) We propose gated skip connections with per-channel learnable parameters to control the data flow for each channel within the module within the macro-module. (b) We introduce a hybrid network that combines the HourGlass and U-Net architectures which minimizes the number of identity connections within the network and increases the performance for the same parameter budget. Our model achieves state-of-the-art results on the MPII and LSP datasets. In addition, with a reduction of 3x in model size and complexity, we show no decrease in performance when compared to the original HourGlass network."},
{"paper": "3D Human Pose Estimation from a Single Image via Distance Matrix Regression", "abstract": "This paper addresses the problem of 3D human pose estimation from a single\nimage. We follow a standard two-step pipeline by first detecting the 2D\nposition of the $N$ body joints, and then using these observations to infer 3D\npose.For the first step, we use a recent CNN-based detector. For the second\nstep, most existing approaches perform 2$N$-to-3$N$ regression of the Cartesian\njoint coordinates. We show that more precise pose estimates can be obtained by\nrepresenting both the 2D and 3D human poses using $N\\times N$ distance\nmatrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network\narchitectures, which by construction, enforce positivity and symmetry of the\npredicted matrices. The approach has also the advantage to naturally handle\nmissing observations and allowing to hypothesize the position of non-observed\njoints. Quantitative results on Humaneva and Human3.6M datasets demonstrate\nconsistent performance gains over state-of-the-art. Qualitative evaluation on\nthe images in-the-wild of the LSP dataset, using the regressor learned on\nHuman3.6M, reveals very promising generalization results."},
{"paper": "3D Pictorial Structures for Multiple Human Pose Estimation", "abstract": "In this work, we address the problem of 3D pose estimation of multiple humans from multiple views. This is a more challenging problem than single human 3D pose estimation due to the much larger state space, partial occlusions as well as across view ambiguities when not knowing the identity of the humans in advance.To address these problems, we first create a reduced state space by triangulation of corresponding body joints obtained from part detectors  in pairs of camera views. In order to resolve the ambiguities of wrong and mixed body parts of multiple humans after triangulation and also those coming from false positive body part detections, we introduce a novel 3D pictorial structures (3DPS) model. Our model infers 3D human body configurations from our reduced state space. The 3DPS model is generic and applicable to both single and multiple human pose estimation. In order to compare to the state-of-the art, we first evaluate our method on single human 3D pose estimation on HumanEva-I [22] and  KTH Multiview Football Dataset II [8] datasets. Then, we introduce and evaluate our method on two datasets for multiple human 3D pose estimation."},
{"paper": "An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge", "abstract": "For the ECCV 2018 PoseTrack Challenge, we present a 3D human pose estimation\nsystem based mainly on the integral human pose regression method. We show a\ncomprehensive ablation study to examine the key performance factors of the\nproposed system.Our system obtains 47mm MPJPE on the CHALL_H80K test dataset,\nplacing second in the ECCV2018 3D human pose estimation challenge. Code will be\nreleased to facilitate future work."},
{"paper": "PoseTrack: Joint Multi-Person Pose Estimation and Tracking", "abstract": "In this work, we introduce the challenging problem of joint multi-person pose\nestimation and tracking of an unknown number of persons in unconstrained\nvideos. Existing methods for multi-person pose estimation in images cannot be\napplied directly to this problem, since it also requires to solve the problem\nof person association over time in addition to the pose estimation for each\nperson.We therefore propose a novel method that jointly models multi-person\npose estimation and tracking in a single formulation. To this end, we represent\nbody joint detections in a video by a spatio-temporal graph and solve an\ninteger linear program to partition the graph into sub-graphs that correspond\nto plausible body pose trajectories for each person. The proposed approach\nimplicitly handles occlusion and truncation of persons. Since the problem has\nnot been addressed quantitatively in the literature, we introduce a challenging\n\"Multi-Person PoseTrack\" dataset, and also propose a completely unconstrained\nevaluation protocol that does not make any assumptions about the scale, size,\nlocation or the number of persons. Finally, we evaluate the proposed approach\nand several baseline methods on our new dataset."},
{"paper": "Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D Human Pose Estimation", "abstract": "Monocular estimation of 3d human pose has attracted increased attention with the availability of large ground-truth motion capture datasets. However, the diversity of training data available is limited and it is not clear to what extent methods generalize outside the specific datasets they are trained on.In this work we carry out a systematic study of the diversity and biases present in specific datasets and its effect on cross-dataset generalization across a compendium of 5 pose datasets. We specifically focus on systematic differences in the distribution of camera viewpoints relative to a body-centered coordinate frame. Based on this observation, we propose an auxiliary task of predicting the camera viewpoint in addition to pose. We find that models trained to jointly predict viewpoint and pose systematically show significantly improved cross-dataset generalization."},
{"paper": "A simple yet effective baseline for 3d human pose estimation", "abstract": "Following the success of deep convolutional networks, state-of-the-art\nmethods for 3d human pose estimation have focused on deep end-to-end systems\nthat predict 3d joint locations given raw image pixels. Despite their excellent\nperformance, it is often not easy to understand whether their remaining error\nstems from a limited 2d pose (visual) understanding, or from a failure to map\n2d poses into 3-dimensional positions.With the goal of understanding these\nsources of error, we set out to build a system that given 2d joint locations\npredicts 3d positions. Much to our surprise, we have found that, with current\ntechnology, \"lifting\" ground truth 2d joint locations to 3d space is a task\nthat can be solved with a remarkably low error rate: a relatively simple deep\nfeed-forward network outperforms the best reported result by about 30\\% on\nHuman3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf\nstate-of-the-art 2d detector (\\ie, using images as input) yields state of the\nart results -- this includes an array of systems that have been trained\nend-to-end specifically for this task. Our results indicate that a large\nportion of the error of modern deep 3d pose estimation systems stems from their\nvisual analysis, and suggests directions to further advance the state of the\nart in 3d human pose estimation."},
{"paper": "Beyond Weak Perspective for Monocular 3D Human Pose Estimation", "abstract": "We consider the task of 3D joints location and orientation prediction from a monocular video with the skinned multi-person linear (SMPL) model. We first infer 2D joints locations with an off-the-shelf pose estimation algorithm.We use the SPIN algorithm and estimate initial predictions of body pose, shape and camera parameters from a deep regression neural network. We then adhere to the SMPLify algorithm which receives those initial parameters, and optimizes them so that inferred 3D joints from the SMPL model would fit the 2D joints locations. This algorithm involves a projection step of 3D joints to the 2D image plane. The conventional approach is to follow weak perspective assumptions which use ad-hoc focal length. Through experimentation on the 3D Poses in the Wild (3DPW) dataset, we show that using full perspective projection, with the correct camera center and an approximated focal length, provides favorable results. Our algorithm has resulted in a winning entry for the 3DPW Challenge, reaching first place in joints orientation accuracy."},
{"paper": "Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach", "abstract": "We propose to estimate 3D human pose from multi-view images and a few IMUs attached at person's limbs. It operates by firstly detecting 2D poses from the two signals, and then lifting them to the 3D space.We present a geometric approach to reinforce the visual features of each pair of joints based on the IMUs. This notably improves 2D pose estimation accuracy especially when one joint is occluded. We call this approach Orientation Regularized Network (ORN). Then we lift the multi-view 2D poses to the 3D space by an Orientation Regularized Pictorial Structure Model (ORPSM) which jointly minimizes the projection error between the 3D and 2D poses, along with the discrepancy between the 3D pose and IMU orientations. The simple two-step approach reduces the error of the state-of-the-art by a large margin on a public dataset. Our code will be released at https://github.com/CHUNYUWANG/imu-human-pose-pytorch."},
{"paper": "3D Human Motion Estimation via Motion Compression and Refinement", "abstract": "We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our method, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement.This two-step encoding of human motion captures human motion in two stages: a general human motion estimation step that captures the coarse overall motion, and a residual estimation that adds back person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates."},
{"paper": "Learnable Triangulation of Human Pose", "abstract": "We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images.The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow modelling a human pose prior. Crucially, both approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page (https://saic-violet.github.io/learnable-triangulation)."},
{"paper": "3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training", "abstract": "Estimating 3D poses from a monocular video is still a challenging task, despite the significant progress that has been made in recent years. Generally, the performance of existing methods drops when the target person is too small/large, or the motion is too fast/slow relative to the scale and speed of the training data.Moreover, to our knowledge, many of these methods are not designed or trained under severe occlusion explicitly, making their performance on handling occlusion compromised. Addressing these problems, we introduce a spatio-temporal network for robust 3D human pose estimation. As humans in videos may appear in different scales and have various motion speeds, we apply multi-scale spatial features for 2D joints or keypoints prediction in each individual frame, and multi-stride temporal convolutional net-works (TCNs) to estimate 3D joints or keypoints. Furthermore, we design a spatio-temporal discriminator based on body structures as well as limb motions to assess whether the predicted pose forms a valid pose and a valid movement. During training, we explicitly mask out some keypoints to simulate various occlusion cases, from minor to severe occlusion, so that our network can learn better and becomes robust to various degrees of occlusion. As there are limited 3D ground-truth data, we further utilize 2D video data to inject a semi-supervised learning capability to our network. Experiments on public datasets validate the effectiveness of our method, and our ablation studies show the strengths of our network\\'s individual submodules."},
{"paper": "3D Human Pose Estimation with 2D Marginal Heatmaps", "abstract": "Automatically determining three-dimensional human pose from monocular RGB\nimage data is a challenging problem. The two-dimensional nature of the input\nresults in intrinsic ambiguities which make inferring depth particularly\ndifficult.Recently, researchers have demonstrated that the flexible\nstatistical modelling capabilities of deep neural networks are sufficient to\nmake such inferences with reasonable accuracy. However, many of these models\nuse coordinate output techniques which are memory-intensive, not\ndifferentiable, and/or do not spatially generalise well. We propose\nimprovements to 3D coordinate prediction which avoid the aforementioned\nundesirable traits by predicting 2D marginal heatmaps under an augmented\nsoft-argmax scheme. Our resulting model, MargiPose, produces visually coherent\nheatmaps whilst maintaining differentiability. We are also able to achieve\nstate-of-the-art accuracy on publicly available 3D human pose estimation data."},
{"paper": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation", "abstract": "Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids.Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation."},
{"paper": "DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model", "abstract": "The goal of this paper is to advance the state-of-the-art of articulated pose\nestimation in scenes with multiple people. To that end we contribute on three\nfronts.We propose (1) improved body part detectors that generate effective\nbottom-up proposals for body parts; (2) novel image-conditioned pairwise terms\nthat allow to assemble the proposals into a variable number of consistent body\npart configurations; and (3) an incremental optimization strategy that explores\nthe search space more efficiently thus leading both to better performance and\nsignificant speed-up factors. Evaluation is done on two single-person and two\nmulti-person pose estimation benchmarks. The proposed approach significantly\noutperforms best known multi-person pose estimation results while demonstrating\ncompetitive performance on the task of single person pose estimation. Models\nand code available at http://pose.mpi-inf.mpg.de"},
{"paper": "RMPE: Regional Multi-person Pose Estimation", "abstract": "Multi-person pose estimation in the wild is challenging. Although\nstate-of-the-art human detectors have demonstrated good performance, small\nerrors in localization and recognition are inevitable.These errors can cause\nfailures for a single-person pose estimator (SPPE), especially for methods that\nsolely depend on human detection results. In this paper, we propose a novel\nregional multi-person pose estimation (RMPE) framework to facilitate pose\nestimation in the presence of inaccurate human bounding boxes. Our framework\nconsists of three components: Symmetric Spatial Transformer Network (SSTN),\nParametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals\nGenerator (PGPG). Our method is able to handle inaccurate bounding boxes and\nredundant detections, allowing it to achieve a 17% increase in mAP over the\nstate-of-the-art methods on the MPII (multi person) dataset.Our model and\nsource codes are publicly available."},
{"paper": "Distribution-Aware Coordinate Representation for Human Pose Estimation", "abstract": "While being the de facto standard coordinate representation in human pose estimation, heatmap is never systematically investigated in the literature, to our best knowledge. This work fills this gap by studying the coordinate representation with a particular focus on the heatmap.Interestingly, we found that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method widely used by existing methods, and propose a more principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO, consistently validating the usefulness and effectiveness of our novel coordinate representation idea."},
{"paper": "6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints", "abstract": "We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real-time novel object instances of known object categories such as bowls, laptops, and mugs.6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking."},
{"paper": "Real-Time Seamless Single Shot 6D Object Pose Prediction", "abstract": "We propose a single-shot approach for simultaneously detecting an object in\nan RGB image and predicting its 6D pose without requiring multiple stages or\nhaving to examine multiple hypotheses. Unlike a recently proposed single-shot\ntechnique for this task (Kehl et al., ICCV'17) that only predicts an\napproximate 6D pose that must then be refined, ours is accurate enough not to\nrequire additional post-processing.As a result, it is much faster - 50 fps on\na Titan X (Pascal) GPU - and more suitable for real-time processing. The key\ncomponent of our method is a new CNN architecture inspired by the YOLO network\ndesign that directly predicts the 2D image locations of the projected vertices\nof the object's 3D bounding box. The object's 6D pose is then estimated using a\nPnP algorithm. For single object and multiple object pose estimation on the LINEMOD and\nOCCLUSION datasets, our approach substantially outperforms other recent\nCNN-based approaches when they are all used without post-processing. During\npost-processing, a pose refinement step can be used to boost the accuracy of\nthe existing methods, but at 10 fps or less, they are much slower than our\nmethod."},
{"paper": "se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains", "abstract": "Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose.This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz."},
{"paper": "Deep Ordinal Regression with Label Diversity", "abstract": "Regression via classification (RvC) is a common method used for regression problems in deep learning, where the target variable belongs to a set of continuous values. By discretizing the target into a set of non-overlapping classes, it has been shown that training a classifier can improve neural network accuracy compared to using a standard regression approach.However, it is not clear how the set of discrete classes should be chosen and how it affects the overall solution. In this work, we propose that using several discrete data representations simultaneously can improve neural network learning compared to a single representation. Our approach is end-to-end differentiable and can be added as a simple extension to conventional learning methods, such as deep neural networks. We test our method on three challenging tasks and show that our method reduces the prediction error compared to a baseline RvC approach while maintaining a similar model complexity."},
{"paper": "EfficientPose -- An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach", "abstract": "In this paper we introduce EfficientPose, a new approach for 6D object pose estimation. Our method is highly accurate, efficient and scalable over a wide range of computational resources.Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves a new state-of-theart accuracy of 97.35% in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at https://github.com/ybkscht/EfficientPose."},
{"paper": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes", "abstract": "Estimating the 6D pose of known objects is important for robots to interact\nwith the real world. The problem is challenging due to the variety of objects\nas well as the complexity of a scene caused by clutter and occlusions between\nobjects.In this work, we introduce PoseCNN, a new Convolutional Neural Network\nfor 6D object pose estimation. PoseCNN estimates the 3D translation of an\nobject by localizing its center in the image and predicting its distance from\nthe camera. The 3D rotation of the object is estimated by regressing to a\nquaternion representation. We also introduce a novel loss function that enables\nPoseCNN to handle symmetric objects. In addition, we contribute a large scale\nvideo dataset for 6D object pose estimation named the YCB-Video dataset. Our\ndataset provides accurate 6D poses of 21 objects from the YCB dataset observed\nin 92 videos with 133,827 frames. We conduct extensive experiments on our\nYCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is\nhighly robust to occlusions, can handle symmetric objects, and provide accurate\npose estimation using only color images as input. When using depth data to\nfurther refine the poses, our approach achieves state-of-the-art results on the\nchallenging OccludedLINEMOD dataset. Our code and dataset are available at\nhttps://rse-lab.cs.washington.edu/projects/posecnn/."},
{"paper": "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation", "abstract": "The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to \"instance-level\" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time.To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks."},
{"paper": "MaskedFusion: Mask-based 6D Object Pose Estimation", "abstract": "MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data.Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3% on average using the ADD metric on the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods. The code is available on GitHub (https://github.com/kroglice/MaskedFusion)."},
{"paper": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation", "abstract": "In this work, we present a novel data-driven method for robust 6DoF object pose estimation from a single RGBD image. Unlike previous methods that directly regressing pose parameters, we tackle this challenging task with a keypoint-based approach.Specifically, we propose a deep Hough voting network to detect 3D keypoints of objects and then estimate the 6D pose parameters within a least-squares fitting manner. Our method is a natural extension of 2D-keypoint approaches that successfully work on RGB based 6DoF estimation. It allows us to fully utilize the geometric constraint of rigid objects with the extra depth information and is easy for a network to learn and optimize. Extensive experiments were conducted to demonstrate the effectiveness of 3D-keypoint detection in the 6D pose estimation task. Experimental results also show our method outperforms the state-of-the-art methods by large margins on several benchmarks. Code and video are available at https://github.com/ethnhe/PVN3D.git."},
{"paper": "Learning Trajectory Dependencies for Human Motion Prediction", "abstract": "Human motion prediction, i.e., forecasting future body poses given observed pose sequence, has typically been tackled with recurrent neural networks (RNNs). However, as evidenced by prior work, the resulted RNN models suffer from prediction errors accumulation, leading to undesired discontinuities in motion prediction.In this paper, we propose a simple feed-forward deep network for motion prediction, which takes into account both temporal smoothness and spatial dependencies among human body joints. In this context, we then propose to encode temporal information by working in trajectory space, instead of the traditionally-used pose space. This alleviates us from manually defining the range of temporal dependencies (or temporal convolutional filter size, as done in previous work). Moreover, spatial dependency of human pose is encoded by treating a human pose as a generic graph (rather than a human skeletal kinematic tree) formed by links between every pair of body joints. Instead of using a pre-defined graph structure, we design a new graph convolutional network to learn graph connectivity automatically. This allows the network to capture long range dependencies beyond that of human kinematic tree. We evaluate our approach on several standard benchmark datasets for motion prediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our experiments clearly demonstrate that the proposed approach achieves state of the art performance, and is applicable to both angle-based and position-based pose representations. The code is available at https://github.com/wei-mao-2019/LearnTrajDep"},
{"paper": "ImageNet performance correlates with pose estimation robustness and generalization on out-of-domain data", "abstract": "Neural networks are highly effective tools for pose estimation. However, robustness to outof-domain data remains a challenge, especially for small training sets that are common for real world applications.Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets). We developed a novel dataset of 30 horses that allowed\r\nfor both \u201cwithin-domain\u201d and \u201cout-of-domain\u201d (unseen horse) benchmarking - this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. Our results demonstrate that transfer learning is beneficial for out-of-domain robustness."},
{"paper": "Making the Invisible Visible: Action Recognition Through Walls and Occlusions", "abstract": "Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community.But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition."},
{"task": "Video Synchronization", "papers": {}},
{"task": "Action Spotting", "papers": {}},
{"task": "Localization In Video Forgery", "papers": {}},
{"paper": "Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation", "abstract": "Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices.To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images."},
{"task": "Video Similarity", "papers": {}},
{"task": "Video Compressive Sensing", "papers": {}},
{"task": "Supervised Video Summarization", "papers": {"0": "Discriminative Feature Learning for Unsupervised Video Summarization", "1": "Discriminative Feature Learning for Unsupervised Video Summarization"}},
{"task": "Unsupervised Video Summarization", "papers": {"0": "Discriminative Feature Learning for Unsupervised Video Summarization", "1": "Discriminative Feature Learning for Unsupervised Video Summarization"}},
{"task": "Video Background Subtraction", "papers": {}},
{"task": "Video Denoising", "papers": {"0": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation", "1": "DVDnet: A Fast Network for Deep Video Denoising", "2": "DVDnet: A Fast Network for Deep Video Denoising", "3": "DVDnet: A Fast Network for Deep Video Denoising", "4": "DVDnet: A Fast Network for Deep Video Denoising", "5": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation", "6": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation", "7": "DVDnet: A Fast Network for Deep Video Denoising", "8": "DVDnet: A Fast Network for Deep Video Denoising", "9": "DVDnet: A Fast Network for Deep Video Denoising"}},
{"task": "Predict Future Video Frames", "papers": {}},
{"task": "Video-to-Video Synthesis", "papers": {"0": "Few-shot Video-to-Video Synthesis", "1": "Few-shot Video-to-Video Synthesis"}},
{"task": "Video Summarization", "papers": {"0": "Summarizing Videos with Attention", "1": "Summarizing Videos with Attention"}},
{"task": "Video Description", "papers": {}},
{"task": "Motion Compensation", "papers": {}},
{"task": "Video Compression", "papers": {}},
{"task": "Video Recognition", "papers": {}},
{"task": "Video Frame Interpolation", "papers": {"0": "Softmax Splatting for Video Frame Interpolation", "1": "Softmax Splatting for Video Frame Interpolation", "2": "Softmax Splatting for Video Frame Interpolation", "3": "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution"}},
{"task": "Video Retrieval", "papers": {"0": "Use What You Have: Video Retrieval Using Representations From Collaborative Experts", "1": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips", "2": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning", "3": "Use What You Have: Video Retrieval Using Representations From Collaborative Experts", "4": "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training", "5": "Use What You Have: Video Retrieval Using Representations From Collaborative Experts", "6": "Use What You Have: Video Retrieval Using Representations From Collaborative Experts"}},
{"task": "Video Understanding", "papers": {}},
{"task": "Video Generation", "papers": {"0": "Lower Dimensional Kernels for Video Discriminators", "1": "Lower Dimensional Kernels for Video Discriminators", "2": "Sliced Wasserstein Generative Models", "3": "Adversarial Video Generation on Complex Datasets", "4": "Adversarial Video Generation on Complex Datasets", "5": "Adversarial Video Generation on Complex Datasets"}},
{"task": "Video Classification", "papers": {"0": "Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network", "1": "A Multigrid Method for Efficiently Training Video Models", "2": "A Multigrid Method for Efficiently Training Video Models", "3": "A Multigrid Method for Efficiently Training Video Models", "4": "MotionSqueeze: Neural Motion Feature Learning for Video Understanding", "5": "MotionSqueeze: Neural Motion Feature Learning for Video Understanding"}},
{"paper": "Discriminative Feature Learning for Unsupervised Video Summarization", "abstract": "In this paper, we address the problem of unsupervised video summarization\nthat automatically extracts key-shots from an input video. Specifically, we\ntackle two critical issues based on our empirical observations: (i) Ineffective\nfeature learning due to flat distributions of output importance scores for each\nframe, and (ii) training difficulty when dealing with long-length video inputs.To alleviate the first problem, we propose a simple yet effective\nregularization loss term called variance loss. The proposed variance loss\nallows a network to predict output scores for each frame with high discrepancy\nwhich enables effective feature learning and significantly improves model\nperformance. For the second problem, we design a novel two-stream network named\nChunk and Stride Network (CSNet) that utilizes local (chunk) and global\n(stride) temporal view on the video features. Our CSNet gives better\nsummarization results for long-length videos compared to the existing methods. In addition, we introduce an attention mechanism to handle the dynamic\ninformation in videos. We demonstrate the effectiveness of the proposed methods\nby conducting extensive ablation studies and show that our final model achieves\nnew state-of-the-art results on two benchmark datasets."},
{"paper": "Few-shot Video-to-Video Synthesis", "abstract": "Video-to-video synthesis (vid2vid) aims at converting an input semantic video, such as videos of human poses or segmentation masks, to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly, existing approaches share two major limitations.First, they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second, a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations, we propose a few-shot vid2vid framework, which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos, talking-head videos, and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches."},
{"task": "Action Classification", "papers": {"0": "Omni-sourced Webly-supervised Learning for Video Recognition", "1": "AssembleNet++: Assembling Modality Representations via Attention Connections", "2": "AViD Dataset: Anonymized Videos from Diverse Countries", "3": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "4": "Two-Stream Convolutional Networks for Action Recognition in Videos", "5": "3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization", "6": "W-TALC: Weakly-supervised Temporal Activity Localization and Classification", "7": "MARS: Motion-Augmented RGB Stream for Action Recognition", "8": "VideoBERT: A Joint Model for Video and Language Representation Learning", "9": "Learning Spatio-Temporal Representation with Local and Global Diffusion"}},
{"paper": "DVDnet: A Fast Network for Deep Video Denoising", "abstract": "In this paper, we propose a state-of-the-art video denoising algorithm based on a convolutional neural network architecture. Previous neural network based approaches to video denoising have been unsuccessful as their performance cannot compete with the performance of patch-based methods.However, our approach outperforms other patch-based competitors with significantly lower computing times. In contrast to other existing neural network denoisers, our algorithm exhibits several desirable properties such as a small memory footprint, and the ability to handle a wide range of noise levels with a single network model. The combination between its denoising performance and lower computational load makes this algorithm attractive for practical denoising applications. We compare our method with different state-of-art algorithms, both visually and with respect to objective quality metrics. The experiments show that our algorithm compares favorably to other state-of-art methods. Video examples, code and models are publicly available at \\url{https://github.com/m-tassano/dvdnet}."},
{"task": "Video Prediction", "papers": {"0": "Scalable Gradients for Stochastic Differential Equations", "1": "Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics", "2": "ODE$^2$VAE: Deep generative second order ODEs with Bayesian neural networks", "3": "Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders", "4": "Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders", "5": "Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders"}},
{"task": "Video Object Segmentation", "papers": {"0": "Making a Case for 3D Convolutions for Object Segmentation in Videos", "1": "Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation", "2": "Video Object Segmentation with Language Referring Expressions"}},
{"paper": "Summarizing Videos with Attention", "abstract": "In this work we propose a novel method for supervised, keyshots based video\nsummarization by applying a conceptually simple and computationally efficient\nsoft, self-attention mechanism. Current state of the art methods leverage\nbi-directional recurrent networks such as BiLSTM combined with attention.These\nnetworks are complex to implement and computationally demanding compared to\nfully connected networks. To that end we propose a simple, self-attention based\nnetwork for video summarization which performs the entire sequence to sequence\ntransformation in a single feed forward pass and single backward pass during\ntraining. Our method sets a new state of the art results on two benchmarks\nTvSum and SumMe, commonly used in this domain."},
{"paper": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation", "abstract": "In this paper, we propose a state-of-the-art video denoising algorithm based on a convolutional neural network architecture. Until recently, video denoising with neural networks had been a largely under explored domain, and existing methods could not compete with the performance of the best patch-based methods.The approach we introduce in this paper, called FastDVDnet, shows similar or better performance than other state-of-the-art competitors with significantly lower computing times. In contrast to other existing neural network denoisers, our algorithm exhibits several desirable properties such as fast runtimes, and the ability to handle a wide range of noise levels with a single network model. The characteristics of its architecture make it possible to avoid using a costly motion compensation stage while achieving excellent performance. The combination between its denoising performance and lower computational load makes this algorithm attractive for practical denoising applications. We compare our method with different state-of-art algorithms, both visually and with respect to objective quality metrics."},
{"task": "Object Tracking", "papers": {}},
{"task": "3D Object Super-Resolution", "papers": {}},
{"paper": "Softmax Splatting for Video Frame Interpolation", "abstract": "Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way.We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation."},
{"paper": "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training", "abstract": "We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer.In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities."},
{"paper": "Sliced Wasserstein Generative Models", "abstract": "In generative modeling, the Wasserstein distance (WD) has emerged as a useful\nmetric to measure the discrepancy between generated and real data\ndistributions. Unfortunately, it is challenging to approximate the WD of\nhigh-dimensional distributions.In contrast, the sliced Wasserstein distance\n(SWD) factorizes high-dimensional distributions into their multiple\none-dimensional marginal distributions and is thus easier to approximate. In\nthis paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by\nconventional SWD approximation methods, we propose to approximate SWDs with a\nsmall number of parameterized orthogonal projections in an end-to-end deep\nlearning fashion. As concrete applications of our SWD approximations, we design\ntwo types of differentiable SWD blocks to equip modern generative\nframeworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In\nthe experiments, we not only show the superiority of the proposed generative\nmodels on standard image synthesis benchmarks, but also demonstrate the\nstate-of-the-art performance on challenging high resolution image and video\ngeneration in an unsupervised manner."},
{"task": "Video Super-Resolution", "papers": {"0": "Revisiting Temporal Modeling for Video Super-resolution", "1": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network", "2": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network", "3": "iSeeBetter: Spatio-Temporal Video Super Resolution using Recurrent-Generative Back-Projection Networks", "4": "Revisiting Temporal Modeling for Video Super-resolution", "5": "Revisiting Temporal Modeling for Video Super-resolution"}},
{"paper": "Adversarial Video Generation on Complex Datasets", "abstract": "Generative models of natural images have progressed towards high fidelity samples by the strong leveraging of scale. We attempt to carry this success to the field of video modeling by showing that large Generative Adversarial Networks trained on the complex Kinetics-600 dataset are able to produce video samples of substantially higher complexity and fidelity than previous work.Our proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. We evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fr\\'echet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600."},
{"task": "Image Super-Resolution", "papers": {"0": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "1": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "2": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "3": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "4": "Second-Order Attention Network for Single Image Super-Resolution", "5": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "6": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "7": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "8": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "9": "Densely Residual Laplacian Super-Resolution", "10": "Single Image Super-Resolution via a Holistic Attention Network", "11": "Densely Residual Laplacian Super-Resolution", "12": "Densely Residual Laplacian Super-Resolution", "13": "HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment", "14": "HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment", "15": "Densely Residual Laplacian Super-Resolution", "16": "Deep Back-Projection Networks for Single Image Super-resolution", "17": "Deep Back-Projection Networks for Single Image Super-resolution", "18": "Densely Residual Laplacian Super-Resolution", "19": "Exemplar Guided Face Image Super-Resolution without Facial Landmarks", "20": "Learning Warped Guidance for Blind Face Restoration", "21": "HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment", "22": "Densely Residual Laplacian Super-Resolution", "23": "Deep Back-Projection Networks for Single Image Super-resolution", "24": "Densely Residual Laplacian Super-Resolution", "25": "RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution", "26": "Deep Back-Projection Networks for Single Image Super-resolution", "27": "Edge-Informed Single Image Super-Resolution", "28": "Image Super-Resolution by Neural Texture Transfer", "29": "Image Super-Resolution by Neural Texture Transfer", "30": "Learning Parallax Attention for Stereo Image Super-Resolution", "31": "Learning Parallax Attention for Stereo Image Super-Resolution", "32": "Learning Parallax Attention for Stereo Image Super-Resolution", "33": "Learning Parallax Attention for Stereo Image Super-Resolution", "34": "Learning Parallax Attention for Stereo Image Super-Resolution", "35": "Learning Parallax Attention for Stereo Image Super-Resolution", "36": "Deep Back-Projection Networks for Single Image Super-resolution", "37": "Deep Back-Projection Networks for Single Image Super-resolution", "38": "Underwater Image Super-Resolution using Deep Residual Multipliers", "39": "Image Super-Resolution via Attention based Back Projection Networks", "40": "Image Super-Resolution via Attention based Back Projection Networks", "41": "Image Super-Resolution via Attention based Back Projection Networks", "42": "Image Super-Resolution via Attention based Back Projection Networks", "43": "Image Super-Resolution via Attention based Back Projection Networks", "44": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "45": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "46": "Cascade Convolutional Neural Network for Image Super-Resolution"}},
{"paper": "A Multigrid Method for Efficiently Training Video Models", "abstract": "Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research.Following standard practice for training image models, video model training assumes a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but they are inaccurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, non-local, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5x faster (wall-clock time, same hardware) while also improving accuracy (+0.8% absolute) on Kinetics-400 compared to the baseline training method. Code is available online."},
{"task": "Multi-Frame Super-Resolution", "papers": {"0": "Multi-image Super Resolution of Remotely Sensed Images using Residual Feature Attention Deep Neural Networks"}},
{"paper": "MotionSqueeze: Neural Motion Feature Learning for Video Understanding", "abstract": "Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding.In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&V2 datasets."},
{"paper": "Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network", "abstract": "High accuracy video label prediction (classification) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the efficiency for creating models.Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classification method based on a deep convolutional graph neural network(DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation re ecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks."},
{"paper": "Omni-sourced Webly-supervised Learning for Video Recognition", "abstract": "We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning.First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training."},
{"paper": "ODE$^2$VAE: Deep generative second order ODEs with Bayesian neural networks", "abstract": "We present Ordinary Differential Equation Variational Auto-Encoder (ODE$^2$VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE$^2$VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics.Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks."},
{"paper": "Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders", "abstract": "We introduce MGP-VAE (Multi-disentangled-features Gaussian Processes Variational AutoEncoder), a variational autoencoder which uses Gaussian processes (GP) to model the latent space for the unsupervised learning of disentangled representations in video sequences. We improve upon previous work by establishing a framework by which multiple features, static or dynamic, can be disentangled.Specifically we use fractional Brownian motions (fBM) and Brownian bridges (BB) to enforce an inter-frame correlation structure in each independent channel, and show that varying this structure enables one to capture different factors of variation in the data. We demonstrate the quality of our representations with experiments on three publicly available datasets, and also quantify the improvement using a video prediction task. Moreover, we introduce a novel geodesic loss function which takes into account the curvature of the data manifold to improve learning. Our experiments show that the combination of the improved representations with the novel loss function enable MGP-VAE to outperform the baselines in video prediction."},
{"paper": "Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation", "abstract": "In this paper, we introduce a novel network, called discriminative feature network (DFNet), to address the unsupervised video object segmentation task. To capture the inherent correlation among video frames, we learn discriminative features (D-features) from the input images that reveal feature distribution from a global perspective.The D-features are then used to establish correspondence with all features of test image under conditional random field (CRF) formulation, which is leveraged to enforce consistency between pixels. The experiments verify that DFNet outperforms state-of-the-art methods by a large margin with a mean IoU score of 83.4% and ranks first on the DAVIS-2016 leaderboard while using much fewer parameters and achieving much more efficient performance in the inference phase. We further evaluate DFNet on the FBMS dataset and the video saliency dataset ViSal, reaching a new state-of-the-art. To further demonstrate the generalizability of our framework, DFNet is also applied to the image object co-segmentation task. We perform experiments on a challenging dataset PASCAL-VOC and observe the superiority of DFNet. The thorough experiments verify that DFNet is able to capture and mine the underlying relations of images and discover the common foreground objects."},
{"paper": "Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics", "abstract": "Natural spatiotemporal processes can be highly non-stationary in many ways,\ne.g. the low-level non-stationarity such as spatial correlations or temporal\ndependencies of local pixel values; and the high-level variations such as the\naccumulation, deformation or dissipation of radar echoes in precipitation\nforecasting. From Cramer's Decomposition, any non-stationary process can be\ndecomposed into deterministic, time-variant polynomials, plus a zero-mean\nstochastic term.By applying differencing operations appropriately, we may turn\ntime-variant polynomials into a constant, making the deterministic component\npredictable. However, most previous recurrent neural networks for\nspatiotemporal prediction do not use the differential signals effectively, and\ntheir relatively simple state transition functions prevent them from learning\ntoo complicated variations in spacetime. We propose the Memory In Memory (MIM)\nnetworks and corresponding recurrent blocks for this purpose. The MIM blocks\nexploit the differential signals between adjacent recurrent states to model the\nnon-stationary and approximately stationary properties in spatiotemporal\ndynamics with two cascaded, self-renewed memory modules. By stacking multiple\nMIM blocks, we could potentially handle higher-order non-stationarity. The MIM\nnetworks achieve the state-of-the-art results on four spatiotemporal prediction\ntasks across both synthetic and real-world datasets. We believe that the\ngeneral idea of this work can be potentially applied to other time-series\nforecasting tasks."},
{"paper": "Scalable Gradients for Stochastic Differential Equations", "abstract": "The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers.Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset."},
{"paper": "Learning Spatio-Temporal Representation with Local and Global Diffusion", "abstract": "Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for visual recognition problems. Nevertheless, the convolutional filters in these networks are local operations while ignoring the large-range dependency.Such drawback becomes even worse particularly for video recognition, since video is an information-intensive media with complex temporal variations. In this paper, we present a novel framework to boost the spatio-temporal representation learning by Local and Global Diffusion (LGD). Specifically, we construct a novel neural network architecture that learns the local and global representations in parallel. The architecture is composed of LGD blocks, where each block updates local and global features by modeling the diffusions between these two representations. Diffusions effectively interact two aspects of information, i.e., localized and holistic, for more powerful way of representation learning. Furthermore, a kernelized classifier is introduced to combine the representations from two aspects for video recognition. Our LGD networks achieve clear improvements on the large-scale Kinetics-400 and Kinetics-600 video classification datasets against the best competitors by 3.5% and 0.7%. We further examine the generalization of both the global and local representations produced by our pre-trained LGD networks on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Superior performances over several state-of-the-art techniques on these benchmarks are reported. Code is available at: https://github.com/ZhaofanQiu/local-and-global-diffusion-networks."},
{"paper": "Making a Case for 3D Convolutions for Object Segmentation in Videos", "abstract": "The task of object segmentation in videos is usually accomplished by processing appearance and motion information separately using standard 2D convolutional networks, followed by a learned fusion of the two sources of information. On the other hand, 3D convolutional networks have been successfully applied for video classification tasks, but have not been leveraged as effectively to problems involving dense per-pixel interpretation of videos compared to their 2D convolutional counterparts and lag behind the aforementioned networks in terms of performance.In this work, we show that 3D CNNs can be effectively applied to dense video prediction tasks such as salient object segmentation. We propose a simple yet effective encoder-decoder network architecture consisting entirely of 3D convolutions that can be trained end-to-end using a standard cross-entropy loss. To this end, we leverage an efficient 3D encoder, and propose a 3D decoder architecture, that comprises novel 3D Global Convolution layers and 3D Refinement modules. Our approach outperforms existing state-of-the-arts by a large margin on the DAVIS'16 Unsupervised, FBMS and ViSal dataset benchmarks in addition to being faster, thus showing that our architecture can efficiently learn expressive spatio-temporal features and produce high quality video segmentation masks. Our code and models will be made publicly available."},
{"paper": "Video Object Segmentation with Language Referring Expressions", "abstract": "Most state-of-the-art semi-supervised video object segmentation methods rely\non a pixel-accurate mask of a target object provided for the first frame of a\nvideo. However, obtaining a detailed segmentation mask is expensive and\ntime-consuming.In this work we explore an alternative way of identifying a\ntarget object, namely by employing language referring expressions. Besides\nbeing a more practical and natural way of pointing out a target object, using\nlanguage specifications can help to avoid drift as well as make the system more\nrobust to complex dynamics and appearance variations. Leveraging recent\nadvances of language grounding models designed for images, we propose an\napproach to extend them to video data, ensuring temporally coherent\npredictions. To evaluate our method we augment the popular video object\nsegmentation benchmarks, DAVIS'16 and DAVIS'17 with language descriptions of\ntarget objects. We show that our language-supervised approach performs on par\nwith the methods which have access to a pixel-level mask of the target object\non DAVIS'16 and is competitive to methods using scribbles on the challenging\nDAVIS'17 dataset."},
{"paper": "MARS: Motion-Augmented RGB Stream for Action Recognition", "abstract": "Most state-of-the-art methods for action recognition consist of a two-stream architecture with 3D convolutions: an appearance stream for RGB frames and a motion stream for optical flow frames. Although combining flow with RGB improves the performance, the cost of computing accurate optical flow is high, and increases action recognition latency.This limits the usage of two-stream approaches in real-world applications requiring low latency. In this paper, we introduce two learning approaches to train a standard 3D CNN, operating on RGB frames, that mimics the motion stream, and as a result avoids flow computation at test time. First, by minimizing a feature-based loss compared to the Flow stream, we show that the network reproduces the motion stream with high fidelity. Second, to leverage both appearance and motion information effectively, we train with a linear combination of the feature-based loss and the standard cross-entropy loss for action recognition. We denote the stream trained using this combined loss as Motion-Augmented RGB Stream (MARS). As a single stream, MARS performs better than RGB or Flow alone, for instance with 72.7% accuracy on Kinetics compared to 72.0% and 65.6% with RGB and Flow streams respectively."},
{"paper": "Underwater Image Super-Resolution using Deep Residual Multipliers", "abstract": "We present a deep residual network-based generative model for single image super-resolution (SISR) of underwater imagery for use by autonomous underwater robots. We also provide an adversarial training pipeline for learning SISR from paired data.In order to supervise the training, we formulate an objective function that evaluates the \\textit{perceptual quality} of an image based on its global content, color, and local style information. Additionally, we present USR-248, a large-scale dataset of three sets of underwater images of 'high' (640x480) and 'low' (80x60, 160x120, and 320x240) spatial resolution. USR-248 contains paired instances for supervised training of 2x, 4x, or 8x SISR models. Furthermore, we validate the effectiveness of our proposed model through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. We also analyze its practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions."},
{"paper": "W-TALC: Weakly-supervised Temporal Activity Localization and Classification", "abstract": "Most activity localization methods in the literature suffer from the burden\nof frame-wise annotation requirement. Learning from weak labels may be a\npotential solution towards reducing such manual labeling effort.Recent years\nhave witnessed a substantial influx of tagged videos on the Internet, which can\nserve as a rich source of weakly-supervised training data. Specifically, the\ncorrelations between videos with similar tags can be utilized to temporally\nlocalize the activities. Towards this goal, we present W-TALC, a\nWeakly-supervised Temporal Activity Localization and Classification framework\nusing only video-level labels. The proposed network can be divided into two\nsub-networks, namely the Two-Stream based feature extractor network and a\nweakly-supervised module, which we learn by optimizing two complimentary loss\nfunctions. Qualitative and quantitative results on two challenging datasets -\nThumos14 and ActivityNet1.2, demonstrate that the proposed method is able to\ndetect activities at a fine granularity and achieve better performance than\ncurrent state-of-the-art methods."},
{"paper": "VideoBERT: A Joint Model for Video and Language Representation Learning", "abstract": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision.In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features."},
{"paper": "Image Super-Resolution by Neural Texture Transfer", "abstract": "Due to the significant information loss in low-resolution (LR) images, it has\nbecome extremely challenging to further advance the state-of-the-art of single\nimage super-resolution (SISR). Reference-based super-resolution (RefSR), on the\nother hand, has proven to be promising in recovering high-resolution (HR)\ndetails when a reference (Ref) image with similar content as that of the LR\ninput is given.However, the quality of RefSR can degrade severely when Ref is\nless similar. This paper aims to unleash the potential of RefSR by leveraging\nmore texture details from Ref images with stronger robustness even when\nirrelevant Ref images are provided. Inspired by the recent work on image\nstylization, we formulate the RefSR problem as neural texture transfer. We\ndesign an end-to-end deep model which enriches HR details by adaptively\ntransferring the texture from Ref images according to their textural\nsimilarity. Instead of matching content in the raw pixel space as done by\nprevious methods, our key contribution is a multi-level matching conducted in\nthe neural space. This matching scheme facilitates multi-scale neural transfer\nthat allows the model to benefit more from those semantically related Ref\npatches, and gracefully degrade to SISR performance on the least relevant Ref\ninputs. We build a benchmark dataset for the general research of RefSR, which\ncontains Ref images paired with LR inputs with varying levels of similarity. Both quantitative and qualitative evaluations demonstrate the superiority of\nour method over state-of-the-art."},
{"paper": "Learning Parallax Attention for Stereo Image Super-Resolution", "abstract": "Stereo image pairs can be used to improve the performance of super-resolution\n(SR) since additional information is provided from a second viewpoint. However,\nit is challenging to incorporate this information for SR since disparities\nbetween stereo images vary significantly.In this paper, we propose a\nparallax-attention stereo superresolution network (PASSRnet) to integrate the\ninformation from a stereo image pair for SR. Specifically, we introduce a\nparallax-attention mechanism with a global receptive field along the epipolar\nline to handle different stereo images with large disparity variations. We also\npropose a new and the largest dataset for stereo image SR (namely, Flickr1024). Extensive experiments demonstrate that the parallax-attention mechanism can\ncapture correspondence between stereo images to improve SR performance with a\nsmall computational and memory cost. Comparative results show that our PASSRnet\nachieves the state-of-the-art performance on the Middlebury, KITTI 2012 and\nKITTI 2015 datasets."},
{"paper": "Cascade Convolutional Neural Network for Image Super-Resolution", "abstract": "With the development of the super-resolution convolutional neural network (SRCNN), deep learning technique has been widely applied in the field of image super-resolution. Previous works mainly focus on optimizing the structure of SRCNN, which have been achieved well performance in speed and restoration quality for image super-resolution.However, most of these approaches only consider a specific scale image during the training process, while ignoring the relationship between different scales of images. Motivated by this concern, in this paper, we propose a cascaded convolution neural network for image super-resolution (CSRCNN), which includes three cascaded Fast SRCNNs and each Fast SRCNN can process a specific scale image. Images of different scales can be trained simultaneously and the learned network can make full use of the information resided in different scales of images. Extensive experiments show that our network can achieve well performance for image SR."},
{"paper": "Image Super-Resolution via Attention based Back Projection Networks", "abstract": "Deep learning based image Super-Resolution (SR) has shown rapid development due to its ability of big data digestion. Generally, deeper and wider networks can extract richer feature maps and generate SR images with remarkable quality.However, the more complex network we have, the more time consumption is required for practical applications. It is important to have a simplified network for efficient image SR. In this paper, we propose an Attention based Back Projection Network (ABPN) for image super-resolution. Similar to some recent works, we believe that the back projection mechanism can be further developed for SR. Enhanced back projection blocks are suggested to iteratively update low- and high-resolution feature residues. Inspired by recent studies on attention models, we propose a Spatial Attention Block (SAB) to learn the cross-correlation across features at different layers. Based on the assumption that a good SR image should be close to the original LR image after down-sampling. We propose a Refined Back Projection Block (RBPB) for final reconstruction. Extensive experiments on some public and AIM2019 Image Super-Resolution Challenge datasets show that the proposed ABPN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements."},
{"paper": "Multi-image Super Resolution of Remotely Sensed Images using Residual Feature Attention Deep Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) have been consistently proved state-of-the-art results in image Super-Resolution (SR), representing an exceptional opportunity for the remote sensing field to extract further information and knowledge from captured data. However, most of the works published in the literature have been focusing on the Single-Image Super-Resolution problem so far.At present, satellite based remote sensing platforms offer huge data availability with high temporal resolution and low spatial resolution. In this context, the presented research proposes a novel residual attention model (RAMS) that efficiently tackles the multi-image super-resolution task, simultaneously exploiting spatial and temporal correlations to combine multiple images. We introduce the mechanism of visual feature attention with 3D convolutions in order to obtain an aware data fusion and information extraction of the multiple low-resolution images, transcending limitations of the local region of convolutional operations. Moreover, having multiple inputs with the same scene, our representation learning network makes extensive use of nestled residual connections to let flow redundant low-frequency signals and focus the computation on more important high-frequency components. Extensive experimentation and evaluations against other available solutions, either for single or multi-image super-resolution, have demonstrated that the proposed deep learning-based solution can be considered state-of-the-art for Multi-Image Super-Resolution for remote sensing applications."},
{"paper": "RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution", "abstract": "Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma.However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/Projects/RankSRGAN"},
{"paper": "Learning Warped Guidance for Blind Face Restoration", "abstract": "This paper studies the problem of blind face restoration from an\nunconstrained blurry, noisy, low-resolution, or compressed image (i.e.,\ndegraded observation). For better recovery of fine facial details, we modify\nthe problem setting by taking both the degraded observation and a high-quality\nguided image of the same identity as input to our guided face restoration\nnetwork (GFRNet).However, the degraded observation and guided image generally\nare different in pose, illumination and expression, thereby making plain CNNs\n(e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle\nthis issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a\nreconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow\nfield for warping the guided image to correct pose and expression (i.e., warped\nguidance), while the RecNet takes the degraded observation and warped guidance\nas input to produce the restoration result. Due to that the ground-truth flow\nfield is unavailable, landmark loss together with total variation\nregularization are incorporated to guide the learning of WarpNet. Furthermore,\nto make the model applicable to blind restoration, our GFRNet is trained on the\nsynthetic data with versatile settings on blur kernel, noise level,\ndownsampling scale factor, and JPEG quality factor. Experiments show that our\nGFRNet not only performs favorably against the state-of-the-art image and face\nrestoration methods, but also generates visually photo-realistic results on\nreal degraded facial images."},
{"paper": "Edge-Informed Single Image Super-Resolution", "abstract": "The recent increase in the extensive use of digital imaging technologies has brought with it a simultaneous demand for higher-resolution images. We develop a novel edge-informed approach to single image super-resolution (SISR).The SISR problem is reformulated as an image inpainting task. We use a two-stage inpainting model as a baseline for super-resolution and show its effectiveness for different scale factors (x2, x4, x8) compared to basic interpolation schemes. This model is trained using a joint optimization of image contents (texture and color) and structures (edges). Quantitative and qualitative comparisons are included and the proposed model is compared with current state-of-the-art techniques. We show that our method of decoupling structure and texture reconstruction improves the quality of the final reconstructed high-resolution image. Code and models available at: https://github.com/knazeri/edge-informed-sisr"},
{"paper": "Exemplar Guided Face Image Super-Resolution without Facial Landmarks", "abstract": "Nowadays, due to the ubiquitous visual media there are vast amounts of already available high-resolution (HR) face images. Therefore, for super-resolving a given very low-resolution (LR) face image of a person it is very likely to find another HR face image of the same person which can be used to guide the process.In this paper, we propose a convolutional neural network (CNN)-based solution, namely GWAInet, which applies super-resolution (SR) by a factor 8x on face images guided by another unconstrained HR face image of the same person with possible differences in age, expression, pose or size. GWAInet is trained in an adversarial generative manner to produce the desired high quality perceptual image results. The utilization of the HR guiding image is realized via the use of a warper subnetwork that aligns its contents to the input image and the use of a feature fusion chain for the extracted features from the warped guiding image and the input image. In training, the identity loss further helps in preserving the identity related features by minimizing the distance between the embedding vectors of SR and HR ground truth images. Contrary to the current state-of-the-art in face super-resolution, our method does not require facial landmark points for its training, which helps its robustness and allows it to produce fine details also for the surrounding face region in a uniform manner. Our method GWAInet produces photo-realistic images in upscaling factor 8x and outperforms state-of-the-art in quantitative terms and perceptual quality."},
{"paper": "HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment", "abstract": "Existing face restoration researches typically relies on either the degradation prior or explicit guidance labels for training, which often results in limited generalization ability over real-world images with heterogeneous degradations and rich background contents. In this paper, we investigate the more challenging and practical \"dual-blind\" version of the problem by lifting the requirements on both types of prior, termed as \"Face Renovation\"(FR).Specifically, we formulated FR as a semantic-guided generation problem and tackle it with a collaborative suppression and replenishment (CSR) approach. This leads to HiFaceGAN, a multi-stage framework containing several nested CSR units that progressively replenish facial details based on the hierarchical semantic guidance extracted from the front-end content-adaptive suppression modules. Extensive experiments on both synthetic and real face images have verified the superior performance of HiFaceGAN over a wide range of challenging restoration subtasks, demonstrating its versatility, robustness and generalization ability towards real-world face processing applications."},
{"paper": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "abstract": "Deep convolutional neural network based image super-resolution (SR) models have shown superior performance in recovering the underlying high resolution (HR) images from low resolution (LR) images obtained from the predefined downscaling methods. In this paper we propose a learned image downscaling method based on content adaptive resampler (CAR) with consideration on the upscaling process.The proposed resampler network generates content adaptive image resampling kernels that are applied to the original HR input to generate pixels on the downscaled image. Moreover, a differentiable upscaling (SR) module is employed to upscale the LR result into its underlying HR counterpart. By back-propagating the reconstruction error down to the original HR input across the entire framework to adjust model parameters, the proposed framework achieves a new state-of-the-art SR performance through upscaling guided image resamplers which adaptively preserve detailed information that is essential to the upscaling. Experimental results indicate that the quality of the generated LR image is comparable to that of the traditional interpolation based method, but the significant SR performance gain is achieved by deep SR models trained jointly with the CAR model. The code is publicly available on: URL https://github.com/sunwj/CAR."},
{"paper": "Deep Back-Projection Networks for Single Image Super-resolution", "abstract": "Previous feed-forward architectures of recently proposed deep super-resolution networks learn the features of low-resolution inputs and the non-linear mapping from those to a high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images.We propose Deep Back-Projection Networks (DBPN), the winner of two image super-resolution challenges (NTIRE2018 and PIRM2018), that exploit iterative up- and down-sampling layers. These layers are formed as a unit providing an error feedback mechanism for projection errors. We construct mutually-connected up- and down-sampling units each of which represents different types of low- and high-resolution components. We also show that extending this idea to demonstrate a new insight towards more efficient network design substantially, such as parameter sharing on the projection module and transition layer on projection step. The experimental results yield superior results and in particular establishing new state-of-the-art results across multiple data sets, especially for large scaling factors such as 8x."},
{"paper": "Second-Order Attention Network for Single Image Super-Resolution", "abstract": "Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs.To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel train- able second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality."},
{"paper": "iSeeBetter: Spatio-Temporal Video Super Resolution using Recurrent-Generative Back-Projection Networks", "abstract": "Recently, learning-based models have enhanced the performance of single-image super-resolution (SISR). However, applying SISR successively to each video frame leads to a lack of temporal coherency.Convolutional neural networks (CNNs) outperform traditional approaches in terms of image quality metrics such as peak signal to noise ratio (PSNR) and structural similarity (SSIM). However, generative adversarial networks (GANs) offer a competitive advantage by being able to mitigate the issue of a lack of finer texture details, usually seen with CNNs when super-resolving at large upscaling factors. We present iSeeBetter, a novel GAN-based spatio-temporal approach to video super-resolution (VSR) that renders temporally consistent super-resolution videos. iSeeBetter extracts spatial and temporal information from the current and neighboring frames using the concept of recurrent back-projection networks as its generator. Furthermore, to improve the \"naturality\" of the super-resolved image while eliminating artifacts seen with traditional algorithms, we utilize the discriminator from super-resolution generative adversarial network (SRGAN). Although mean squared error (MSE) as a primary loss-minimization objective improves PSNR/SSIM, these metrics may not capture fine details in the image resulting in misrepresentation of perceptual quality. To address this, we use a four-fold (MSE, perceptual, adversarial, and total-variation (TV)) loss function. Our results demonstrate that iSeeBetter offers superior VSR fidelity and surpasses state-of-the-art performance."},
{"paper": "Single Image Super-Resolution via a Holistic Attention Network", "abstract": "Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer.However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features. Extensive experiments demonstrate that the proposed HAN performs favorably against the state-of-the-art single image super-resolution approaches."},
{"paper": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network", "abstract": "Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction.This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods."},
{"paper": "Revisiting Temporal Modeling for Video Super-resolution", "abstract": "Video super-resolution plays an important role in surveillance video analysis and ultra-high-definition video display, which has drawn much attention in both the research and industrial communities. Although many deep learning-based VSR methods have been proposed, it is hard to directly compare these methods since the different loss functions and training datasets have a significant impact on the super-resolution results.In this work, we carefully study and compare three temporal modeling methods (2D CNN with early fusion, 3D CNN with slow fusion and Recurrent Neural Network) for video super-resolution. We also propose a novel Recurrent Residual Network (RRN) for efficient video super-resolution, where residual learning is utilized to stabilize the training of RNN and meanwhile to boost the super-resolution performance. Extensive experiments show that the proposed RRN is highly computational efficiency and produces temporal consistent VSR results with finer details than other temporal modeling methods. Besides, the proposed method achieves state-of-the-art results on several widely used benchmarks."},
{"paper": "Densely Residual Laplacian Super-Resolution", "abstract": "Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times.Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately."},
{"paper": "AssembleNet++: Assembling Modality Representations via Attention Connections", "abstract": "We create a family of powerful video models which are able to: (i) learn interactions between semantic object information and raw appearance and motion features, and (ii) deploy attention in order to better learn the importance of features at each convolutional block of the network. A new network component named peer-attention is introduced, which dynamically learns the attention weights using another block or input modality.Even without pre-training, our models outperform the previous work on standard public activity recognition datasets with continuous videos, establishing new state-of-the-art. We also confirm that our findings of having neural connections from the object modality and the use of peer-attention is generally applicable for different existing architectures, improving their performances. We name our model explicitly as AssembleNet++. The code will be available at: https://sites.google.com/corp/view/assemblenet/"},
{"paper": "Two-Stream Convolutional Networks for Action Recognition in Videos", "abstract": "We investigate architectures of discriminatively trained deep Convolutional\nNetworks (ConvNets) for action recognition in video. The challenge is to\ncapture the complementary information on appearance from still frames and\nmotion between frames.We also aim to generalise the best performing\nhand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet\narchitecture which incorporates spatial and temporal networks. Second, we\ndemonstrate that a ConvNet trained on multi-frame dense optical flow is able to\nachieve very good performance in spite of limited training data. Finally, we\nshow that multi-task learning, applied to two different action classification\ndatasets, can be used to increase the amount of training data and improve the\nperformance on both. Our architecture is trained and evaluated on the standard video actions\nbenchmarks of UCF-101 and HMDB-51, where it is competitive with the state of\nthe art. It also exceeds by a large margin previous attempts to use deep nets\nfor video classification."},
{"paper": "3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization", "abstract": "Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models.In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art. Source code is available at https://github.com/naraysa/3c-net."},
{"paper": "AViD Dataset: Anonymized Videos from Diverse Countries", "abstract": "We introduce a new public video dataset for action recognition: Anonymized Videos from Diverse countries (AViD). Unlike existing public video datasets, AViD is a collection of action videos from many different countries.The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries. Further, all the face identities in the AViD videos are properly anonymized to protect their privacy. It also is a static dataset where each video is licensed with the creative commons license. We confirm that most of the existing video datasets are statistically biased to only capture action videos from a limited number of countries. We experimentally illustrate that models trained with such biased datasets do not transfer perfectly to action videos from the other countries, and show that AViD addresses such problem. We also confirm that the new AViD dataset could serve as a good dataset for pretraining the models, performing comparably or better than prior datasets."},
{"paper": "Lower Dimensional Kernels for Video Discriminators", "abstract": "This work presents an analysis of the discriminators used in Generative Adversarial Networks (GANs) for Video. We show that unconstrained video discriminator architectures induce a loss surface with high curvature which make optimisation difficult.We also show that this curvature becomes more extreme as the maximal kernel dimension of video discriminators increases. With these observations in hand, we propose a family of efficient Lower-Dimensional Video Discriminators for GANs (LDVD GANs). The proposed family of discriminators improve the performance of video GAN models they are applied to and demonstrate good performance on complex and diverse datasets such as UCF-101. In particular, we show that they can double the performance of Temporal-GANs and provide for state-of-the-art performance on a single GPU."},
{"paper": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips", "abstract": "Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale.In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: www.di.ens.fr/willow/research/howto100m/."},
{"paper": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning", "abstract": "Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities.The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext"},
{"paper": "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution", "abstract": "In this paper, we explore the space-time video super-resolution task, which aims to generate a high-resolution (HR) slow-motion video from a low frame rate (LFR), low-resolution (LR) video. A simple solution is to split it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR).However, temporal interpolation and spatial super-resolution are intra-related in this task. Two-stage methods cannot fully take advantage of the natural property. In addition, state-of-the-art VFI or VSR networks require a large frame-synthesis or reconstruction module for predicting high-quality video frames, which makes the two-stage methods have large model sizes and thus be time-consuming. To overcome the problems, we propose a one-stage space-time video super-resolution framework, which directly synthesizes an HR slow-motion video from an LFR, LR video. Rather than synthesizing missing LR video frames as VFI networks do, we firstly temporally interpolate LR frame features in missing LR video frames capturing local temporal contexts by the proposed feature temporal interpolation network. Then, we propose a deformable ConvLSTM to align and aggregate temporal information simultaneously for better leveraging global temporal contexts. Finally, a deep reconstruction network is adopted to predict HR slow-motion video frames. Extensive experiments on benchmark datasets demonstrate that the proposed method not only achieves better quantitative and qualitative performance but also is more than three times faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR and DAIN+RBPN."},
{"task": "Super-Resolution", "papers": {"0": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks"}},
{"task": "Pedestrian Density Estimation", "papers": {}},
{"task": "Fast Vehicle Detection", "papers": {}},
{"paper": "Use What You Have: Video Retrieval Using Representations From Collaborative Experts", "abstract": "The rapid growth of video on the internet has made searching for video content using natural language queries a significant challenge. Human-generated queries for video datasets `in the wild' vary a lot in terms of degree of specificity, with some queries describing specific details such as the names of famous identities, content from speech, or text available on the screen.Our goal is to condense the multi-modal, extremely high dimensional information from videos into a single, compact video representation for the task of video retrieval using free-form text queries, where the degree of specificity is open-ended. For this we exploit existing knowledge in the form of pre-trained semantic embeddings which include 'general' features such as motion, appearance, and scene features from visual content. We also explore the use of more 'specific' cues from ASR and OCR which are intermittently available for videos and find that these signals remain challenging to use effectively for retrieval. We propose a collaborative experts model to aggregate information from these different pre-trained experts and assess our approach empirically on five retrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and data can be found at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/. This paper contains a correction to results reported in the previous version."},
{"task": "3D Car Instance Understanding", "papers": {}},
{"task": "Driver Attention Monitoring", "papers": {}},
{"task": "Loop Closure Detection", "papers": {}},
{"task": "Pedestrian Attribute Recognition", "papers": {"0": "Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization", "1": "Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization", "2": "Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization"}},
{"task": "Traffic Sign Recognition", "papers": {"0": "Deep Learning for Large-Scale Traffic-Sign Detection and Recognition", "1": "Deep neural network for traffic sign recognition systems: An analysis of spatial transformers and stochastic optimisation methods", "2": "A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection", "3": "A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection", "4": "Deep Learning for Large-Scale Traffic-Sign Detection and Recognition"}},
{"task": "Pedestrian Detection", "papers": {"0": "Pedestrian Detection: The Elephant In The Room", "1": "Pedestrian Detection: The Elephant In The Room"}},
{"task": "Autonomous Navigation", "papers": {}},
{"task": "Simultaneous Localization and Mapping", "papers": {}},
{"task": "Autonomous Vehicles", "papers": {}},
{"task": "Lane Detection", "papers": {"0": "Key Points Estimation and Point Instance Segmentation Approach for Lane Detection", "1": "Keep your Eyes on the Lane: Attention-guided Lane Detection", "2": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition", "3": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition", "4": "Learning Lightweight Lane Detection CNNs by Self Attention Distillation"}},
{"task": "Self-Driving Cars", "papers": {}},
{"task": "Autonomous Driving", "papers": {}},
{"paper": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks", "abstract": "Convolutional neural network (CNN) depth is of crucial importance for image\nsuper-resolution (SR). However, we observe that deeper networks for image SR\nare more difficult to train.The low-resolution inputs and features contain\nabundant low-frequency information, which is treated equally across channels,\nhence hindering the representational ability of CNNs. To solve these problems,\nwe propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very\ndeep network, which consists of several residual groups with long skip\nconnections. Each residual group contains some residual blocks with short skip\nconnections. Meanwhile, RIR allows abundant low-frequency information to be\nbypassed through multiple skip connections, making the main network focus on\nlearning high-frequency information. Furthermore, we propose a channel\nattention mechanism to adaptively rescale channel-wise features by considering\ninterdependencies among channels. Extensive experiments show that our RCAN\nachieves better accuracy and visual improvements against state-of-the-art\nmethods."},
{"paper": "Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization", "abstract": "Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute.However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to define local feature areas. We propose a flexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-specific localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K."},
{"task": "Facial Beauty Prediction", "papers": {"0": "Transferring Rich Deep Features for Facial Beauty Prediction", "1": "Transferring Rich Deep Features for Facial Beauty Prediction"}},
{"task": "Facial Attribute Classification", "papers": {}},
{"paper": "Pedestrian Detection: The Elephant In The Room", "abstract": "Pedestrian detection is used in many vision based applications ranging from video surveillance to autonomous driving. Despite achieving high performance, it is still largely unknown how well existing detectors generalize to unseen data.To this end, we conduct a comprehensive study in this paper, using a general principle of direct cross-dataset evaluation. Through this study, we find that existing state-of-the-art pedestrian detectors generalize poorly from one dataset to another. We demonstrate that there are two reasons for this trend. Firstly, they over-fit on popular datasets in a traditional single-dataset training and test pipeline. Secondly, the training source is generally not dense in pedestrians and diverse in scenarios. Accordingly, through experiments we find that a general purpose object detector works better in direct cross-dataset evaluation compared with state-of-the-art pedestrian detectors and we illustrate that diverse and dense datasets, collected by crawling the web, serve to be an efficient source of pre-training for pedestrian detection. Furthermore, we find that a progressive training pipeline works good for autonomous driving oriented detector. We improve upon previous state-of-the-art on reasonable/heavy subsets of CityPersons dataset by 1.3%/1.7% and on Caltech by 1.8%/14.9% in terms of log average miss rate (MR^2) points without any fine-tuning on the test set. Detector trained through proposed pipeline achieves top rank at the leaderborads of CityPersons [42] and ECP [4]. Code and models can be accessed at https://github.com/hasanirtiza/Pedestron."},
{"paper": "A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection", "abstract": "Traffic light and sign detectors on autonomous cars are integral for road\nscene perception. The literature is abundant with deep learning networks that\ndetect either lights or signs, not both, which makes them unsuitable for\nreal-life deployment due to the limited graphics processing unit (GPU) memory\nand power available on embedded systems.The root cause of this issue is that\nno public dataset contains both traffic light and sign labels, which leads to\ndifficulties in developing a joint detection framework. We present a deep\nhierarchical architecture in conjunction with a mini-batch proposal selection\nmechanism that allows a network to detect both traffic lights and signs from\ntraining on separate traffic light and sign datasets. Our method solves the\noverlapping issue where instances from one dataset are not labelled in the\nother dataset. We are the first to present a network that performs joint\ndetection on traffic lights and signs. We measure our network on the\nTsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small\nTraffic Lights benchmark for traffic light detection and show it outperforms\nthe existing Bosch Small Traffic light state-of-the-art method. We focus on\nautonomous car deployment and show our network is more suitable than others\nbecause of its low memory footprint and real-time image processing time. Qualitative results can be viewed at https://youtu.be/_YmogPzBXOw"},
{"task": "Robust Face Alignment", "papers": {}},
{"task": "Heterogeneous Face Recognition", "papers": {"0": "A-LINK: Recognizing Disguised Faces via Active Learning based Inter-Domain Knowledge", "1": "A2-LINK: Recognizing Disguised Faces via Active Learning and Adversarial Noise based Inter-Domain Knowledge", "2": "A-LINK: Recognizing Disguised Faces via Active Learning based Inter-Domain Knowledge"}},
{"paper": "Deep neural network for traffic sign recognition systems: An analysis of spatial transformers and stochastic optimisation methods", "abstract": "This paper presents a Deep Learning approach for traffic sign recognition systems. Several classification experiments are conducted over publicly available traffic sign datasets from Germany and Belgium using a Deep Neural Network which comprises Convolutional layers and Spatial Transformer Networks.Such trials are built to measure the impact of diverse factors with the end goal of designing a Convolutional Neural Network that can improve the state-of-the-art of traffic sign classification task. First, different adaptive and non-adaptive stochastic gradient descent optimisation algorithms such as SGD, SGD-Nesterov, RMSprop and Adam are evaluated. Subsequently, multiple combinations of Spatial Transformer Networks placed at distinct positions within the main neural network are analysed. The recognition rate of the proposed Convolutional Neural Network reports an accuracy of 99.71% in the German Traffic Sign Recognition Benchmark, outperforming previous state-of-the-art methods and also being more efficient in terms of memory requirements."},
{"task": "Unsupervised Facial Landmark Detection", "papers": {"0": "Unsupervised Learning of Object Landmarks through Conditional Image Generation", "1": "Unsupervised Learning of Landmarks by Descriptor Vector Exchange", "2": "Unsupervised Learning of Object Landmarks through Conditional Image Generation", "3": "Unsupervised Learning of Landmarks by Descriptor Vector Exchange"}},
{"task": "Talking Face Generation", "papers": {"0": "Towards Automatic Face-to-Face Translation"}},
{"paper": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition", "abstract": "In this paper, we propose a unified end-to-end trainable multi-task network\nthat jointly handles lane and road marking detection and recognition that is\nguided by a vanishing point under adverse weather conditions. We tackle rainy\nand low illumination conditions, which have not been extensively studied until\nnow due to clear challenges.For example, images taken under rainy days are\nsubject to low illumination, while wet roads cause light reflection and distort\nthe appearance of lane and road markings. At night, color distortion occurs\nunder limited illumination. As a result, no benchmark dataset exists and only a\nfew developed algorithms work under poor weather conditions. To address this\nshortcoming, we build up a lane and road marking benchmark which consists of\nabout 20,000 images with 17 lane and road marking classes under four different\nscenarios: no rain, rain, heavy rain, and night. We train and evaluate several\nversions of the proposed multi-task network and validate the importance of each\ntask. The resulting approach, VPGNet, can detect and classify lanes and road\nmarkings, and predict a vanishing point with a single forward pass. Experimental results show that our approach achieves high accuracy and\nrobustness under various conditions in real-time (20 fps). The benchmark and\nthe VPGNet model will be publicly available."},
{"task": "Age-Invariant Face Recognition", "papers": {"0": "Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition", "1": "Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition", "2": "Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition", "3": "Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition"}},
{"task": "Face Sketch Synthesis", "papers": {"0": "Semi-Supervised Learning for Face Sketch Synthesis in the Wild", "1": "Semi-Supervised Learning for Face Sketch Synthesis in the Wild", "2": "Semi-Supervised Learning for Face Sketch Synthesis in the Wild"}},
{"paper": "Keep your Eyes on the Lane: Attention-guided Lane Detection", "abstract": "Modern lane detection methods have achieved remarkable performances in complex real-world scenarios, but many have issues maintaining real-time efficiency, which is important for autonomous vehicles. In this work, we propose LaneATT: an anchor-based deep lane detection model, which, akin to other generic deep object detectors, uses the anchors for the feature pooling step.Since lanes follow a regular pattern and are highly correlated, we hypothesize that in some cases global information may be crucial to infer their positions, especially in conditions such as occlusion, missing lane markers, and others. Thus, we propose a novel anchor-based attention mechanism that aggregates global information. The model was evaluated extensively on two of the most widely used datasets in the literature. The results show that our method outperforms the current state-of-the-art methods showing both a higher efficacy and efficiency. Moreover, we perform an ablation study and discuss efficiency trade-off options that are useful in practice. To reproduce our findings, source code and pretrained models are available at https://github.com/lucastabelini/LaneATT"},
{"paper": "Deep Learning for Large-Scale Traffic-Sign Detection and Recognition", "abstract": "Automatic detection and recognition of traffic signs plays a crucial role in\nmanagement of the traffic-sign inventory. It provides accurate and timely way\nto manage traffic-sign inventory with a minimal human effort.In the computer\nvision community the recognition and detection of traffic signs is a\nwell-researched problem. A vast majority of existing approaches perform well on\ntraffic signs needed for advanced drivers-assistance and autonomous systems. However, this represents a relatively small number of all traffic signs (around\n50 categories out of several hundred) and performance on the remaining set of\ntraffic signs, which are required to eliminate the manual labor in traffic-sign\ninventory management, remains an open question. In this paper, we address the\nissue of detecting and recognizing a large number of traffic-sign categories\nsuitable for automating traffic-sign inventory management. We adopt a\nconvolutional neural network (CNN) approach, the Mask R-CNN, to address the\nfull pipeline of detection and recognition with automatic end-to-end learning. We propose several improvements that are evaluated on the detection of traffic\nsigns and result in an improved overall performance. This approach is applied\nto detection of 200 traffic-sign categories represented in our novel dataset. Results are reported on highly challenging traffic-sign categories that have\nnot yet been considered in previous works. We provide comprehensive analysis of\nthe deep learning method for the detection of traffic signs with large\nintra-category appearance variation and show below 3% error rates with the\nproposed approach, which is sufficient for deployment in practical applications\nof traffic-sign inventory management."},
{"paper": "Learning Lightweight Lane Detection CNNs by Self Attention Distillation", "abstract": "Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions.In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing topdown and layer-wise attention distillation within the network itself. SAD can be easily incorporated in any feedforward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks. Our code is available at https://github.com/cardwing/Codes-for-Lane-Detection."},
{"paper": "Key Points Estimation and Point Instance Segmentation Approach for Lane Detection", "abstract": "Perception techniques for autonomous driving should be adaptive to various environments. In the case of traffic line detection, an essential perception module, many condition should be considered, such as number of traffic lines and computing power of the target system.To address these problems, in this paper, we propose a traffic line detection method called Point Instance Network (PINet); the method is based on the key points estimation and instance segmentation approach. The PINet includes several stacked hourglass networks that are trained simultaneously. Therefore the size of the trained models can be chosen according to the computing power of the target environment. We cast a clustering problem of the predicted key points as an instance segmentation problem; the PINet can be trained regardless of the number of the traffic lines. The PINet achieves competitive accuracy and false positive on the TuSimple and Culane datasets, popular public datasets for lane detection. Our code is available at https://github.com/koyeongmin/PINet_new"},
{"paper": "Transferring Rich Deep Features for Facial Beauty Prediction", "abstract": "Feature extraction plays a significant part in computer vision tasks. In this\npaper, we propose a method which transfers rich deep features from a pretrained\nmodel on face verification task and feeds the features into Bayesian ridge\nregression algorithm for facial beauty prediction.We leverage the deep neural\nnetworks that extracts more abstract features from stacked layers. Through\nsimple but effective feature fusion strategy, our method achieves improved or\ncomparable performance on SCUT-FBP dataset and ECCV HotOrNot dataset. Our\nexperiments demonstrate the effectiveness of the proposed method and clarify\nthe inner interpretability of facial beauty perception."},
{"paper": "A-LINK: Recognizing Disguised Faces via Active Learning based Inter-Domain Knowledge", "abstract": "Recent advancements in deep learning have significantly increased the capabilities of face recognition. However, face recognition in an unconstrained environment is still an active research challenge.Covariates such as pose and low resolution have received significant attention, but \u201cdisguise\u201d is considered an onerous covariate of face recognition. One primary reason for this is the unavailability of large and representative databases. To address the problem of recognizing disguised faces, we propose an active learning framework A-LINK, that intelligently selects training samples from the target domain data, such that the decision boundary does not overfit to a particular set of variations, and better generalizes to encode variability. The framework further applies domain adaptation with the actively selected training samples to fine-tune the network. We demonstrate the effectiveness of the proposed framework on DFW and Multi-PIE datasets with state-of-the-art models such as LCSSE and DenseNet."},
{"paper": "A2-LINK: Recognizing Disguised Faces via Active Learning and Adversarial Noise based Inter-Domain Knowledge", "abstract": "Face recognition in the unconstrained environment is an ongoing research challenge. Although several covariates of face recognition such as pose and low resolution have received significant attention\u201c, disguise\u201d is considered an onerous covariate of face recognition.One of the primary reasons for this is the scarcity of large and representative labeled databases, along with the lack of algorithms that work well for multiple covariates in such environments. In order to address the problem of face recognition in the presence of disguise, the paper proposes an active learning framework termed as A2-LINK. Starting with a face recognition machine-learning model, A2-LINK intelligently selects training samples from the target domain to be labeled and, using hybrid noises such as adversarial noise, fine-tunes a model that works well both in the presence and absence of disguise. Experimental results demonstrate the effectiveness and generalization of the proposed framework on the DFW and DFW2019 datasets with state-of-the-art deep learning featurization models such as LCSSE, ArcFace, and DenseNet."},
{"paper": "Unsupervised Learning of Landmarks by Descriptor Vector Exchange", "abstract": "Equivariance to random image transformations is an effective method to learn landmarks of object categories, such as the eyes and the nose in faces, without manual supervision. However, this method does not explicitly guarantee that the learned landmarks are consistent with changes between different instances of the same object, such as different facial identities.In this paper, we develop a new perspective on the equivariance approach by noting that dense landmark detectors can be interpreted as local image descriptors equipped with invariance to intra-category variations. We then propose a direct method to enforce such an invariance in the standard equivariant loss. We do so by exchanging descriptor vectors between images of different object instances prior to matching them geometrically. In this manner, the same vectors must work regardless of the specific object identity considered. We use this approach to learn vectors that can simultaneously be interpreted as local descriptors and dense landmarks, combining the advantages of both. Experiments on standard benchmarks show that this approach can match, and in some cases surpass state-of-the-art performance amongst existing methods that learn landmarks without supervision. Code is available at www.robots.ox.ac.uk/~vgg/research/DVE/."},
{"paper": "Unsupervised Learning of Object Landmarks through Conditional Image Generation", "abstract": "We propose a method for learning landmark detectors for visual objects (such\nas the eyes and the nose in a face) without any manual supervision. We cast\nthis as the problem of generating images that combine the appearance of the\nobject as seen in a first example image with the geometry of the object as seen\nin a second example image, where the two examples differ by a viewpoint change\nand/or an object deformation.In order to factorize appearance and geometry, we\nintroduce a tight bottleneck in the geometry-extraction process that selects\nand distils geometry-related features. Compared to standard image generation\nproblems, which often use generative adversarial networks, our generation task\nis conditioned on both appearance and geometry and thus is significantly less\nambiguous, to the point that adopting a simple perceptual loss formulation is\nsufficient. We demonstrate that our approach can learn object landmarks from\nsynthetic image deformations or videos, all without manual supervision, while\noutperforming state-of-the-art unsupervised landmark detectors. We further show\nthat our method is applicable to a large variety of datasets - faces, people,\n3D objects, and digits - without any modifications."},
{"paper": "Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition", "abstract": "Despite the remarkable progress in face recognition related technologies,\nreliably recognizing faces across ages still remains a big challenge. The\nappearance of a human face changes substantially over time, resulting in\nsignificant intra-class variations.As opposed to current techniques for\nage-invariant face recognition, which either directly extract age-invariant\nfeatures for recognition, or first synthesize a face that matches target age\nbefore feature extraction, we argue that it is more desirable to perform both\ntasks jointly so that they can leverage each other. To this end, we propose a\ndeep Age-Invariant Model (AIM) for face recognition in the wild with three\ndistinct novelties. First, AIM presents a novel unified deep architecture\njointly performing cross-age face synthesis and recognition in a mutual\nboosting way. Second, AIM achieves continuous face rejuvenation/aging with\nremarkable photorealistic and identity-preserving properties, avoiding the\nrequirement of paired data and the true age of testing samples. Third, we\ndevelop effective and novel training strategies for end-to-end learning the\nwhole deep architecture, which generates powerful age-invariant face\nrepresentations explicitly disentangled from the age variation. Moreover, we\npropose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset\nto facilitate existing efforts and push the frontiers of age-invariant face\nrecognition research. Extensive experiments on both our CAFR and several other\ncross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the\nproposed AIM model over the state-of-the-arts. Benchmarking our model on one of\nthe most popular unconstrained face recognition datasets IJB-C additionally\nverifies the promising generalizability of AIM in recognizing faces in the\nwild."},
{"paper": "Towards Automatic Face-to-Face Translation", "abstract": "In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as \"Face-to-Face Translation\". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization.In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards \"Face-to-Face Translation\" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available. Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0 Code and models: https://github.com/Rudrabha/LipGAN"},
{"paper": "Semi-Supervised Learning for Face Sketch Synthesis in the Wild", "abstract": "Face sketch synthesis has made great progress in the past few years. Recent methods based on deep neural networks are able to generate high quality sketches from face photos.However, due to the lack of training data (photo-sketch pairs), none of such deep learning based methods can be applied successfully to face photos in the wild. In this paper, we propose a semi-supervised deep learning architecture which extends face sketch synthesis to handle face photos in the wild by exploiting additional face photos in training. Instead of supervising the network with ground truth sketches, we first perform patch matching in feature space between the input photo and photos in a small reference set of photo-sketch pairs. We then compose a pseudo sketch feature representation using the corresponding sketch feature patches to supervise our network. With the proposed approach, we can train our networks using a small reference set of photo-sketch pairs together with a large face photo dataset without ground truth sketches. Experiments show that our method achieve state-of-the-art performance both on public benchmarks and face photos in the wild. Codes are available at https://github.com/chaofengc/Face-Sketch-Wild."},
{"task": "Facial Action Unit Detection", "papers": {"0": "Multi-View Dynamic Facial Action Unit Detection"}},
{"task": "Age And Gender Classification", "papers": {"0": "Compacting, Picking and Growing for Unforgetting Continual Learning", "1": "Compacting, Picking and Growing for Unforgetting Continual Learning"}},
{"task": "Action Unit Detection", "papers": {"0": "AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection"}},
{"task": "Face Hallucination", "papers": {"0": "HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment"}},
{"task": "Robust Face Recognition", "papers": {}},
{"task": "Facial Inpainting", "papers": {"0": "Learning Symmetry Consistent Deep CNNs for Face Completion", "1": "Learning Symmetry Consistent Deep CNNs for Face Completion", "2": "Image Fine-grained Inpainting"}},
{"task": "Age Estimation", "papers": {"0": "C3AE: Exploring the Limits of Compact Model for Age Estimation", "1": "Rank-consistent Ordinal Regression for Neural Networks", "2": "Deep Ordinal Regression with Label Diversity", "3": "Facial age estimation by deep residual decision making", "4": "Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression", "5": "Rank-consistent Ordinal Regression for Neural Networks", "6": "Deep Label Distribution Learning with Label Ambiguity"}},
{"task": "Gender Prediction", "papers": {"0": "Increasingly Packing Multiple Facial-Informatics Modules in A Unified Deep-Learning Model via Lifelong Learning"}},
{"task": "Face Identification", "papers": {"0": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", "1": "Support Vector Guided Softmax Loss for Face Recognition", "2": "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "3": "FacePoseNet: Making a Case for Landmark-Free Face Alignment"}},
{"task": "Face Reconstruction", "papers": {}},
{"task": "Face Swapping", "papers": {}},
{"task": "Face Anti-Spoofing", "papers": {"0": "Learn Convolutional Neural Network for Face Anti-Spoofing", "1": "Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing", "2": "Learn Convolutional Neural Network for Face Anti-Spoofing", "3": "Deep Pixel-wise Binary Supervision for Face Presentation Attack Detection", "4": "Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks"}},
{"task": "3D Face Reconstruction", "papers": {"0": "Towards Fast, Accurate and Stable 3D Dense Face Alignment", "1": "Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network"}},
{"paper": "Multi-View Dynamic Facial Action Unit Detection", "abstract": "We propose a novel convolutional neural network approach to address the\nfine-grained recognition problem of multi-view dynamic facial action unit\ndetection. We leverage recent gains in large-scale object recognition by\nformulating the task of predicting the presence or absence of a specific action\nunit in a still image of a human face as holistic classification.We then\nexplore the design space of our approach by considering both shared and\nindependent representations for separate action units, and also different CNN\narchitectures for combining color and motion information. We then move to the\nnovel setup of the FERA 2017 Challenge, in which we propose a multi-view\nextension of our approach that operates by first predicting the viewpoint from\nwhich the video was taken, and then evaluating an ensemble of action unit\ndetectors that were trained for that specific viewpoint. Our approach is\nholistic, efficient, and modular, since new action units can be easily included\nin the overall system. Our approach significantly outperforms the baseline of\nthe FERA 2017 Challenge, with an absolute improvement of 14% on the F1-metric. Additionally, it compares favorably against the winner of the FERA 2017\nchallenge. Code source is available at https://github.com/BCV-Uniandes/AUNets."},
{"paper": "AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection", "abstract": "Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions.However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that \\textit{only} static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available online."},
{"task": "Facial Landmark Detection", "papers": {"0": "Face Alignment using a 3D Deeply-initialized Ensemble of Regression Trees", "1": "Style Aggregated Network for Facial Landmark Detection", "2": "Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors", "3": "Style Aggregated Network for Facial Landmark Detection", "4": "Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection", "5": "Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization"}},
{"paper": "Compacting, Picking and Growing for Unforgetting Continual Learning", "abstract": "Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning.Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training."},
{"paper": "Image Fine-grained Inpainting", "abstract": "Image inpainting techniques have shown promising improvement with the assistance of generative adversarial networks (GANs) recently. However, most of them often suffered from completed results with unreasonable structure or blurriness.To mitigate this problem, in this paper, we present a one-stage model that utilizes dense combinations of dilated convolutions to obtain larger and more effective receptive fields. Benefited from the property of this network, we can more easily recover large regions in an incomplete image. To better train this efficient generator, except for frequently-used VGG feature matching loss, we design a novel self-guided regression loss for concentrating on uncertain areas and enhancing the semantic details. Besides, we devise a geometrical alignment constraint item to compensate for the pixel-based distance between prediction features and ground-truth ones. We also employ a discriminator with local and global branches to ensure local-global contents consistency. To further improve the quality of generated images, discriminator feature matching on the local branch is introduced, which dynamically minimizes the similarity of intermediate features between synthetic and ground-truth patches. Extensive experiments on several public datasets demonstrate that our approach outperforms current state-of-the-art methods. Code is available at https://github.com/Zheng222/DMFN."},
{"paper": "Deep Label Distribution Learning with Label Ambiguity", "abstract": "Convolutional Neural Networks (ConvNets) have achieved excellent recognition\nperformance in various visual recognition tasks. A large labeled training set\nis one of the most important factors for its success.However, it is difficult\nto collect sufficient training images with precise labels in some domains such\nas apparent age estimation, head pose estimation, multi-label classification\nand semantic segmentation. Fortunately, there is ambiguous information among\nlabels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete\nlabel distribution, and learn the label distribution by minimizing a\nKullback-Leibler divergence between the predicted and ground-truth label\ndistributions using deep ConvNets. The proposed DLDL (Deep Label Distribution\nLearning) method effectively utilizes the label ambiguity in both feature\nlearning and classifier learning, which help prevent the network from\nover-fitting even when the training set is small. Experimental results show\nthat the proposed approach produces significantly better results than\nstate-of-the-art methods for age estimation and head pose estimation. At the\nsame time, it also improves recognition performance for multi-label\nclassification and semantic segmentation tasks."},
{"paper": "Increasingly Packing Multiple Facial-Informatics Modules in A Unified Deep-Learning Model via Lifelong Learning", "abstract": "Simultaneously running multiple modules is a key requirement for a smart multimedia system for facial applications including face recognition, facial expression understanding, and gender identification. To effectively integrate them, a continual learning approach to learn new tasks without forgetting is introduced.Unlike previous methods growing monotonically in size, our approach maintains the compactness in continual learning. The proposed packing-and-expanding method is effective and easy to implement, which can iteratively shrink and enlarge the model to integrate new functions. Our integrated multitask model can achieve similar accuracy with only 39.9% of the original size."},
{"paper": "FacePoseNet: Making a Case for Landmark-Free Face Alignment", "abstract": "We show how a simple convolutional neural network (CNN) can be trained to\naccurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose,\ndirectly from image intensities. We further explain how this FacePoseNet (FPN)\ncan be used to align faces in 2D and 3D as an alternative to explicit facial\nlandmark detection for these tasks.We claim that in many cases the standard\nmeans of measuring landmark detector accuracy can be misleading when comparing\ndifferent face alignments. Instead, we compare our FPN with existing methods by\nevaluating how they affect face recognition accuracy on the IJB-A and IJB-B\nbenchmarks: using the same recognition pipeline, but varying the face alignment\nmethod. Our results show that (a) better landmark detection accuracy measured\non the 300W benchmark does not necessarily imply better face recognition\naccuracy. (b) Our FPN provides superior 2D and 3D face alignment on both\nbenchmarks. Finally, (c), FPN aligns faces at a small fraction of the\ncomputational cost of comparably accurate landmark detectors. For many\npurposes, FPN is thus a far faster and far more accurate face alignment method\nthan using facial landmark detectors."},
{"paper": "Support Vector Guided Softmax Loss for Face Recognition", "abstract": "Face recognition has witnessed significant progresses due to the advances of\ndeep convolutional neural networks (CNNs), the central challenge of which, is\nfeature discrimination. To address it, one group tries to exploit mining-based\nstrategies (\\textit{e.g.}, hard example mining and focal loss) to focus on the\ninformative examples.The other group devotes to designing margin-based loss\nfunctions (\\textit{e.g.}, angular, additive and additive angular margins) to\nincrease the feature margin from the perspective of ground truth class. Both of\nthem have been well-verified to learn discriminative features. However, they\nsuffer from either the ambiguity of hard examples or the lack of discriminative\npower of other classes. In this paper, we design a novel loss function, namely\nsupport vector guided softmax loss (SV-Softmax), which adaptively emphasizes\nthe mis-classified points (support vectors) to guide the discriminative\nfeatures learning. So the developed SV-Softmax loss is able to eliminate the\nambiguity of hard examples as well as absorb the discriminative power of other\nclasses, and thus results in more discrimiantive features. To the best of our\nknowledge, this is the first attempt to inherit the advantages of mining-based\nand margin-based losses into one framework. Experimental results on several\nbenchmarks have demonstrated the effectiveness of our approach over\nstate-of-the-arts."},
{"paper": "Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks", "abstract": "Face recognition has evolved as a widely used biometric modality. However, its vulnerability against presentation attacks poses a significant security threat.Though presentation attack detection (PAD) methods try to address this issue, they often fail in generalizing to unseen attacks. In this work, we propose a new framework for PAD using a one-class classifier, where the representation used is learned with a Multi-Channel Convolutional Neural Network (MCCNN). A novel loss function is introduced, which forces the network to learn a compact embedding for bonafide class while being far from the representation of attacks. A one-class Gaussian Mixture Model is used on top of these embeddings for the PAD task. The proposed framework introduces a novel approach to learn a robust PAD system from bonafide and available (known) attack classes. This is particularly important as collecting bonafide data and simpler attacks are much easier than collecting a wide variety of expensive attacks. The proposed system is evaluated on the publicly available WMCA multi-channel face PAD database, which contains a wide variety of 2D and 3D attacks. Further, we have performed experiments with MLFP and SiW-M datasets using RGB channels only. Superior performance in unseen attack protocols shows the effectiveness of the proposed approach. Software, data, and protocols to reproduce the results are made available publicly."},
{"paper": "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "abstract": "Face recognition achieves exceptional success thanks to the emergence of deep\nlearning. However, many contemporary face recognition models still perform\nrelatively poor in processing profile faces compared to frontal faces.A key\nreason is that the number of frontal and profile training faces are highly\nimbalanced - there are extensively more frontal training samples compared to\nprofile ones. In addition, it is intrinsically hard to learn a deep\nrepresentation that is geometrically invariant to large pose variations. In\nthis study, we hypothesize that there is an inherent mapping between frontal\nand profile faces, and consequently, their discrepancy in the deep\nrepresentation space can be bridged by an equivariant mapping. To exploit this\nmapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block,\nwhich is capable of adaptively adding residuals to the input deep\nrepresentation to transform a profile face representation to a canonical pose\nthat simplifies recognition. The DREAM block consistently enhances the\nperformance of profile face recognition for many strong deep networks,\nincluding ResNet models, without deliberately augmenting training data of\nprofile faces. The block is easy to use, light-weight, and can be implemented\nwith a negligible computational overhead."},
{"task": "Face Generation", "papers": {}},
{"paper": "Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network", "abstract": "We propose a straightforward method that simultaneously reconstructs the 3D\nfacial structure and provides dense alignment. To achieve this, we design a 2D\nrepresentation called UV position map which records the 3D shape of a complete\nface in UV space, then train a simple Convolutional Neural Network to regress\nit from a single 2D image.We also integrate a weight mask into the loss\nfunction during training to improve the performance of the network. Our method\ndoes not rely on any prior face model, and can reconstruct full facial geometry\nalong with semantic meaning. Meanwhile, our network is very light-weighted and\nspends only 9.8ms to process an image, which is extremely faster than previous\nworks. Experiments on multiple challenging datasets show that our method\nsurpasses other state-of-the-art methods on both reconstruction and alignment\ntasks by a large margin."},
{"paper": "Deep Pixel-wise Binary Supervision for Face Presentation Attack Detection", "abstract": "Face recognition has evolved as a prominent biometric authentication modality. However, vulnerability to presentation attacks curtails its reliable deployment.Automatic detection of presentation attacks is essential for secure use of face recognition technology in unattended scenarios. In this work, we introduce a Convolutional Neural Network (CNN) based framework for presentation attack detection, with deep pixel-wise supervision. The framework uses only frame level information making it suitable for deployment in smart devices with minimal computational and time overhead. We demonstrate the effectiveness of the proposed approach in public datasets for both intra as well as cross-dataset experiments. The proposed approach achieves an HTER of 0% in Replay Mobile dataset and an ACER of 0.42% in Protocol-1 of OULU dataset outperforming state of the art methods."},
{"task": "Facial Expression Recognition", "papers": {"0": "Increasingly Packing Multiple Facial-Informatics Modules in A Unified Deep-Learning Model via Lifelong Learning", "1": "Challenges in Representation Learning: A report on three machine learning contests", "2": "Frame attention networks for facial expression recognition in videos", "3": "MicroExpNet: An Extremely Small and Fast Model For Expression Recognition From Face Images", "4": "Pyramid With Super Resolution for In-the-Wild Facial Expression Recognition", "5": "Covariance Pooling For Facial Expression Recognition", "6": "DeXpression: Deep Convolutional Neural Network for Expression Recognition", "7": "Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition", "8": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition", "9": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition", "10": "Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network", "11": "Covariance Pooling For Facial Expression Recognition", "12": "Greedy Search for Descriptive Spatial Face Features", "13": "Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network", "14": "Efficient Facial Feature Learning with Wide Ensemble-based Convolutional Neural Networks"}},
{"paper": "Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing", "abstract": "Face anti-spoofing (a.k.a presentation attack detection) has drawn growing\nattention due to the high-security demand in face authentication systems. Existing CNN-based approaches usually well recognize the spoofing faces when\ntraining and testing spoofing samples display similar patterns, but their\nperformance would drop drastically on testing spoofing faces of unseen scenes.In this paper, we try to boost the generalizability and applicability of these\nmethods by designing a CNN model with two major novelties. First, we propose a\nsimple yet effective Total Pairwise Confusion (TPC) loss for CNN training,\nwhich enhances the generalizability of the learned Presentation Attack (PA)\nrepresentations. Secondly, we incorporate a Fast Domain Adaptation (FDA)\ncomponent into the CNN model to alleviate negative effects brought by domain\nchanges. Besides, our proposed model, which is named Generalizable Face\nAuthentication CNN (GFA-CNN), works in a multi-task manner, performing face\nanti-spoofing and face recognition simultaneously. Experimental results show\nthat GFA-CNN outperforms previous face anti-spoofing approaches and also well\npreserves the identity information of input face images."},
{"paper": "Towards Fast, Accurate and Stable 3D Dense Face Alignment", "abstract": "Existing methods of 3D dense face alignment mainly concentrate on accuracy, thus limiting the scope of their practical applications. In this paper, we propose a novel regression framework which makes a balance among speed, accuracy and stability.Firstly, on the basis of a lightweight backbone, we propose a meta-joint optimization strategy to dynamically regress a small set of 3DMM parameters, which greatly enhances speed and accuracy simultaneously. To further improve the stability on videos, we present a virtual synthesis method to transform one still image to a short-video which incorporates in-plane and out-of-plane face moving. On the premise of high accuracy and stability, our model runs at over 50fps on a single CPU core and outperforms other state-of-the-art heavy models simultaneously. Experiments on several challenging datasets validate the efficiency of our method. Pre-trained models and code are available at https://github.com/cleardusk/3DDFA_V2."},
{"paper": "Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection", "abstract": "Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images.A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data to become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance."},
{"paper": "Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization", "abstract": "3D face shape is more expressive and viewpoint-consistent than its 2D\ncounterpart. However, 3D facial landmark localization in a single image is\nchallenging due to the ambiguous nature of landmarks under 3D perspective.Existing approaches typically adopt a suboptimal two-step strategy, performing\n2D landmark localization followed by depth estimation. In this paper, we\npropose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial\nlandmark localization, addressing it more effectively in an end-to-end fashion. First, a compact volumetric representation is proposed to encode the per-voxel\nlikelihood of positions being the 3D landmarks. The dimensionality of such a\nrepresentation is fixed regardless of the number of target landmarks, so that\nthe curse of dimensionality could be avoided. Then, a stacked hourglass network\nis adopted to estimate the volumetric representation from coarse to fine,\nfollowed by a 3D convolution network that takes the estimated volume as input\nand regresses 3D coordinates of the face shape. In this way, the 3D structural\nconstraints between landmarks could be learned by the neural network in a more\nefficient manner. Moreover, the proposed pipeline enables end-to-end training\nand improves the robustness and accuracy of 3D facial landmark localization. The effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D\ndatasets. Experimental results show that the proposed method achieves\nstate-of-the-art performance in comparison with existing methods."},
{"paper": "Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors", "abstract": "In this paper, we present supervision-by-registration, an unsupervised\napproach to improve the precision of facial landmark detectors on both images\nand video. Our key observation is that the detections of the same landmark in\nadjacent frames should be coherent with registration, i.e., optical flow.Interestingly, the coherency of optical flow is a source of supervision that\ndoes not require manual labeling, and can be leveraged during detector\ntraining. For example, we can enforce in the training loss function that a\ndetected landmark at frame$_{t-1}$ followed by optical flow tracking from\nframe$_{t-1}$ to frame$_t$ should coincide with the location of the detection\nat frame$_t$. Essentially, supervision-by-registration augments the training\nloss function with a registration loss, thus training the detector to have\noutput that is not only close to the annotations in labeled images, but also\nconsistent with registration on large amounts of unlabeled videos. End-to-end\ntraining with the registration loss is made possible by a differentiable\nLucas-Kanade operation, which computes optical flow registration in the forward\npass, and back-propagates gradients that encourage temporal coherency in the\ndetector. The output of our method is a more precise image-based facial\nlandmark detector, which can be applied to single images or video. With\nsupervision-by-registration, we demonstrate (1) improvements in facial landmark\ndetection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities),\nand (2) significant reduction of jittering in video detections."},
{"paper": "Style Aggregated Network for Facial Landmark Detection", "abstract": "Recent advances in facial landmark detection achieve success by learning\ndiscriminative features from rich deformation of face shapes and poses. Besides\nthe variance of faces themselves, the intrinsic variance of image styles, e.g.,\ngrayscale vs. color images, light vs. dark, intense vs. dull, and so on, has\nconstantly been overlooked.This issue becomes inevitable as increasing web\nimages are collected from various sources for training neural networks. In this\nwork, we propose a style-aggregated approach to deal with the large intrinsic\nvariance of image styles for facial landmark detection. Our method transforms\noriginal face images to style-aggregated images by a generative adversarial\nmodule. The proposed scheme uses the style-aggregated image to maintain face\nimages that are more robust to environmental changes. Then the original face\nimages accompanying with style-aggregated ones play a duet to train a landmark\ndetector which is complementary to each other. In this way, for each face, our\nmethod takes two images as input, i.e., one in its original style and the other\nin the aggregated style. In experiments, we observe that the large variance of\nimage styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image\nstyles by comparing to a variant of our approach, in which the generative\nadversarial module is removed, and no style-aggregated images are used. Our\napproach is demonstrated to perform well when compared with state-of-the-art\nalgorithms on benchmark datasets AFLW and 300-W. Code is publicly available on\nGitHub: https://github.com/D-X-Y/SAN"},
{"paper": "Face Alignment using a 3D Deeply-initialized Ensemble of Regression Trees", "abstract": "Face alignment algorithms locate a set of landmark points in images of faces taken in unrestricted situations. State-of-the-art approaches typically fail or lose accuracy in the presence of occlusions, strong deformations, large pose variations and ambiguous configurations.In this paper we present 3DDE, a robust and efficient face alignment algorithm based on a coarse-to-fine cascade of ensembles of regression trees. It is initialized by robustly fitting a 3D face model to the probability maps produced by a convolutional neural network. With this initialization we address self-occlusions and large face rotations. Further, the regressor implicitly imposes a prior face shape on the solution, addressing occlusions and ambiguous face configurations. Its coarse-to-fine structure tackles the combinatorial explosion of parts deformation. In the experiments performed, 3DDE improves the state-of-the-art in 300W, COFW, AFLW and WFLW data sets. Finally, we perform cross-dataset experiments that reveal the existence of a significant data set bias in these benchmarks."},
{"paper": "Learn Convolutional Neural Network for Face Anti-Spoofing", "abstract": "Though having achieved some progresses, the hand-crafted texture features, e.g., LBP [23], LBP-TOP [11] are still unable to capture the most discriminative cues between genuine and fake faces. In this paper, instead of designing feature by ourselves, we rely on the deep convolutional neural network (CNN) to learn features of high discriminative ability in a supervised manner.Combined with some data pre-processing, the face anti-spoofing performance improves drastically. In the experiments, over 70% relative decrease of Half Total Error Rate (HTER) is achieved on two challenging datasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art. Meanwhile, the experimental results from inter-tests between two datasets indicates CNN can obtain features with better generalization ability. Moreover, the nets trained using combined data from two datasets have less biases between two datasets."},
{"paper": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", "abstract": "One of the main challenges in feature learning using Deep Convolutional\nNeural Networks (DCNNs) for large-scale face recognition is the design of\nappropriate loss functions that enhance discriminative power. Centre loss\npenalises the distance between the deep features and their corresponding class\ncentres in the Euclidean space to achieve intra-class compactness.SphereFace\nassumes that the linear transformation matrix in the last fully connected layer\ncan be used as a representation of the class centres in an angular space and\npenalises the angles between the deep features and their corresponding weights\nin a multiplicative way. Recently, a popular line of research is to incorporate\nmargins in well-established loss functions in order to maximise face class\nseparability. In this paper, we propose an Additive Angular Margin Loss\n(ArcFace) to obtain highly discriminative features for face recognition. The\nproposed ArcFace has a clear geometric interpretation due to the exact\ncorrespondence to the geodesic distance on the hypersphere. We present arguably\nthe most extensive experimental evaluation of all the recent state-of-the-art\nface recognition methods on over 10 face recognition benchmarks including a new\nlarge-scale image database with trillion level of pairs and a large-scale video\ndataset. We show that ArcFace consistently outperforms the state-of-the-art and\ncan be easily implemented with negligible computational overhead. We release\nall refined training data, training codes, pre-trained models and training\nlogs, which will help reproduce the results in this paper."},
{"paper": "Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression", "abstract": "Facial aging and facial rejuvenation analyze a given face photograph to\npredict a future look or estimate a past look of the person. To achieve this,\nit is critical to preserve human identity and the corresponding aging\nprogression and regression with high accuracy.However, existing methods cannot\nsimultaneously handle these two objectives well. We propose a novel generative\nadversarial network based approach, named the Conditional Multi-Adversarial\nAutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation\ntechnique to control the aging accuracy and takes a high-level feature\nrepresentation to preserve personalized identity. Specifically, the face is\nfirst mapped to a latent vector through a convolutional encoder. The latent\nvector is then projected onto the face manifold conditional on the age through\na deconvolutional generator. The latent vector preserves personalized face\nfeatures and the age controls facial aging and rejuvenation. A discriminator\nand an ordinal regression are imposed on the encoder and the generator in\ntandem, making the generated face images to be more photorealistic while\nsimultaneously exhibiting desirable aging effects. Besides, a high-level\nfeature representation is utilized to preserve personalized identity of the\ngenerated face. Experiments on two benchmark datasets demonstrate appealing\nperformance of the proposed method over the state-of-the-art."},
{"paper": "Facial age estimation by deep residual decision making", "abstract": "Residual representation learning simplifies the optimization problem of learning complex functions and has been widely used by traditional convolutional neural networks. However, it has not been applied to deep neural decision forest (NDF).In this paper we incorporate residual learning into NDF and the resulting model achieves state-of-the-art level accuracy on three public age estimation benchmarks while requiring less memory and computation. We further employ gradient-based technique to visualize the decision-making process of NDF and understand how it is influenced by facial image inputs. The code and pre-trained models will be available at https://github.com/Nicholasli1995/VisualizingNDF."},
{"paper": "Rank-consistent Ordinal Regression for Neural Networks", "abstract": "In many real-world predictions tasks, class labels include information about the relative ordering between labels, which is not captured by commonly-used loss functions such as multi-category cross-entropy. Recently, ordinal regression frameworks have been adopted by the deep learning community to take such ordering information into account.Using a framework that transforms ordinal targets into binary classification subtasks, neural networks were equipped with ordinal regression capabilities. However, this method suffers from inconsistencies among the different binary classifiers. We hypothesize that addressing the inconsistency issue in these binary classification task-based neural networks improves predictive performance. To test this hypothesis, we propose the COnsistent RAnk Logits (CORAL) framework with strong theoretical guarantees for rank-monotonicity and consistent confidence scores. Moreover, the proposed method is architecture-agnostic and can extend arbitrary state-of-the-art deep neural network classifiers for ordinal regression tasks. The empirical evaluation of the proposed rank-consistent method on a range of face-image datasets for age prediction shows a substantial reduction of the prediction error compared to the reference ordinal regression network."},
{"paper": "Efficient Facial Feature Learning with Wide Ensemble-based Convolutional Neural Networks", "abstract": "Ensemble methods, traditionally built with independently trained de-correlated models, have proven to be efficient methods for reducing the remaining residual generalization error, which results in robust and accurate methods for real-world applications. In the context of deep learning, however, training an ensemble of deep networks is costly and generates high redundancy which is inefficient.In this paper, we present experiments on Ensembles with Shared Representations (ESRs) based on convolutional networks to demonstrate, quantitatively and qualitatively, their data processing efficiency and scalability to large-scale datasets of facial expressions. We show that redundancy and computational load can be dramatically reduced by varying the branching level of the ESR without loss of diversity and generalization power, which are both important for ensemble performance. Experiments on large-scale datasets suggest that ESRs reduce the remaining residual generalization error on the AffectNet and FER+ datasets, reach human-level performance, and outperform state-of-the-art methods on facial expression recognition in the wild using emotion and affect concepts."},
{"paper": "Learning Symmetry Consistent Deep CNNs for Face Completion", "abstract": "Deep convolutional networks (CNNs) have achieved great success in face\ncompletion to generate plausible facial structures. These methods, however, are\nlimited in maintaining global consistency among face components and recovering\nfine facial details.On the other hand, reflectional symmetry is a prominent\nproperty of face image and benefits face recognition and consistency modeling,\nyet remaining uninvestigated in deep face completion. In this work, we leverage\ntwo kinds of symmetry-enforcing subnets to form a symmetry-consistent CNN model\n(i.e., SymmFCNet) for effective face completion. For missing pixels on only one\nof the half-faces, an illumination-reweighted warping subnet is developed to\nguide the warping and illumination reweighting of the other half-face. As for\nmissing pixels on both of half-faces, we present a generative reconstruction\nsubnet together with a perceptual symmetry loss to enforce symmetry consistency\nof recovered structures. The SymmFCNet is constructed by stacking generative\nreconstruction subnet upon illumination-reweighted warping subnet, and can be\nend-to-end learned from training set of unaligned face images. Experiments show\nthat SymmFCNet can generate high quality results on images with synthetic and\nreal occlusion, and performs favorably against state-of-the-arts."},
{"task": "Face Alignment", "papers": {"0": "Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression", "1": "Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation", "2": "Towards Fast, Accurate and Stable 3D Dense Face Alignment", "3": "Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression", "4": "Nonlinear 3D Face Morphable Model", "5": "Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network", "6": "Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment", "7": "Towards Fast, Accurate and Stable 3D Dense Face Alignment", "8": "Progressive Face Super-Resolution via Attention to Facial Landmark", "9": "Progressive Face Super-Resolution via Attention to Facial Landmark", "10": "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)", "11": "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)", "12": "Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources", "13": "Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face Alignment in the Wild (3DFAW) Challenge", "14": "Convolutional aggregation of local evidence for large pose face alignment", "15": "Convolutional aggregation of local evidence for large pose face alignment"}},
{"paper": "Greedy Search for Descriptive Spatial Face Features", "abstract": "Facial expression recognition methods use a combination of geometric and\nappearance-based features. Spatial features are derived from displacements of\nfacial landmarks, and carry geometric information.These features are either\nselected based on prior knowledge, or dimension-reduced from a large pool. In\nthis study, we produce a large number of potential spatial features using two\ncombinations of facial landmarks. Among these, we search for a descriptive\nsubset of features using sequential forward selection. The chosen feature\nsubset is used to classify facial expressions in the extended Cohn-Kanade\ndataset (CK+), and delivered 88.7% recognition accuracy without using any\nappearance-based features."},
{"paper": "C3AE: Exploring the Limits of Compact Model for Age Estimation", "abstract": "Age estimation is a classic learning problem in computer vision. Many larger\nand deeper CNNs have been proposed with promising performance, such as AlexNet,\nVggNet, GoogLeNet and ResNet.However, these models are not practical for the\nembedded/mobile devices. Recently, MobileNets and ShuffleNets have been\nproposed to reduce the number of parameters, yielding lightweight models. However, their representation has been weakened because of the adoption of\ndepth-wise separable convolution. In this work, we investigate the limits of\ncompact model for small-scale image and propose an extremely Compact yet\nefficient Cascade Context-based Age Estimation model(C3AE). This model\npossesses only 1/9 and 1/2000 parameters compared with MobileNets/ShuffleNets\nand VggNet, while achieves competitive performance. In particular, we re-define\nage estimation problem by two-points representation, which is implemented by a\ncascade model. Moreover, to fully utilize the facial context information,\nmulti-branch CNN network is proposed to aggregate multi-scale context. Experiments are carried out on three age estimation datasets. The\nstate-of-the-art performance on compact model has been achieved with a\nrelatively large margin."},
{"paper": "Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network", "abstract": "Facial expression recognition has been an active research area over the past\nfew decades, and it is still challenging due to the high intra-class variation. Traditional approaches for this problem rely on hand-crafted features such as\nSIFT, HOG and LBP, followed by a classifier trained on a database of images or\nvideos.Most of these works perform reasonably well on datasets of images captured in\na controlled condition, but fail to perform as good on more challenging\ndatasets with more image variation and partial faces. In recent years, several works proposed an end-to-end framework for facial\nexpression recognition, using deep learning models. Despite the better performance of these works, there still seems to be a\ngreat room for improvement. In this work, we propose a deep learning approach based on attentional\nconvolutional network, which is able to focus on important parts of the face,\nand achieves significant improvement over previous models on multiple datasets,\nincluding FER-2013, CK+, FERG, and JAFFE. We also use a visualization technique which is able to find important face\nregions for detecting different emotions, based on the classifier's output. Through experimental results, we show that different emotions seems to be\nsensitive to different parts of the face."},
{"task": "Face Detection", "papers": {"0": "RetinaFace: Single-stage Dense Face Localisation in the Wild", "1": "Accurate Face Detection for High Performance", "2": "Accurate Face Detection for High Performance", "3": "DSFD: Dual Shot Face Detector", "4": "Selective Refinement Network for High Performance Face Detection", "5": "Selective Refinement Network for High Performance Face Detection"}},
{"task": "Face Verification", "papers": {"0": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition", "1": "Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis", "2": "SeqFace: Make full use of sequence information for face recognition", "3": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", "4": "Support Vector Guided Softmax Loss for Face Recognition", "5": "Dynamic Multi-Task Learning for Face Recognition with Facial Expression", "6": "Dynamic Multi-Task Learning for Face Recognition with Facial Expression", "7": "Probabilistic Face Embeddings", "8": "FacePoseNet: Making a Case for Landmark-Free Face Alignment", "9": "Dual Variational Generation for Low-Shot Heterogeneous Face Recognition", "10": "Dual Variational Generation for Low-Shot Heterogeneous Face Recognition", "11": "Dual Variational Generation for Low-Shot Heterogeneous Face Recognition", "12": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition", "13": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition", "14": "Dual Variational Generation for Low-Shot Heterogeneous Face Recognition", "15": "MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices"}},
{"paper": "DeXpression: Deep Convolutional Neural Network for Expression Recognition", "abstract": "We propose a convolutional neural network (CNN) architecture for facial\nexpression recognition. The proposed architecture is independent of any\nhand-crafted feature extraction and performs better than the earlier proposed\nconvolutional neural network based approaches.We visualize the automatically\nextracted features which have been learned by the network in order to provide a\nbetter understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP)\nand MMI Facial Expression Databse are used for the quantitative evaluation. On\nthe CKP set the current state of the art approach, using CNNs, achieves an\naccuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion\nrecognition is 93.33%. The proposed architecture achieves 99.6% for CKP and\n98.63% for MMI, therefore performing better than the state of the art using\nCNNs. Automatic facial expression recognition has a broad spectrum of\napplications such as human-computer interaction and safety systems. This is due\nto the fact that non-verbal cues are important forms of communication and play\na pivotal role in interpersonal communication. The performance of the proposed\narchitecture endorses the efficacy and reliable usage of the proposed work for\nreal world applications."},
{"paper": "Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition", "abstract": "Facial expression recognition from videos in the wild is a challenging task due to the lack of abundant labelled training data. Large DNN (deep neural network) architectures and ensemble methods have resulted in better performance, but soon reach saturation at some point due to data inadequacy.In this paper, we use a self-training method that utilizes a combination of a labelled dataset and an unlabelled dataset (Body Language Dataset - BoLD). Experimental analysis shows that training a noisy student network iteratively helps in achieving significantly better results. Additionally, our model isolates different regions of the face and processes them independently using a multi-level attention mechanism which further boosts the performance. Our results show that the proposed method achieves state-of-the-art performance on benchmark datasets CK+ and AFEW 8.0 when compared to other single models."},
{"paper": "Covariance Pooling For Facial Expression Recognition", "abstract": "Classifying facial expressions into different categories requires capturing\nregional distortions of facial landmarks. We believe that second-order\nstatistics such as covariance is better able to capture such distortions in\nregional facial fea- tures.In this work, we explore the benefits of using a\nman- ifold network structure for covariance pooling to improve facial\nexpression recognition. In particular, we first employ such kind of manifold\nnetworks in conjunction with tradi- tional convolutional networks for spatial\npooling within in- dividual image feature maps in an end-to-end deep learning\nmanner. By doing so, we are able to achieve a recognition accuracy of 58.14% on\nthe validation set of Static Facial Expressions in the Wild (SFEW 2.0) and\n87.0% on the vali- dation set of Real-World Affective Faces (RAF) Database. Both of these results are the best results we are aware of. Besides, we\nleverage covariance pooling to capture the tem- poral evolution of per-frame\nfeatures for video-based facial expression recognition. Our reported results\ndemonstrate the advantage of pooling image-set features temporally by stacking\nthe designed manifold network of covariance pool-ing on top of convolutional\nnetwork layers."},
{"paper": "Pyramid With Super Resolution for In-the-Wild Facial Expression Recognition", "abstract": "Facial Expression Recognition (FER) is a challenging task that improves natural human-computer interaction. This paper focuses on automatic FER on a single in-the-wild (ITW) image.ITW images suffer real problems of pose, direction, and input resolution. In this study, we propose a pyramid with super-resolution (PSR) network architecture to solve the ITW FER task. We also introduce a prior distribution label smoothing (PDLS) loss function that applies the additional prior knowledge of the confusion about each expression in the FER task. Experiments on the three most popular ITW FER datasets showed that our approach outperforms all the state-of-the-art methods."},
{"paper": "MicroExpNet: An Extremely Small and Fast Model For Expression Recognition From Face Images", "abstract": "This paper is aimed at creating extremely small and fast convolutional neural networks (CNN) for the problem of facial expression recognition (FER) from frontal face images. To this end, we employed the popular knowledge distillation (KD) method and identified two major shortcomings with its use: 1) a fine-grained grid search is needed for tuning the temperature hyperparameter and 2) to find the optimal size-accuracy balance, one needs to search for the final network size (or the compression rate).On the other hand, KD is proved to be useful for model compression for the FER problem, and we discovered that its effects gets more and more significant with the decreasing model size. In addition, we hypothesized that translation invariance achieved using max-pooling layers would not be useful for the FER problem as the expressions are sensitive to small, pixel-wise changes around the eye and the mouth. However, we have found an intriguing improvement on generalization when max-pooling is used. We conducted experiments on two widely-used FER datasets, CK+ and Oulu-CASIA. Our smallest model (MicroExpNet), obtained using knowledge distillation, is less than 1MB in size and works at 1851 frames per second on an Intel i7 CPU. Despite being less accurate than the state-of-the-art, MicroExpNet still provides significant insights for designing a microarchitecture for the FER problem."},
{"paper": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition", "abstract": "Occlusion and pose variations, which can change facial appearance significantly, are two major obstacles for automatic Facial Expression Recognition (FER). Though automatic FER has made substantial progresses in the past few decades, occlusion-robust and pose-invariant issues of FER have received relatively less attention, especially in real-world scenarios.This paper addresses the real-world pose and occlusion robust FER problem with three-fold contributions. First, to stimulate the research of FER under real-world occlusions and variant poses, we build several in-the-wild facial expression datasets with manual annotations for the community. Second, we propose a novel Region Attention Network (RAN), to adaptively capture the importance of facial regions for occlusion and pose variant FER. The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed-length representation. Last, inspired by the fact that facial expressions are mainly defined by facial action units, we propose a region biased loss to encourage high attention weights for the most important regions. We validate our RAN and region biased loss on both our built test datasets and four popular datasets: FERPlus, AffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose. Our method also achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW. Code and the collected test data will be publicly available."},
{"paper": "Frame attention networks for facial expression recognition in videos", "abstract": "The video-based facial expression recognition aims to classify a given video into several basic emotions. How to integrate facial features of individual frames is crucial for this task.In this paper, we propose the Frame Attention Networks (FAN), to automatically highlight some discriminative frames in an end-to-end framework. The network takes a video with a variable number of face images as its input and produces a fixed-dimension representation. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which embeds face images into feature vectors. The frame attention module learns multiple attention weights which are used to adaptively aggregate the feature vectors to form a single discriminative video representation. We conduct extensive experiments on CK+ and AFEW8.0 datasets. Our proposed FAN shows superior performance compared to other CNN based methods and achieves state-of-the-art performance on CK+."},
{"paper": "Challenges in Representation Learning: A report on three machine learning contests", "abstract": "The ICML 2013 Workshop on Challenges in Representation Learning focused on\nthree challenges: the black box learning challenge, the facial expression\nrecognition challenge, and the multimodal learning challenge. We describe the\ndatasets created for these challenges and summarize the results of the\ncompetitions.We provide suggestions for organizers of future challenges and\nsome comments on what kind of knowledge can be gained from machine learning\ncompetitions."},
{"paper": "Convolutional aggregation of local evidence for large pose face alignment", "abstract": "Methods for unconstrained face alignment must satisfy two requirements: they must not rely on accurate initialisation/face detection and they should perform equally well for the whole spectrum of facial poses. To the best of our knowledge, there are no methods meeting these requirements to satisfactory extent, and in this paper, we propose Convolutional Aggregation of Local Evidence (CALE), a Convolutional Neural Network (CNN) architecture particularly designed for addressing both of them.In particular, to remove the requirement for accurate face detection, our system firstly performs facial part detection, providing confidence scores for the location of each of the facial landmarks (local evidence). Next, these score maps along with early CNN features are aggregated by our system through joint regression in order to refine the landmarks\u2019 location. Besides playing the role of a graphical model, CNN regression is a key feature of our system, guiding the network to rely on context for predicting the location of occluded landmarks, typically encountered in very large poses. The whole system is trained end-to-end with intermediate supervision. When applied to AFLW-PIFA, the most challenging human face alignment test set to date, our method provides more than 50% gain in localisation accuracy when compared to other recently published methods for large pose face alignment. Going beyond human faces, we also demonstrate that CALE is effective in dealing with very large changes in shape and appearance, typically encountered in animal faces."},
{"paper": "Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression", "abstract": "Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied.In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks. Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss."},
{"paper": "Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face Alignment in the Wild (3DFAW) Challenge", "abstract": "This paper describes our submission to the 1st 3D Face Alignment in the Wild\n(3DFAW) Challenge. Our method builds upon the idea of convolutional part\nheatmap regression [1], extending it for 3D face alignment.Our method\ndecomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z\n(depth) estimation. At the first stage, our method estimates the X,Y\ncoordinates of the facial landmarks by producing a set of 2D heatmaps, one for\neach landmark, using convolutional part heatmap regression. Then, these\nheatmaps, alongside the input RGB image, are used as input to a very deep\nsubnetwork trained via residual learning for regressing the Z coordinate. Our\nmethod ranked 1st in the 3DFAW Challenge, surpassing the second best result by\nmore than 22%."},
{"paper": "Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources", "abstract": "Our goal is to design architectures that retain the groundbreaking\nperformance of CNNs for landmark localization and at the same time are\nlightweight, compact and suitable for applications with limited computational\nresources. To this end, we make the following contributions: (a) we are the\nfirst to study the effect of neural network binarization on localization tasks,\nnamely human pose estimation and face alignment.We exhaustively evaluate\nvarious design choices, identify performance bottlenecks, and more importantly\npropose multiple orthogonal ways to boost performance. (b) Based on our\nanalysis, we propose a novel hierarchical, parallel and multi-scale residual\narchitecture that yields large performance improvement over the standard\nbottleneck block while having the same number of parameters, thus bridging the\ngap between the original network and its binarized counterpart. (c) We perform\na large number of ablation studies that shed light on the properties and the\nperformance of the proposed block. (d) We present results for experiments on\nthe most challenging datasets for human pose estimation and face alignment,\nreporting in many cases state-of-the-art performance. Code can be downloaded\nfrom https://www.adrianbulat.com/binary-cnn-landmarks"},
{"paper": "MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices", "abstract": "Face Analysis Project on MXNetNone"},
{"task": "Temporal Action Proposal Generation", "papers": {"0": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation", "1": "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation", "2": "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer"}},
{"task": "Weakly-supervised Temporal Action Localization", "papers": {"0": "Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization"}},
{"task": "Weakly Supervised Action Localization", "papers": {"0": "Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization", "1": "Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization", "2": "Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization", "3": "Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization"}},
{"paper": "Dual Variational Generation for Low-Shot Heterogeneous Face Recognition", "abstract": "Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework.It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. The related code is available at https://github.com/BradyFU/DVG."},
{"paper": "Probabilistic Face Embeddings", "abstract": "Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations.We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system."},
{"paper": "SeqFace: Make full use of sequence information for face recognition", "abstract": "Deep convolutional neural networks (CNNs) have greatly improved the Face\nRecognition (FR) performance in recent years. Almost all CNNs in FR are trained\non the carefully labeled datasets containing plenty of identities.However,\nsuch high-quality datasets are very expensive to collect, which restricts many\nresearchers to achieve state-of-the-art performance. In this paper, we propose\na framework, called SeqFace, for learning discriminative face features. Besides\na traditional identity training dataset, the designed SeqFace can train CNNs by\nusing an additional dataset which includes a large number of face sequences\ncollected from videos. Moreover, the label smoothing regularization (LSR) and a\nnew proposed discriminative sequence agent (DSA) loss are employed to enhance\ndiscrimination power of deep face features via making full use of the sequence\ndata. Our method achieves excellent performance on Labeled Faces in the Wild\n(LFW), YouTube Faces (YTF), only with a single ResNet. The code and models are\npublicly available on-line (https://github.com/huangyangyu/SeqFace)."},
{"paper": "Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis", "abstract": "Synthesizing realistic profile faces is promising for more efficiently  training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images.To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively."},
{"paper": "Dynamic Multi-Task Learning for Face Recognition with Facial Expression", "abstract": "Benefiting from the joint learning of the multiple tasks in the deep multi-task networks, many applications have shown the promising performance comparing to single-task learning. However, the performance of multi-task learning framework is highly dependant on the relative weights of the tasks.How to assign the weight of each task is a critical issue in the multi-task learning. Instead of tuning the weights manually which is exhausted and time-consuming, in this paper we propose an approach which can dynamically adapt the weights of the tasks according to the difficulty for training the task. Specifically, the proposed method does not introduce the hyperparameters and the simple structure allows the other multi-task deep learning networks can easily realize or reproduce this method. We demonstrate our approach for face recognition with facial expression and facial expression recognition from a single input image based on a deep multi-task learning Conventional Neural Networks (CNNs). Both the theoretical analysis and the experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning method. This multi-task learning with dynamic weights also boosts of the performance on the different tasks comparing to the state-of-art methods with single-task learning."},
{"paper": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition", "abstract": "To improve the discriminative and generalization ability of lightweight network for face recognition, we propose an efficient variable group convolutional network called VarGFaceNet. Variable group convolution is introduced by VarGNet to solve the conflict between small computational cost and the unbalance of computational intensity inside a block.We employ variable group convolution to design our network which can support large scale face identification while reduce computational cost and parameters. Specifically, we use a head setting to reserve essential information at the start of the network and propose a particular embedding setting to reduce parameters of fully-connected layer for embedding. To enhance interpretation ability, we employ an equivalence of angular distillation loss to guide our lightweight network and we apply recursive knowledge distillation to relieve the discrepancy between the teacher model and the student model. The champion of deepglint-light track of LFR (2019) challenge demonstrates the effectiveness of our model and approach. Implementation of VarGFaceNet will be released at https://github.com/zma-c-137/VarGFaceNet soon."},
{"paper": "Selective Refinement Network for High Performance Face Detection", "abstract": "High performance face detection remains a very challenging problem,\nespecially when there exists many tiny faces. This paper presents a novel\nsingle-shot face detector, named Selective Refinement Network (SRN), which\nintroduces novel two-step classification and regression operations selectively\ninto an anchor-based face detector to reduce false positives and improve\nlocation accuracy simultaneously.In particular, the SRN consists of two\nmodules: the Selective Two-step Classification (STC) module and the Selective\nTwo-step Regression (STR) module. The STC aims to filter out most simple\nnegative anchors from low level detection layers to reduce the search space for\nthe subsequent classifier, while the STR is designed to coarsely adjust the\nlocations and sizes of anchors from high level detection layers to provide\nbetter initialization for the subsequent regressor. Moreover, we design a\nReceptive Field Enhancement (RFE) block to provide more diverse receptive\nfield, which helps to better capture faces in some extreme poses. As a\nconsequence, the proposed SRN detector achieves state-of-the-art performance on\nall the widely used face detection benchmarks, including AFW, PASCAL face,\nFDDB, and WIDER FACE datasets. Codes will be released to facilitate further\nstudies on the face detection problem."},
{"paper": "Accurate Face Detection for High Performance", "abstract": "Face detection has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs). Its central issue in recent years is how to improve the detection performance of tiny faces.To this end, many recent works propose some specific strategies, redesign the architecture and introduce new loss functions for tiny object detection. In this report, we start from the popular one-stage RetinaNet approach and apply some recent tricks to obtain a high performance face detector. Specifically, we apply the Intersection over Union (IoU) loss function for regression, employ the two-step classification and regression for detection, revisit the data augmentation based on data-anchor-sampling for training, utilize the max-out operation for classification and use the multi-scale testing strategy for inference. As a consequence, the proposed face detection method achieves state-of-the-art performance on the most popular and challenging face detection benchmark WIDER FACE dataset."},
{"paper": "RetinaFace: Single-stage Dense Face Localisation in the Wild", "abstract": "Face Analysis Project on MXNetNone"},
{"paper": "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)", "abstract": "This paper investigates how far a very deep neural network is from attaining\nclose to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following 5 contributions: (a) we construct, for the\nfirst time, a very strong baseline by combining a state-of-the-art architecture\nfor landmark localization with a state-of-the-art residual block, train it on a\nvery large yet synthetically expanded 2D facial landmark dataset and finally\nevaluate it on all other 2D facial landmark datasets.(b) We create a guided by\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\nall existing datasets, leading to the creation of LS3D-W, the largest and most\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\nthat, we train a neural network for 3D face alignment and evaluate it on the\nnewly introduced LS3D-W. (d) We further look into the effect of all\n\"traditional\" factors affecting face alignment performance like large pose,\ninitialization and resolution, and introduce a \"new\" one, namely the size of\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\nperformance of remarkable accuracy which is probably close to saturating the\ndatasets used. Training and testing code as well as the dataset can be\ndownloaded from https://www.adrianbulat.com/face-alignment/"},
{"paper": "Progressive Face Super-Resolution via Attention to Facial Landmark", "abstract": "Face Super-Resolution (SR) is a subfield of the SR domain that specifically targets the reconstruction of face images. The main challenge of face SR is to restore essential facial features without distortion.We propose a novel face SR method that generates photo-realistic 8x super-resolved face images with fully retained facial details. To that end, we adopt a progressive training method, which allows stable training by splitting the network into successive steps, each producing output with a progressively higher resolution. We also propose a novel facial attention loss and apply it at each step to focus on restoring facial attributes in greater details by multiplying the pixel difference and heatmap values. Lastly, we propose a compressed version of the state-of-the-art face alignment network (FAN) for landmark heatmap extraction. With the proposed FAN, we can extract the heatmaps suitable for face SR and also reduce the overall training time. Experimental results verify that our method outperforms state-of-the-art methods in both qualitative and quantitative measurements, especially in perceptual quality."},
{"paper": "DSFD: Dual Shot Face Detector", "abstract": "In this paper, we propose a novel face detection network with three novel\ncontributions that address three key aspects of face detection, including\nbetter feature learning, progressive loss design and anchor assign based data\naugmentation, respectively. First, we propose a Feature Enhance Module (FEM)\nfor enhancing the original feature maps to extend the single shot detector to\ndual shot detector.Second, we adopt Progressive Anchor Loss (PAL) computed by\ntwo different sets of anchors to effectively facilitate the features. Third, we\nuse an Improved Anchor Matching (IAM) by integrating novel anchor assign\nstrategy into data augmentation to provide better initialization for the\nregressor. Since these techniques are all related to the two-stream design, we\nname the proposed network as Dual Shot Face Detector (DSFD). Extensive\nexperiments on popular benchmarks, WIDER FACE and FDDB, demonstrate the\nsuperiority of DSFD over the state-of-the-art face detectors."},
{"paper": "Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment", "abstract": "Face Analysis Project on MXNetNone"},
{"paper": "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer", "abstract": "Dense video captioning aims to localize and describe important events in untrimmed videos. Existing methods mainly tackle this task by exploiting only visual features, while completely neglecting the audio track.Only a few prior works have utilized both modalities, yet they show poor results or demonstrate the importance on a dataset with a specific domain. In this paper, we introduce Bi-modal Transformer which generalizes the Transformer architecture for a bi-modal input. We show the effectiveness of the proposed model with audio and visual modalities on the dense video captioning task, yet the module is capable of digesting any two modalities in a sequence-to-sequence task. We also show that the pre-trained bi-modal encoder as a part of the bi-modal transformer can be used as a feature extractor for a simple proposal generation module. The performance is demonstrated on a challenging ActivityNet Captions dataset where our model achieves outstanding performance. The code is available: v-iashin.github.io/bmt"},
{"paper": "Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization", "abstract": "Weakly-supervised temporal action localization aims to detect intervals of action instances with only video-level action labels for training. A crucial challenge is to separate frames of action classes from remaining, denoted as background frames (i.e., frames not belonging to any action class).Previous methods attempt background modeling by either synthesizing pseudo background videos with static frames or introducing an auxiliary class for background. However, they overlook an essential fact that background frames could be dynamic and inconsistent. Accordingly, we cast the problem of identifying background frames as out-of-distribution detection and isolate it from conventional action classification. Beyond our base action localization network, we propose a module to estimate the probability of being background (i.e., uncertainty [20]), which allows us to learn uncertainty given only video-level labels via multiple instance learning. A background entropy loss is further designed to reject background frames by forcing them to have uniform probability distribution for action classes. Extensive experiments verify the effectiveness of our background modeling and show that our method significantly outperforms state-of-the-art methods on the standard benchmarks - THUMOS'14 and ActivityNet (1.2 and 1.3). Our code and the trained model are available at https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation."},
{"paper": "Nonlinear 3D Face Morphable Model", "abstract": "As a classic statistical model of 3D facial shape and texture, 3D Morphable\nModel (3DMM) is widely used in facial analysis, e.g., model fitting, image\nsynthesis. Conventional 3DMM is learned from a set of well-controlled 2D face\nimages with associated 3D face scans, and represented by two sets of PCA basis\nfunctions.Due to the type and amount of training data, as well as the linear\nbases, the representation power of 3DMM can be limited. To address these\nproblems, this paper proposes an innovative framework to learn a nonlinear 3DMM\nmodel from a large set of unconstrained face images, without collecting 3D face\nscans. Specifically, given a face image as input, a network encoder estimates\nthe projection, shape and texture parameters. Two decoders serve as the\nnonlinear 3DMM to map from the shape and texture parameters to the 3D shape and\ntexture, respectively. With the projection parameter, 3D shape, and texture, a\nnovel analytically-differentiable rendering layer is designed to reconstruct\nthe original input face. The entire network is end-to-end trainable with only\nweak supervision. We demonstrate the superior representation power of our\nnonlinear 3DMM over its linear counterpart, and its contribution to face\nalignment and 3D reconstruction."},
{"paper": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation", "abstract": "Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals.To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance."},
{"paper": "Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation", "abstract": "Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement.Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW, 300W, COFW, and AFLW datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign."},
{"paper": "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation", "abstract": "Temporal action proposal generation is an important yet challenging problem,\nsince temporal proposals with rich action content are indispensable for\nanalysing real-world videos with long duration and high proportion irrelevant\ncontent. This problem requires methods not only generating proposals with\nprecise temporal boundaries, but also retrieving proposals to cover truth\naction instances with high recall and high overlap using relatively fewer\nproposals.To address these difficulties, we introduce an effective proposal\ngeneration method, named Boundary-Sensitive Network (BSN), which adopts \"local\nto global\" fashion. Locally, BSN first locates temporal boundaries with high\nprobabilities, then directly combines these boundaries as proposals. Globally,\nwith Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating\nthe confidence of whether a proposal contains an action within its region. We\nconduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14,\nwhere BSN outperforms other state-of-the-art temporal action proposal\ngeneration methods with high recall and high temporal precision. Finally,\nfurther experiments demonstrate that by combining existing action classifiers,\nour method significantly improves the state-of-the-art temporal action\ndetection performance."},
{"paper": "Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization", "abstract": "Temporally localizing activities within untrimmed videos has been extensively studied in recent years. Despite recent advances, existing methods for weakly-supervised temporal activity localization struggle to recognize when an activity is not occurring.To address this issue, we propose a novel method named A2CL-PT. Two triplets of the feature space are considered in our approach: one triplet is used to learn discriminative features for each activity class, and the other one is used to distinguish the features where no activity occurs (i.e. background features) from activity-related features for each video. To further improve the performance, we build our network using two parallel branches which operate in an adversarial way: the first branch localizes the most salient activities of a video and the second one finds other supplementary activities from non-localized parts of the video. Extensive experiments performed on THUMOS14 and ActivityNet datasets demonstrate that our proposed method is effective. Specifically, the average mAP of IoU thresholds from 0.1 to 0.9 on the THUMOS14 dataset is significantly improved from 27.9% to 30.0%."},
{"task": "Face Recognition", "papers": {"0": "Data-specific Adaptive Threshold for Face Recognition and Authentication", "1": "Data-specific Adaptive Threshold for Face Recognition and Authentication", "2": "Data-specific Adaptive Threshold for Face Recognition and Authentication"}},
{"task": "Medical Image Retrieval", "papers": {}},
{"task": "Multi-Label Image Retrieval", "papers": {}},
{"task": "Image Instance Retrieval", "papers": {}},
{"task": "Text-Image Retrieval", "papers": {"0": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "1": "Deep Visual-Semantic Alignments for Generating Image Descriptions"}},
{"task": "Sketch-Based Image Retrieval", "papers": {"0": "Deep Shape Matching", "1": "Deep Shape Matching", "2": "Deep Shape Matching"}},
{"task": "Content-Based Image Retrieval", "papers": {}},
{"paper": "Learning Actor Relation Graphs for Group Activity Recognition", "abstract": "Modeling relation between actors is important for recognizing group activity\nin a multi-person scene. This paper aims at learning discriminative relation\nbetween actors efficiently using deep models.To this end, we propose to build\na flexible and efficient Actor Relation Graph (ARG) to simultaneously capture\nthe appearance and position relation between actors. Thanks to the Graph\nConvolutional Network, the connections in ARG could be automatically learned\nfrom group activity videos in an end-to-end manner, and the inference on ARG\ncould be efficiently performed with standard matrix operations. Furthermore, in\npractice, we come up with two variants to sparsify ARG for more effective\nmodeling in videos: spatially localized ARG and temporal randomized ARG. We\nperform extensive experiments on two standard group activity recognition\ndatasets: the Volleyball dataset and the Collective Activity dataset, where\nstate-of-the-art performance is achieved on both datasets. We also visualize\nthe learned actor graphs and relation features, which demonstrate that the\nproposed ARG is able to capture the discriminative relation information for\ngroup activity recognition."},
{"task": "Image Retrieval", "papers": {"0": "Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders", "1": "Combination of Multiple Global Descriptors for Image Retrieval", "2": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing", "3": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing", "4": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing", "5": "Combination of Multiple Global Descriptors for Image Retrieval", "6": "Combination of Multiple Global Descriptors for Image Retrieval", "7": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing", "8": "Combination of Multiple Global Descriptors for Image Retrieval", "9": "Retrieving Similar E-Commerce Images Using Deep Learning", "10": "MultiGrain: a unified image embedding for classes and instances", "11": "Deep Triplet Quantization", "12": "Fashion Image Retrieval with Capsule Networks", "13": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "14": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "15": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "16": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "17": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "18": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval"}},
{"task": "Depth And Camera Motion", "papers": {}},
{"task": "Depth Estimation", "papers": {"0": "Atlas: End-to-End 3D Scene Reconstruction from Posed Images", "1": "Atlas: End-to-End 3D Scene Reconstruction from Posed Images", "2": "DELTAS: Depth Estimation by Learning Triangulation And densification of Sparse points", "3": "Attention-based View Selection Networks for Light-field Disparity Estimation", "4": "Robust Learning Through Cross-Task Consistency", "5": "SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth Estimation"}},
{"task": "Stereo Depth Estimation", "papers": {"0": "Anytime Stereo Image Depth Estimation on Mobile Devices", "1": "Anytime Stereo Image Depth Estimation on Mobile Devices"}},
{"task": "Few-Shot Imitation Learning", "papers": {}},
{"task": "Few-Shot Camera-Adaptive Color Constancy", "papers": {}},
{"task": "Monocular Depth Estimation", "papers": {"0": "From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation", "1": "From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation", "2": "Feature-metric Loss for Self-supervised Learning of Depth and Egomotion", "3": "Robust Learning Through Cross-Task Consistency"}},
{"paper": "Data-specific Adaptive Threshold for Face Recognition and Authentication", "abstract": "Many face recognition systems boost the performance using deep learning\nmodels, but only a few researches go into the mechanisms for dealing with\nonline registration. Although we can obtain discriminative facial features\nthrough the state-of-the-art deep model training, how to decide the best\nthreshold for practical use remains a challenge.We develop a technique of\nadaptive threshold mechanism to improve the recognition accuracy. We also\ndesign a face recognition system along with the registering procedure to handle\nonline registration. Furthermore, we introduce a new evaluation protocol to\nbetter evaluate the performance of an algorithm for real-world scenarios. Under\nour proposed protocol, our method can achieve a 22\\% accuracy improvement on\nthe LFW dataset."},
{"task": "3D Action Recognition", "papers": {"0": "A System for Real-Time Interactive Analysis of Deep Learning Training"}},
{"paper": "Deep Shape Matching", "abstract": "We cast shape matching as metric learning with convolutional networks. We\nbreak the end-to-end process of image representation into two parts.Firstly,\nwell established efficient methods are chosen to turn the images into edge\nmaps. Secondly, the network is trained with edge maps of landmark images, which\nare automatically obtained by a structure-from-motion pipeline. The learned\nrepresentation is evaluated on a range of different tasks, providing\nimprovements on challenging cases of domain generalization, generic\nsketch-based image retrieval or its fine-grained counterpart. In contrast to\nother methods that learn a different model per task, object category, or\ndomain, we use the same network throughout all our experiments, achieving\nstate-of-the-art results in multiple benchmarks."},
{"paper": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "abstract": "We present a model that generates natural language descriptions of images and\ntheir regions. Our approach leverages datasets of images and their sentence\ndescriptions to learn about the inter-modal correspondences between language\nand visual data.Our alignment model is based on a novel combination of\nConvolutional Neural Networks over image regions, bidirectional Recurrent\nNeural Networks over sentences, and a structured objective that aligns the two\nmodalities through a multimodal embedding. We then describe a Multimodal\nRecurrent Neural Network architecture that uses the inferred alignments to\nlearn to generate novel descriptions of image regions. We demonstrate that our\nalignment model produces state of the art results in retrieval experiments on\nFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generated\ndescriptions significantly outperform retrieval baselines on both full images\nand on a new dataset of region-level annotations."},
{"paper": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "abstract": "Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments.Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks."},
{"task": "Few-Shot Relation Classification", "papers": {}},
{"task": "One-Shot Learning", "papers": {"0": "Siamese neural networks for one-shot image recognition"}},
{"task": "Few-Shot Learning", "papers": {"0": "DPGN: Distribution Propagation Graph Network for Few-shot Learning"}},
{"paper": "Fashion Image Retrieval with Capsule Networks", "abstract": "In this study, we investigate in-shop clothing retrieval performance of densely-connected Capsule Networks with dynamic routing. To achieve this, we propose Triplet-based design of Capsule Network architecture with two different feature extraction methods.In our design, Stacked-convolutional (SC) and Residual-connected (RC) blocks are used to form the input of capsule layers. Experimental results show that both of our designs outperform all variants of the baseline study, namely FashionNet, without relying on the landmark information. Moreover, when compared to the SOTA architectures on clothing retrieval, our proposed Triplet Capsule Networks achieve comparable recall rates only with half of parameters used in the SOTA architectures."},
{"paper": "Deep Triplet Quantization", "abstract": "Deep hashing establishes efficient and effective image retrieval by\nend-to-end learning of deep representations and hash codes from similarity\ndata. We present a compact coding solution, focusing on deep learning to\nquantization approach that has shown superior performance over hashing\nsolutions for similarity retrieval.We propose Deep Triplet Quantization (DTQ),\na novel approach to learning deep quantization models from the similarity\ntriplets. To enable more effective triplet training, we design a new triplet\nselection approach, Group Hard, that randomly selects hard triplets in each\nimage group. To generate compact binary codes, we further apply a triplet\nquantization with weak orthogonality during triplet training. The quantization\nloss reduces the codebook redundancy and enhances the quantizability of deep\nrepresentations through back-propagation. Extensive experiments demonstrate\nthat DTQ can generate high-quality and compact binary codes, which yields\nstate-of-the-art image retrieval performance on three benchmark datasets,\nNUS-WIDE, CIFAR-10, and MS-COCO."},
{"task": "Cross-Domain Few-Shot", "papers": {"0": "Cross-Domain Few-Shot Learning with Meta Fine-Tuning"}},
{"paper": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "abstract": "While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks.GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark."},
{"paper": "SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth Estimation", "abstract": "Monocular depth estimation is an ill-posed problem, and as such critically relies on scene priors and semantics. Due to its complexity, we propose a deep neural network model based on a semantic divide-and-conquer approach.Our model decomposes a scene into semantic segments, such as object instances and background stuff classes, and then predicts a scale and shift invariant depth map for each semantic segment in a canonical space. Semantic segments of the same category share the same depth decoder, so the global depth prediction task is decomposed into a series of category-specific ones, which are simpler to learn and easier to generalize to new scene types. Finally, our model stitches each local depth segment by predicting its scale and shift based on the global context of the image. The model is trained end-to-end using a multi-task loss for panoptic segmentation and depth prediction, and is therefore able to leverage large-scale panoptic segmentation datasets to boost its semantic understanding. We validate the effectiveness of our approach and show state-of-the-art performance on three benchmark datasets."},
{"paper": "Anytime Stereo Image Depth Estimation on Mobile Devices", "abstract": "Many applications of stereo depth estimation in robotics require the\ngeneration of accurate disparity maps in real time under significant\ncomputational constraints. Current state-of-the-art algorithms force a choice\nbetween either generating accurate mappings at a slow pace, or quickly\ngenerating inaccurate ones, and additionally these methods typically require\nfar too many parameters to be usable on power- or memory-constrained devices.Motivated by these shortcomings, we propose a novel approach for disparity\nprediction in the anytime setting. In contrast to prior work, our end-to-end\nlearned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried\nat any time to output its current best estimate. Our final model can process\n1242$ \\times $375 resolution images within a range of 10-35 FPS on an NVIDIA\nJetson TX2 module with only marginal increases in error -- using two orders of\nmagnitude fewer parameters than the most competitive baseline. The source code\nis available at https://github.com/mileyan/AnyNet ."},
{"paper": "A System for Real-Time Interactive Analysis of Deep Learning Training", "abstract": "Performing diagnosis or exploratory analysis during the training of deep learning models is challenging but often necessary for making a sequence of decisions guided by the incremental observations. Currently available systems for this purpose are limited to monitoring only the logged data that must be specified before the training process starts.Each time a new information is desired, a cycle of stop-change-restart is required in the training process. These limitations make interactive exploration and diagnosis tasks difficult, imposing long tedious iterations during the model development. We present a new system that enables users to perform interactive queries on live processes generating real-time information that can be rendered in multiple formats on multiple surfaces in the form of several desired visualizations simultaneously. To achieve this, we model various exploratory inspection and diagnostic tasks for deep learning training processes as specifications for streams using a map-reduce paradigm with which many data scientists are already familiar. Our design achieves generality and extensibility by defining composable primitives which is a fundamentally different approach than is used by currently available systems. The open source implementation of our system is available as TensorWatch project at https://github.com/microsoft/tensorwatch."},
{"paper": "Robust Learning Through Cross-Task Consistency", "abstract": "Visual perception entails solving a wide set of tasks (e.g., object detection, depth estimation, etc). The predictions made for different tasks out of one image are not independent, and therefore, are expected to be 'consistent'.We propose a flexible and fully computational framework for learning while enforcing Cross-Task Consistency (X-TAC). The proposed formulation is based on 'inference path invariance' over an arbitrary graph of prediction domains. We observe that learning with cross-task consistency leads to more accurate predictions, better generalization to out-of-distribution samples, and improved sample efficiency. This framework also leads to a powerful unsupervised quantity, called 'Consistency Energy, based on measuring the intrinsic consistency of the system. Consistency Energy well correlates with the supervised error (r=0.67), thus it can be employed as an unsupervised robustness metric as well as for detection of out-of-distribution inputs (AUC=0.99). The evaluations were performed on multiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape."},
{"paper": "Feature-metric Loss for Self-supervised Learning of Depth and Egomotion", "abstract": "Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels.In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by $\\delta_1$ for depth estimation, and significantly outperforms previous method for visual odometry."},
{"paper": "From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation", "abstract": "Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results.The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoder-decoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction. In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method."},
{"paper": "Attention-based View Selection Networks for Light-field Disparity Estimation", "abstract": "This paper introduces a novel deep network for estimating depth maps from a light field image. For utilizing the views more effectively and reducing redundancy within views, we propose a view selection module that generates an attention map indicating the importance of each view and its potential for contributing to accurate depth estimation.By exploring the symmetric property of light field views, we enforce symmetry in the attention map and further improve accuracy. With the attention map, our architecture utilizes all views more effectively and efficiently. Experiments show that the proposed method achieves state-of-the-art performance in terms of accuracy and ranks the first on a popular benchmark for disparity estimation for light field images."},
{"task": "Skeleton Based Action Recognition", "papers": {"0": "Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks", "1": "Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition", "2": "Temporal Extension Module for Skeleton-Based Action Recognition", "3": "PoTion: Pose MoTion Representation for Action Recognition", "4": "Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition", "5": "On Geometric Features for Skeleton-Based Action Recognition using Multilayer LSTM Networks", "6": "Neural Graph Matching Networks for Fewshot 3D Action Recognition", "7": "A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition", "8": "Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition", "9": "Make Skeleton-based Action Recognition Model Smaller, Faster and Better", "10": "Optimized Skeleton-based Action Recognition via Sparsified Graph Regression", "11": "Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition", "12": "Make Skeleton-based Action Recognition Model Smaller, Faster and Better", "13": "Making the Invisible Visible: Action Recognition Through Walls and Occlusions", "14": "Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks", "15": "Multigrid Predictive Filter Flow for Unsupervised Learning on Videos", "16": "View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition", "17": "Bayesian Hierarchical Dynamic Model for Human Action Recognition", "18": "View-Invariant Probabilistic Embedding for Human Pose", "19": "Relational Autoencoder for Feature Extraction", "20": "Quo Vadis, Skeleton Action Recognition ?", "21": "Quo Vadis, Skeleton Action Recognition ?"}},
{"paper": "DELTAS: Depth Estimation by Learning Triangulation And densification of Sparse points", "abstract": "Multi-view stereo (MVS) is the golden mean between the accuracy of active depth sensing and the practicality of monocular depth estimation. Cost volume based approaches employing 3D convolutional neural networks (CNNs) have considerably improved the accuracy of MVS systems.However, this accuracy comes at a high computational cost which impedes practical adoption. Distinct from cost volume approaches, we propose an efficient depth estimation approach by first (a) detecting and evaluating descriptors for interest points, then (b) learning to match and triangulate a small set of interest points, and finally (c) densifying this sparse set of 3D points using CNNs. An end-to-end network efficiently performs all three steps within a deep learning framework and trained with intermediate 2D image and 3D geometric supervision, along with depth supervision. Crucially, our first step complements pose estimation using interest point detection and descriptor learning. We demonstrate state-of-the-art results on depth estimation with lower compute for different scene lengths. Furthermore, our method generalizes to newer environments and the descriptors output by our network compare favorably to strong baselines. Code is available at https://github.com/magicleap/DELTAS"},
{"paper": "Retrieving Similar E-Commerce Images Using Deep Learning", "abstract": "In this paper, we propose a deep convolutional neural network for learning\nthe embeddings of images in order to capture the notion of visual similarity. We present a deep siamese architecture that when trained on positive and\nnegative pairs of images learn an embedding that accurately approximates the\nranking of images in order of visual similarity notion.We also implement a\nnovel loss calculation method using an angular loss metrics based on the\nproblems requirement. The final embedding of the image is combined\nrepresentation of the lower and top-level embeddings. We used fractional\ndistance matrix to calculate the distance between the learned embeddings in\nn-dimensional space. In the end, we compare our architecture with other\nexisting deep architecture and go on to demonstrate the superiority of our\nsolution in terms of image retrieval by testing the architecture on four\ndatasets. We also show how our suggested network is better than the other\ntraditional deep CNNs used for capturing fine-grained image similarities by\nlearning an optimum embedding."},
{"paper": "Siamese neural networks for one-shot image recognition", "abstract": "The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class.In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks."},
{"paper": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing", "abstract": "Diffusion is commonly used as a ranking or re-ranking method in retrieval\ntasks to achieve higher retrieval performance, and has attracted lots of\nattention in recent years. A downside to diffusion is that it performs slowly\nin comparison to the naive k-NN search, which causes a non-trivial online\ncomputational cost on large datasets.To overcome this weakness, we propose a\nnovel diffusion technique in this paper. In our work, instead of applying\ndiffusion to the query, we pre-compute the diffusion results of each element in\nthe database, making the online search a simple linear combination on top of\nthe k-NN search process. Our proposed method becomes 10~ times faster in terms\nof online search speed. Moreover, we propose to use late truncation instead of\nearly truncation in previous works to achieve better retrieval performance."},
{"paper": "Combination of Multiple Global Descriptors for Image Retrieval", "abstract": "Recent studies in image retrieval task have shown that ensembling different models and combining multiple global descriptors lead to performance improvement. However, training different models for the ensemble is not only difficult but also inefficient with respect to time and memory.In this paper, we propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner. The proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset. Moreover, we investigate the effectiveness of combining multiple global descriptors with quantitative and qualitative analysis. Our extensive experiments show that the combined descriptor outperforms a single global descriptor, as it can utilize different types of feature properties. In the benchmark evaluation, the proposed framework achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop Clothes, and Stanford Online Products on image retrieval tasks. Our model implementations and pretrained models are publicly available."},
{"paper": "DPGN: Distribution Propagation Graph Network for Few-shot Learning", "abstract": "Most graph-network-based meta-learning approaches model instance-level relation of examples. We extend this idea further to explicitly model the distribution-level relation of one example to all other examples in a 1-vs-N manner.We propose a novel approach named distribution propagation graph network (DPGN) for few-shot learning. It conveys both the distribution-level relations and instance-level relations in each few-shot learning task. To combine the distribution-level relations and instance-level relations for all examples, we construct a dual complete graph network which consists of a point graph and a distribution graph with each node standing for an example. Equipped with dual graph architecture, DPGN propagates label information from labeled examples to unlabeled examples within several update generations. In extensive experiments on few-shot learning benchmarks, DPGN outperforms state-of-the-art results by a large margin in 5% $\\sim$ 12% under supervised setting and 7% $\\sim$ 13% under semi-supervised setting. Code will be released."},
{"task": "Semantic Segmentation Of Orthoimagery", "papers": {}},
{"paper": "Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders", "abstract": "Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the problem of accurate cross-media retrieval through image-sentence matching based on word-region alignments using supervision only at the global image-sentence level.In particular, we present an approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences, i.e., image regions and words, respectively, in order to preserve the informative richness of both modalities. The proposed approach obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k. Moreover, on MS-COCO, it defeats current approaches also on the sentence retrieval task. Given our long-term interest in scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. In fact, cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way towards the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against the best eight methods in this research area. On the MS-COCO 1K test set, we obtain an improvement of 3.5% and 1.2% respectively on the image and the sentence retrieval tasks on the"},
{"paper": "Cross-Domain Few-Shot Learning with Meta Fine-Tuning", "abstract": "In this paper, we tackle the new Cross-Domain Few-Shot Learning benchmark proposed by the CVPR 2020 Challenge. To this end, we build upon state-of-the-art methods in domain adaptation and few-shot learning to create a system that can be trained to perform both tasks.Inspired by the need to create models designed to be fine-tuned, we explore the integration of transfer-learning (fine-tuning) with meta-learning algorithms, to train a network that has specific layers that are designed to be adapted at a later fine-tuning stage. To do so, we modify the episodic training process to include a first-order MAML-based meta-learning algorithm, and use a Graph Neural Network model as the subsequent meta-learning module. We find that our proposed method helps to boost accuracy significantly, especially when combined with data augmentation. In our final results, we combine the novel method with the baseline method in a simple ensemble, and achieve an average accuracy of 73.78% on the benchmark. This is a 6.51% improvement over existing benchmarks that were trained solely on miniImagenet."},
{"task": "Automated Pancreas Segmentation", "papers": {}},
{"task": "Brain Ventricle Localization And Segmentation In 3D Ultrasound Images", "papers": {}},
{"task": "Placenta Segmentation", "papers": {}},
{"task": "Pulmorary Vessel Segmentation", "papers": {}},
{"task": "Cerebrovascular Network Segmentation", "papers": {}},
{"task": "Acute Stroke Lesion Segmentation", "papers": {}},
{"paper": "MultiGrain: a unified image embedding for classes and instances", "abstract": "MultiGrain is a network architecture producing compact vector representations\nthat are suited both for image classification and particular object retrieval. It builds on a standard classification trunk.The top of the network produces\nan embedding containing coarse and fine-grained information, so that images can\nbe recognized based on the object class, particular object, or if they are\ndistorted copies. Our joint training is simple: we minimize a cross-entropy\nloss for classification and a ranking loss that determines if two images are\nidentical up to data augmentation, with no need for additional labels. A key\ncomponent of MultiGrain is a pooling layer that takes advantage of\nhigh-resolution images with a network trained at a lower resolution. When fed to a linear classifier, the learned embeddings provide\nstate-of-the-art classification accuracy. For instance, we obtain 79.4% top-1\naccuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute\nimprovement over the AutoAugment method. When compared with the cosine\nsimilarity, the same embeddings perform on par with the state-of-the-art for\nimage retrieval at moderate resolutions."},
{"paper": "Quo Vadis, Skeleton Action Recognition ?", "abstract": "In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To begin with, we benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results.To examine skeleton action recognition 'in the wild', we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. The results from benchmarking the top performers of NTU-120 on Skeletics-152 reveal the challenges and domain gap induced by actions 'in the wild'. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. Finally, as a new frontier for action recognition, we introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. It also provides an assessment of top-performing approaches across a spectrum of activity settings and via the introduced datasets, proposes new frontiers for human action recognition."},
{"paper": "Relational Autoencoder for Feature Extraction", "abstract": "Feature extraction becomes increasingly important as data grows high\ndimensional. Autoencoder as a neural network based feature extraction method\nachieves great success in generating abstract features of high dimensional\ndata.However, it fails to consider the relationships of data samples which may\naffect experimental results of using original and new features. In this paper,\nwe propose a Relation Autoencoder model considering both data features and\ntheir relationships. We also extend it to work with other major autoencoder\nmodels including Sparse Autoencoder, Denoising Autoencoder and Variational\nAutoencoder. The proposed relational autoencoder models are evaluated on a set\nof benchmark datasets and the experimental results show that considering data\nrelationships can generate more robust features which achieve lower\nconstruction loss and then lower error rate in further classification compared\nto the other variants of autoencoders."},
{"paper": "View-Invariant Probabilistic Embedding for Human Pose", "abstract": "Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views.This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code is available at https://github.com/google-research/google-research/tree/master/poem."},
{"paper": "Bayesian Hierarchical Dynamic Model for Human Action Recognition", "abstract": "Human action recognition remains as a challenging task partially due to the presence of large variations in the execution of action. To address this issue, we propose a probabilistic model called Hierarchical Dynamic Model (HDM).Leveraging on Bayesian framework, the model parameters are allowed to vary across different sequences of data, which increase the capacity of the model to adapt to intra-class variations on both spatial and temporal extent of actions. Meanwhile, the generative learning process allows the model to preserve the distinctive dynamic pattern for each action class. Through Bayesian inference, we are able to quantify the uncertainty of the classification, providing insight during the decision process. Compared to state-of-the-art methods, our method not only achieves competitive recognition performance within individual dataset but also shows better generalization capability across different datasets. Experiments conducted on data with missing values also show the robustness of the proposed method."},
{"paper": "Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (ConvNets) have recently shown promising\nperformance in many computer vision tasks, especially image-based recognition. How to effectively apply ConvNets to sequence-based data is still an open\nproblem.This paper proposes an effective yet simple method to represent\nspatio-temporal information carried in $3D$ skeleton sequences into three $2D$\nimages by encoding the joint trajectories and their dynamics into color\ndistribution in the images, referred to as Joint Trajectory Maps (JTM), and\nadopts ConvNets to learn the discriminative features for human action\nrecognition. Such an image-based representation enables us to fine-tune\nexisting ConvNets models for the classification of skeleton sequences without\ntraining the networks afresh. The three JTMs are generated in three orthogonal\nplanes and provide complimentary information to each other. The final\nrecognition is further improved through multiply score fusion of the three\nJTMs. The proposed method was evaluated on four public benchmark datasets, the\nlarge NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset\nand UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the\nstate-of-the-art results."},
{"paper": "Multigrid Predictive Filter Flow for Unsupervised Learning on Videos", "abstract": "We introduce multigrid Predictive Filter Flow (mgPFF), a framework for\nunsupervised learning on videos. The mgPFF takes as input a pair of frames and\noutputs per-pixel filters to warp one frame to the other.Compared to optical\nflow used for warping frames, mgPFF is more powerful in modeling sub-pixel\nmovement and dealing with corruption (e.g., motion blur). We develop a\nmultigrid coarse-to-fine modeling strategy that avoids the requirement of\nlearning large filters to capture large displacement. This allows us to train\nan extremely compact model (4.6MB) which operates in a progressive way over\nmultiple resolutions with shared weights. We train mgPFF on unsupervised,\nfree-form videos and show that mgPFF is able to not only estimate long-range\nflow for frame reconstruction and detect video shot transitions, but also\nreadily amendable for video object segmentation and pose tracking, where it\nsubstantially outperforms the published state-of-the-art without bells and\nwhistles. Moreover, owing to mgPFF's nature of per-pixel filter prediction, we\nhave the unique opportunity to visualize how each pixel is evolving during\nsolving these tasks, thus gaining better interpretability."},
{"paper": "View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition", "abstract": "Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in skeleton-based action recognition lies in the large view variations when capturing data.In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints in a learning based data driven manner. We design two view adaptive neural networks, i.e., VA-RNN based on RNN, and VA-CNN based on CNN. For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various viewpoints to much more consistent virtual viewpoints which largely eliminates the viewpoint influence. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the fused prediction. Extensive experimental evaluations on five challenging benchmarks demonstrate that the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches. The source code is available at https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition."},
{"paper": "Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition", "abstract": "Variations of human body skeletons may be considered as dynamic graphs, which\nare generic data representation for numerous real-world applications. In this\npaper, we propose a spatio-temporal graph convolution (STGC) approach for\nassembling the successes of local convolutional filtering and sequence learning\nability of autoregressive moving average.To encode dynamic graphs, the\nconstructed multi-scale local graph convolution filters, consisting of matrices\nof local receptive fields and signal mappings, are recursively performed on\nstructured graph data of temporal and spatial domain. The proposed model is\ngeneric and principled as it can be generalized into other dynamic models. We\ntheoretically prove the stability of STGC and provide an upper-bound of the\nsignal transformation to be learnt. Further, the proposed recursive model can\nbe stacked into a multi-layer architecture. To evaluate our model, we conduct\nextensive experiments on four benchmark skeleton-based action datasets,\nincluding the large-scale challenging NTU RGB+D. The experimental results\ndemonstrate the effectiveness of our proposed model and the improvement over\nthe state-of-the-art."},
{"paper": "Optimized Skeleton-based Action Recognition via Sparsified Graph Regression", "abstract": "With the prevalence of accessible depth sensors, dynamic human body skeletons\nhave attracted much attention as a robust modality for action recognition. Previous methods model skeletons based on RNN or CNN, which has limited\nexpressive power for irregular skeleton joints.While graph convolutional\nnetworks (GCN) have been proposed to address irregular graph-structured data,\nthe fundamental graph construction remains challenging. In this paper, we\nrepresent skeletons naturally on graphs, and propose a graph regression based\nGCN (GR-GCN) for skeleton-based action recognition, aiming to capture the\nspatio-temporal variation in the data. As the graph representation is crucial\nto graph convolution, we first propose graph regression to statistically learn\nthe underlying graph from multiple observations. In particular, we provide\nspatio-temporal modeling of skeletons and pose an optimization problem on the\ngraph structure over consecutive frames, which enforces the sparsity of the\nunderlying graph for efficient representation. The optimized graph not only\nconnects each joint to its neighboring joints in the same frame strongly or\nweakly, but also links with relevant joints in the previous and subsequent\nframes. We then feed the optimized graph into the GCN along with the\ncoordinates of the skeleton sequence for feature learning, where we deploy\nhigh-order and fast Chebyshev approximation of spectral graph convolution. Further, we provide analysis of the variation characterization by the Chebyshev\napproximation. Experimental results validate the effectiveness of the proposed\ngraph regression and show that the proposed GR-GCN achieves the\nstate-of-the-art performance on the widely used NTU RGB+D, UT-Kinect and SYSU\n3D datasets."},
{"paper": "A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition", "abstract": "Current researches of action recognition mainly focus on single-view and\nmulti-view recognition, which can hardly satisfies the requirements of\nhuman-robot interaction (HRI) applications to recognize actions from arbitrary\nviews. The lack of datasets also sets up barriers.To provide data for\narbitrary-view action recognition, we newly collect a large-scale RGB-D action\ndataset for arbitrary-view action analysis, including RGB videos, depth and\nskeleton sequences. The dataset includes action samples captured in 8 fixed\nviewpoints and varying-view sequences which covers the entire 360 degree view\nangles. In total, 118 persons are invited to act 40 action categories, and\n25,600 video samples are collected. Our dataset involves more participants,\nmore viewpoints and a large number of samples. More importantly, it is the\nfirst dataset containing the entire 360 degree varying-view sequences. The\ndataset provides sufficient data for multi-view, cross-view and arbitrary-view\naction analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to\ntackle the problem of arbitrary-view action recognition. Experiment results\nshow that the VS-CNN achieves superior performance."},
{"paper": "Neural Graph Matching Networks for Fewshot 3D Action Recognition", "abstract": "We propose Neural Graph Matching (NGM) Networks, a novel framework that can learn to recognize a previous unseen 3D action class with only a few examples. We achieve this by leveraging the inherent structure of 3D data through a graphical representation.This allows us to modularize our model and lead to strong data-efficiency in few-shot learning. More specifically, NGM Networks jointly learn a graph generator and a graph matching metric function in a end-to-end fashion to directly optimize the few-shot learning objective. We evaluate NGM on two 3D action recognition datasets, CAD-120 and PiGraphs, and show that learning to generate and match graphs both lead to significant improvement of few-shot 3D action recognition over the holistic baselines."},
{"paper": "On Geometric Features for Skeleton-Based Action Recognition using Multilayer LSTM Networks", "abstract": "RNN-based approaches have achieved outstanding performance on action recognition with skeleton inputs. Currently these methods limit their inputs to coordinates of joints and improve the accuracy mainly by extending RNN models to spatial domains in various ways.While such models explore relations between different parts directly from joint coordinates, we provide a simple universal spatial modeling method perpendicular to the RNN model enhancement. Specifically, we select a set of simple geometric features, motivated by the evolution of previous work. With experiments on a 3-layer LSTM framework, we observe that the geometric relational features based on distances between joints and selected lines outperform other features and achieve state-of-art results on four datasets. Further, we show the sparsity of input gate weights in the first LSTM layer trained by geometric features and demonstrate that utilizing joint-line distances as input require less data for training."},
{"paper": "Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition", "abstract": "Skeleton-based human action recognition has attracted great interest thanks to the easy accessibility of the human skeleton data. Recently, there is a trend of using very deep feedforward neural networks to model the 3D coordinates of joints without considering the computational efficiency.In this paper, we propose a simple yet effective semantics-guided neural network (SGN) for skeleton-based action recognition. We explicitly introduce the high level semantics of joints (joint type and frame index) into the network to enhance the feature representation capability. In addition, we exploit the relationship of joints hierarchically through two modules, i.e., a joint-level module for modeling the correlations of joints in the same frame and a framelevel module for modeling the dependencies of frames by taking the joints in the same frame as a whole. A strong baseline is proposed to facilitate the study of this field. With an order of magnitude smaller model size than most previous works, SGN achieves the state-of-the-art performance on the NTU60, NTU120, and SYSU datasets. The source code is available at https://github.com/microsoft/SGN."},
{"paper": "Temporal Extension Module for Skeleton-Based Action Recognition", "abstract": "We present a module that extends the temporal graph of a graph convolutional network (GCN) for action recognition with a sequence of skeletons. Existing methods attempt to represent a more appropriate spatial graph on an intra-frame, but disregard optimization of the temporal graph on the interframe.Concretely, these methods connect between vertices corresponding only to the same joint on the inter-frame. In this work, we focus on adding connections to neighboring multiple vertices on the inter-frame and extracting additional features based on the extended temporal graph. Our module is a simple yet effective method to extract correlated features of multiple joints in human movement. Moreover, our module aids in further performance improvements, along with other GCN methods that optimize only the spatial graph. We conduct extensive experiments on two large datasets, NTU RGB+D and Kinetics-Skeleton, and demonstrate that our module is effective for several existing models and our final model achieves state-of-the-art performance."},
{"paper": "PoTion: Pose MoTion Representation for Action Recognition", "abstract": "Most state-of-the-art methods for action recognition rely on a two-stream architecture that processes appearance and motion independently. In this paper, we claim that considering them jointly offers rich information for action recognition.We introduce a novel representation that gracefully encodes the movement of some semantic keypoints. We use the human joints as these keypoints and term our Pose moTion representation PoTion. Specifically, we first run a state-of-the-art human pose estimator and extract heatmaps for the human joints in each frame. We obtain our PoTion representation by temporally aggregating these probability maps. This is achieved by colorizing each of them depending on the relative time of the frames in the video clip and summing them. This fixed-size representation for an entire video clip is suitable to classify actions using a shallow convolutional neural network. Our experimental evaluation shows that PoTion outperforms other state-of-the-art pose representations. Furthermore, it is complementary to standard appearance and motion streams. When combining PoTion with the recent two-stream I3D approach [5], we obtain state-of-the-art performance on the JHMDB, HMDB and UCF101 datasets."},
{"paper": "Make Skeleton-based Action Recognition Model Smaller, Faster and Better", "abstract": "Although skeleton-based action recognition has achieved great success in recent years, most of the existing methods may suffer from a large model size and slow execution speed. To alleviate this issue, we analyze skeleton sequence properties to propose a Double-feature Double-motion Network (DD-Net) for skeleton-based action recognition.By using a lightweight network structure (i.e., 0.15 million parameters), DD-Net can reach a super fast speed, as 3,500 FPS on one GPU, or, 2,000 FPS on one CPU. By employing robust features, DD-Net achieves the state-of-the-art performance on our experimental datasets: SHREC (i.e., hand actions) and JHMDB (i.e., body actions). Our code will be released with this paper later."},
{"task": "Automatic Liver And Tumor Segmentation", "papers": {}},
{"paper": "Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks", "abstract": "Graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, have achieved remarkable performance for skeleton-based action recognition. However, there still exist several issues in the previous GCN-based models.First, the topology of the graph is set heuristically and fixed over all the model layers and input data. This may not be suitable for the hierarchy of the GCN model and the diversity of the data in action recognition tasks. Second, the second-order information of the skeleton data, i.e., the length and orientation of the bones, is rarely investigated, which is naturally more informative and discriminative for the human action recognition. In this work, we propose a novel multi-stream attention-enhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology in our model can be either uniformly or individually learned based on the input data in an end-to-end manner. This data-driven approach increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Besides, the proposed adaptive graph convolutional layer is further enhanced by a spatial-temporal-channel attention module, which helps the model pay more attention to important joints, frames and features. Moreover, the information of both the joints and bones, together with their motion information, are simultaneously modeled in a multi-stream framework, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin."},
{"paper": "Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition", "abstract": "One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the State-Of-The-Art (SOTA) models of this task tends to be exceedingly sophisticated and over-parameterized, where the low efficiency in model training and inference has obstructed the development in the field, especially for large-scale action datasets.In this work, we propose an efficient but strong baseline based on Graph Convolutional Network (GCN), where three main improvements are aggregated, i.e., early fused Multiple Input Branches (MIB), Residual GCN (ResGCN) with bottleneck structure and Part-wise Attention (PartAtt) block. Firstly, an MIB is designed to enrich informative skeleton features and remain compact representations at an early fusion stage. Then, inspired by the success of the ResNet architecture in Convolutional Neural Network (CNN), a ResGCN module is introduced in GCN to alleviate computational costs and reduce learning difficulties in model training while maintain the model accuracy. Finally, a PartAtt block is proposed to discover the most essential body parts over a whole action sequence and obtain more explainable representations for different skeleton action sequences. Extensive experiments on two large-scale datasets, i.e., NTU RGB+D 60 and 120, validate that the proposed baseline slightly outperforms other SOTA models and meanwhile requires much fewer parameters during training and inference procedures, e.g., at most 34 times less than DGNN, which is one of the best SOTA methods."},
{"task": "Brain Lesion Segmentation From Mri", "papers": {}},
{"task": "Infant Brain Mri Segmentation", "papers": {"0": "Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation"}},
{"task": "Skin Cancer Segmentation", "papers": {"0": "Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation", "1": "Skin Lesion Segmentation using SegNet with Binary Cross-Entropy"}},
{"task": "Volumetric Medical Image Segmentation", "papers": {"0": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"}},
{"task": "Ischemic Stroke Lesion Segmentation", "papers": {}},
{"task": "Nuclear Segmentation", "papers": {"0": "Nuclei Segmentation via a Deep Panoptic Model with Semantic Feature Fusion"}},
{"task": "COVID-19 Image Segmentation", "papers": {}},
{"task": "Pancreas Segmentation", "papers": {"0": "Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation", "1": "Attention U-Net: Learning Where to Look for the Pancreas"}},
{"task": "Electron Microscopy Image Segmentation", "papers": {"0": "Dense Transformer Networks for Brain Electron Microscopy Image Segmentation"}},
{"task": "Iris Segmentation", "papers": {"0": "Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework", "1": "Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework", "2": "Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework"}},
{"task": "Brain Image Segmentation", "papers": {"0": "Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction", "1": "Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction", "2": "Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction", "3": "Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction", "4": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation", "5": "Data augmentation using learned transformations for one-shot medical image segmentation"}},
{"task": "Liver Segmentation", "papers": {"0": "KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation"}},
{"task": "Lung Nodule Segmentation", "papers": {"0": "Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions", "1": "Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis", "2": "XLSor: A Robust and Accurate Lung Segmentor on Chest X-Rays Using Criss-Cross Attention and Customized Radiorealistic Abnormalities Generation", "3": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation", "4": "Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions"}},
{"task": "Cardiac Segmentation", "papers": {}},
{"task": "Cell Segmentation", "papers": {"0": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "1": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}},
{"task": "3D Medical Imaging Segmentation", "papers": {"0": "Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation"}},
{"task": "Brain Segmentation", "papers": {"0": "A Learning Strategy for Contrast-agnostic MRI Segmentation"}},
{"paper": "Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation", "abstract": "Precise 3D segmentation of infant brain tissues is an essential step towards\ncomprehensive volumetric studies and quantitative analysis of early brain\ndevelopement. However, computing such segmentations is very challenging,\nespecially for 6-month infant brain, due to the poor image quality, among other\ndifficulties inherent to infant brain MRI, e.g., the isointense contrast\nbetween white and gray matter and the severe partial volume effect due to small\nbrain sizes.This study investigates the problem with an ensemble of semi-dense\nfully convolutional neural networks (CNNs), which employs T1-weighted and\nT2-weighted MR images as input. We demonstrate that the ensemble agreement is\nhighly correlated with the segmentation errors. Therefore, our method provides\nmeasures that can guide local user corrections. To the best of our knowledge,\nthis work is the first ensemble of 3D CNNs for suggesting annotations within\nimages. Furthermore, inspired by the very recent success of dense networks, we\npropose a novel architecture, SemiDenseNet, which connects all convolutional\nlayers directly to the end of the network. Our architecture allows the\nefficient propagation of gradients during training, while limiting the number\nof parameters, requiring one order of magnitude less parameters than popular\nmedical image segmentation networks such as 3D U-Net. Another contribution of\nour work is the study of the impact that early or late fusions of multiple\nimage modalities might have on the performances of deep architectures. We\nreport evaluations of our method on the public data of the MICCAI iSEG-2017\nChallenge on 6-month infant brain MRI segmentation, and show very competitive\nresults among 21 teams, ranking first or second in most metrics."},
{"task": "Retinal Vessel Segmentation", "papers": {"0": "SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation", "1": "SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation", "2": "Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation", "3": "Deep Vessel Segmentation By Learning Graphical Connectivity"}},
{"task": "Brain Tumor Segmentation", "papers": {"0": "One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation", "1": "Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration", "2": "3D MRI brain tumor segmentation using autoencoder regularization", "3": "Brain Tumor Segmentation with Deep Neural Networks", "4": "One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation", "5": "Automatic Brain Tumor Segmentation using Cascaded Anisotropic Convolutional Neural Networks", "6": "One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation"}},
{"task": "Lesion Segmentation", "papers": {"0": "D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation", "1": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation", "2": "A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation", "3": "Automatic skin lesion segmentation with fully convolutional-deconvolutional networks", "4": "Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation"}},
{"paper": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve\nproblems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images\nwhile most medical data used in clinical practice consists of 3D volumes.In\nthis work we propose an approach to 3D image segmentation based on a\nvolumetric, fully convolutional, neural network. Our CNN is trained end-to-end\non MRI volumes depicting prostate, and learns to predict segmentation for the\nwhole volume at once. We introduce a novel objective function, that we optimise\nduring training, based on Dice coefficient. In this way we can deal with\nsituations where there is a strong imbalance between the number of foreground\nand background voxels. To cope with the limited number of annotated volumes\navailable for training, we augment the data applying random non-linear\ntransformations and histogram matching. We show in our experimental evaluation\nthat our approach achieves good performances on challenging test data while\nrequiring only a fraction of the processing time needed by other previous\nmethods."},
{"paper": "Skin Lesion Segmentation using SegNet with Binary Cross-Entropy", "abstract": "In this paper a simple and computationally efficient approach as per the complexity has been presented for Automatic Skin Lesion Segmentation using a Deep Learning architecture called SegNet including some additional specifications for the improvisation of the results. The secondary objective is to keep the pre/post -processing of the images minimal.The presented model is trained on limited images from the PH2 dataset which includes dermoscopic images, manually segmented. It also contains their masks, the clinical diagnosis and the identification of several dermoscopic structures, performed by professional dermatologists. The aim is to achieve a performance threshold Jaccard Index (IOU) 92% after evaluation."},
{"paper": "Nuclei Segmentation via a Deep Panoptic Model with Semantic Feature Fusion", "abstract": "Automated detection and segmentation of individual nuclei in histopathology images is important for cancer diagnosis and prognosis. Due to the high variability of nuclei appearances and numerous overlapping objects, this task still remains challenging.eep learning based semantic and instance segmentation models have been proposed to address the challenges, but these methods tend to concentrate on either the global or local features and hence still suffer from information loss. In this work, we propose a panoptic segmentation model which incorporates an auxiliary semantic segmentation branch with the instance branch \r\nto integrate global and local features. Furthermore, we design a feature map fusion mechanism in the instance branch and a new mask generator to prevent information loss. Experimental results on three different histopathology datasets demonstrate that our method outperforms the state-of-the-art nuclei segmentation methods and popular semantic and instance segmentation models by a large margin."},
{"paper": "Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation", "abstract": "Deep learning (DL) based semantic segmentation methods have been providing\nstate-of-the-art performance in the last few years. More specifically, these\ntechniques have been successfully applied to medical image classification,\nsegmentation, and detection tasks.One deep learning technique, U-Net, has\nbecome one of the most popular for these applications. In this paper, we\npropose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well\nas a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net\nmodels, which are named RU-Net and R2U-Net respectively. The proposed models\nutilize the power of U-Net, Residual Network, as well as RCNN. There are\nseveral advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature\naccumulation with recurrent residual convolutional layers ensures better\nfeature representation for segmentation tasks. Third, it allows us to design\nbetter U-Net architecture with same number of network parameters with better\nperformance for medical image segmentation. The proposed models are tested on\nthree benchmark datasets such as blood vessel segmentation in retina images,\nskin cancer segmentation, and lung lesion segmentation. The experimental\nresults show superior performance on segmentation tasks compared to equivalent\nmodels including U-Net and residual U-Net (ResU-Net)."},
{"paper": "Attention U-Net: Learning Where to Look for the Pancreas", "abstract": "We propose a novel attention gate (AG) model for medical imaging that\nautomatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an\ninput image while highlighting salient features useful for a specific task.This enables us to eliminate the necessity of using explicit external\ntissue/organ localisation modules of cascaded convolutional neural networks\n(CNNs). AGs can be easily integrated into standard CNN architectures such as\nthe U-Net model with minimal computational overhead while increasing the model\nsensitivity and prediction accuracy. The proposed Attention U-Net architecture\nis evaluated on two large CT abdominal datasets for multi-class image\nsegmentation. Experimental results show that AGs consistently improve the\nprediction performance of U-Net across different datasets and training sizes\nwhile preserving computational efficiency. The code for the proposed\narchitecture is publicly available."},
{"task": "Medical Image Segmentation", "papers": {"0": "PraNet: Parallel Reverse Attention Network for Polyp Segmentation", "1": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation", "2": "CE-Net: Context Encoder Network for 2D Medical Image Segmentation", "3": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation", "4": "Multi-scale self-guided attention for medical image segmentation", "5": "Multi-scale self-guided attention for medical image segmentation", "6": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation", "7": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation", "8": "Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions", "9": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation", "10": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation", "11": "KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation"}},
{"paper": "Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework", "abstract": "Iris segmentation and localization in non-cooperative environment is challenging due to illumination variations, long distances, moving subjects and limited user cooperation, etc. Traditional methods often suffer from poor performance when confronted with iris images captured in these conditions.Recent studies have shown that deep learning methods could achieve impressive performance on iris segmentation task. In addition, as iris is defined as an annular region between pupil and sclera, geometric constraints could be imposed to help locating the iris more accurately and improve the segmentation results. In this paper, we propose a deep multi-task learning framework, named as IrisParseNet, to exploit the inherent correlations between pupil, iris and sclera to boost up the performance of iris segmentation and localization in a unified model. In particular, IrisParseNet firstly applies a Fully Convolutional Encoder-Decoder Attention Network to simultaneously estimate pupil center, iris segmentation mask and iris inner/outer boundary. Then, an effective post-processing method is adopted for iris inner/outer circle localization.To train and evaluate the proposed method, we manually label three challenging iris datasets, namely CASIA-Iris-Distance, UBIRIS.v2, and MICHE-I, which cover various types of noises. Extensive experiments are conducted on these newly annotated datasets, and results show that our method outperforms state-of-the-art methods on various benchmarks. All the ground-truth annotations, annotation codes and evaluation protocols are publicly available at https://github.com/xiamenwcy/IrisParseNet."},
{"paper": "Dense Transformer Networks for Brain Electron Microscopy Image Segmentation", "abstract": "The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data.In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods."},
{"paper": "XLSor: A Robust and Accurate Lung Segmentor on Chest X-Rays Using Criss-Cross Attention and Customized Radiorealistic Abnormalities Generation", "abstract": "This paper proposes a novel framework for lung segmentation in chest X-rays. It consists of two key contributions, a criss-cross attention based\nsegmentation network and radiorealistic chest X-ray image synthesis (i.e. a\nsynthesized radiograph that appears anatomically realistic) for data\naugmentation.The criss-cross attention modules capture rich global contextual\ninformation in both horizontal and vertical directions for all the pixels thus\nfacilitating accurate lung segmentation. To reduce the manual annotation burden\nand to train a robust lung segmentor that can be adapted to pathological lungs\nwith hazy lung boundaries, an image-to-image translation module is employed to\nsynthesize radiorealistic abnormal CXRs from the source of normal ones for data\naugmentation. The lung masks of synthetic abnormal CXRs are propagated from the\nsegmentation results of their normal counterparts, and then serve as pseudo\nmasks for robust segmentor training. In addition, we annotate 100 CXRs with\nlung masks on a more challenging NIH Chest X-ray dataset containing both\nposterioranterior and anteroposterior views for evaluation. Extensive\nexperiments validate the robustness and effectiveness of the proposed\nframework. The code and data can be found from\nhttps://github.com/rsummers11/CADLab/tree/master/Lung_Segmentation_XLSor ."},
{"paper": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation", "abstract": "Segmentation is a fundamental task in medical image analysis. However, most existing methods focus on primary region extraction and ignore edge information, which is useful for obtaining accurate segmentation.In this paper, we propose a generic medical segmentation method, called Edge-aTtention guidance Network (ET-Net), which embeds edge-attention representations to guide the segmentation network. Specifically, an edge guidance module is utilized to learn the edge-attention representations in the early encoding layers, which are then transferred to the multi-scale decoding layers, fused using a weighted aggregation module. The experimental results on four segmentation tasks (i.e., optic disc/cup and vessel segmentation in retinal images, and lung segmentation in chest X-Ray and CT images) demonstrate that preserving edge-attention representations contributes to the final segmentation accuracy, and our proposed method outperforms current state-of-the-art segmentation methods. The source code of our method is available at https://github.com/ZzzJzzZ/ETNet."},
{"paper": "Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis", "abstract": "Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance.To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via self-supervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis."},
{"paper": "A Learning Strategy for Contrast-agnostic MRI Segmentation", "abstract": "We present a deep learning strategy that enables, for the first time, contrast-agnostic semantic segmentation of completely unpreprocessed brain MRI scans, without requiring additional training or fine-tuning for new modalities. Classical Bayesian methods address this segmentation problem with unsupervised intensity models, but require significant computational resources.In contrast, learning-based methods can be fast at test time, but are sensitive to the data available at training. Our proposed learning method, SynthSeg, leverages a set of training segmentations (no intensity images required) to generate synthetic sample images of widely varying contrasts on the fly during training. These samples are produced using the generative model of the classical Bayesian segmentation framework, with randomly sampled parameters for appearance, deformation, noise, and bias field. Because each mini-batch has a different synthetic contrast, the final network is not biased towards any MRI contrast. We comprehensively evaluate our approach on four datasets comprising over 1,000 subjects and four types of MR contrast. The results show that our approach successfully segments every contrast in the data, performing slightly better than classical Bayesian segmentation, and three orders of magnitude faster. Moreover, even within the same type of MRI contrast, our strategy generalizes significantly better across datasets, compared to training using real images. Finally, we find that synthesizing a broad range of contrasts, even if unrealistic, increases the generalization of the neural network. Our code and model are open source at https://github.com/BBillot/SynthSeg."},
{"paper": "KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation", "abstract": "Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these \"traditional\" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely.This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes the U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project our input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation- KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities like ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. The proposed method achieves a better performance as compared to all the recent methods with an additional benefit of fewer parameters and faster convergence. Additionally, we also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. The implementation of KiU-Net can be found here: https://github.com/jeya-maria-jose/KiU-Net-pytorch"},
{"paper": "Deep Vessel Segmentation By Learning Graphical Connectivity", "abstract": "We propose a novel deep-learning-based system for vessel segmentation. Existing methods using CNNs have mostly relied on local appearances learned on\nthe regular image grid, without considering the graphical structure of vessel\nshape.To address this, we incorporate a graph convolutional network into a\nunified CNN architecture, where the final segmentation is inferred by combining\nthe different types of features. The proposed method can be applied to expand\nany type of CNN-based vessel segmentation method to enhance the performance. Experiments show that the proposed method outperforms the current\nstate-of-the-art methods on two retinal image datasets as well as a coronary\nartery X-ray angiography dataset."},
{"paper": "Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions", "abstract": "In recent years, deep learning-based networks have achieved state-of-the-art performance in medical image segmentation. Among the existing networks, U-Net has been successfully applied on medical image segmentation.In this paper, we propose an extension of U-Net, Bi-directional ConvLSTM U-Net with Densely connected convolutions (BCDU-Net), for medical image segmentation, in which we take full advantages of U-Net, bi-directional ConvLSTM (BConvLSTM) and the mechanism of dense convolutions. Instead of a simple concatenation in the skip connection of U-Net, we employ BConvLSTM to combine the feature maps extracted from the corresponding encoding path and the previous decoding up-convolutional layer in a non-linear way. To strengthen feature propagation and encourage feature reuse, we use densely connected convolutions in the last convolutional layer of the encoding path. Finally, we can accelerate the convergence speed of the proposed network by employing batch normalization (BN). The proposed model is evaluated on three datasets of: retinal blood vessel segmentation, skin lesion segmentation, and lung nodule segmentation, achieving state-of-the-art performance."},
{"paper": "Automatic skin lesion segmentation with fully convolutional-deconvolutional networks", "abstract": "This paper summarizes our method and validation results for the ISBI\nChallenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part I:\nLesion SegmentationNone"},
{"paper": "Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation", "abstract": "We propose a dual pathway, 11-layers deep, three-dimensional Convolutional\nNeural Network for the challenging task of brain lesion segmentation. The\ndevised architecture is the result of an in-depth analysis of the limitations\nof current networks proposed for similar applications.To overcome the\ncomputational burden of processing 3D medical scans, we have devised an\nefficient and effective dense training scheme which joins the processing of\nadjacent image patches into one pass through the network while automatically\nadapting to the inherent class imbalance present in the data. Further, we\nanalyze the development of deeper, thus more discriminative 3D CNNs. In order\nto incorporate both local and larger contextual information, we employ a dual\npathway architecture that processes the input images at multiple scales\nsimultaneously. For post-processing of the network's soft segmentation, we use\na 3D fully connected Conditional Random Field which effectively removes false\npositives. Our pipeline is extensively evaluated on three challenging tasks of\nlesion segmentation in multi-channel MRI patient data with traumatic brain\ninjuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art\nfor all three applications, with top ranking performance on the public\nbenchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient,\nwhich allows its adoption in a variety of research and clinical settings. The\nsource code of our implementation is made publicly available."},
{"paper": "A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation", "abstract": "We propose a generalized focal loss function based on the Tversky index to\naddress the issue of data imbalance in medical image segmentation. Compared to\nthe commonly used Dice loss, our loss function achieves a better trade off\nbetween precision and recall when training on small structures such as lesions.To evaluate our loss function, we improve the attention U-Net model by\nincorporating an image pyramid to preserve contextual features. We experiment\non the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and\n21.4% of the images area and improve segmentation accuracy when compared to the\nstandard U-Net by 25.7% and 3.6%, respectively."},
{"paper": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation", "abstract": "Semantic image segmentation is the process of labeling each pixel of an image with its corresponding class. An encoder-decoder based approach, like U-Net and its variants, is a popular strategy for solving medical image segmentation tasks.To improve the performance of U-Net on various segmentation tasks, we propose a novel architecture called DoubleU-Net, which is a combination of two U-Net architectures stacked on top of each other. The first U-Net uses a pre-trained VGG-19 as the encoder, which has already learned features from ImageNet and can be transferred to another task easily. To capture more semantic information efficiently, we added another U-Net at the bottom. We also adopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information within the network. We have evaluated DoubleU-Net using four medical segmentation datasets, covering various imaging modalities such as colonoscopy, dermoscopy, and microscopy. Experiments on the MICCAI 2015 segmentation challenge, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the Lesion boundary segmentation datasets demonstrate that the DoubleU-Net outperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more accurate segmentation masks, especially in the case of the CVC-ClinicDB and MICCAI 2015 segmentation challenge datasets, which have challenging images such as smaller and flat polyps. These results show the improvement over the existing U-Net model. The encouraging results, produced on various medical image segmentation datasets, show that DoubleU-Net can be used as a strong baseline for both medical image segmentation and cross-dataset evaluation testing to measure the generalizability of Deep Learning (DL) models."},
{"paper": "D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation", "abstract": "Assessing the location and extent of lesions caused by chronic stroke is critical for medical diagnosis, surgical planning, and prognosis. In recent years, with the rapid development of 2D and 3D convolutional neural networks (CNN), the encoder-decoder structure has shown great potential in the field of medical image segmentation.However, the 2D CNN ignores the 3D information of medical images, while the 3D CNN suffers from high computational resource demands. This paper proposes a new architecture called dimension-fusion-UNet (D-UNet), which combines 2D and 3D convolution innovatively in the encoding stage. The proposed architecture achieves a better segmentation performance than 2D networks, while requiring significantly less computation time in comparison to 3D networks. Furthermore, to alleviate the data imbalance issue between positive and negative samples for the network training, we propose a new loss function called Enhance Mixing Loss (EML). This function adds a weighted focal coefficient and combines two traditional loss functions. The proposed method has been tested on the ATLAS dataset and compared to three state-of-the-art methods. The results demonstrate that the proposed method achieves the best quality performance in terms of DSC = 0.5349+0.2763 and precision = 0.6331+0.295)."},
{"paper": "Brain Tumor Segmentation with Deep Neural Networks", "abstract": "In this paper, we present a fully automatic brain tumor segmentation method\nbased on Deep Neural Networks (DNNs). The proposed networks are tailored to\nglioblastomas (both low and high grade) pictured in MR images.By their very\nnature, these tumors can appear anywhere in the brain and have almost any kind\nof shape, size, and contrast. These reasons motivate our exploration of a\nmachine learning solution that exploits a flexible, high capacity DNN while\nbeing extremely efficient. Here, we give a description of different model\nchoices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural\nNetworks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally\nused in computer vision. Our CNN exploits both local features as well as more\nglobal contextual features simultaneously. Also, different from most\ntraditional uses of CNNs, our networks use a final layer that is a\nconvolutional implementation of a fully connected layer which allows a 40 fold\nspeed up. We also describe a 2-phase training procedure that allows us to\ntackle difficulties related to the imbalance of tumor labels. Finally, we\nexplore a cascade architecture in which the output of a basic CNN is treated as\nan additional source of information for a subsequent CNN. Results reported on\nthe 2013 BRATS test dataset reveal that our architecture improves over the\ncurrently published state-of-the-art while being over 30 times faster."},
{"paper": "Automatic Brain Tumor Segmentation using Cascaded Anisotropic Convolutional Neural Networks", "abstract": "A cascade of fully convolutional neural networks is proposed to segment\nmulti-modal Magnetic Resonance (MR) images with brain tumor into background and\nthree hierarchical regions: whole tumor, tumor core and enhancing tumor core. The cascade is designed to decompose the multi-class segmentation problem into\na sequence of three binary segmentation problems according to the subregion\nhierarchy.The whole tumor is segmented in the first step and the bounding box\nof the result is used for the tumor core segmentation in the second step. The\nenhancing tumor core is then segmented based on the bounding box of the tumor\ncore segmentation result. Our networks consist of multiple layers of\nanisotropic and dilated convolution filters, and they are combined with\nmulti-view fusion to reduce false positives. Residual connections and\nmulti-scale predictions are employed in these networks to boost the\nsegmentation performance. Experiments with BraTS 2017 validation set show that\nthe proposed method achieved average Dice scores of 0.7859, 0.9050, 0.8378 for\nenhancing tumor core, whole tumor and tumor core, respectively. The\ncorresponding values for BraTS 2017 testing set were 0.7831, 0.8739, and\n0.7748, respectively."},
{"paper": "3D MRI brain tumor segmentation using autoencoder regularization", "abstract": "Automated segmentation of brain tumors from 3D magnetic resonance images\n(MRIs) is necessary for the diagnosis, monitoring, and treatment planning of\nthe disease. Manual delineation practices require anatomical knowledge, are\nexpensive, time consuming and can be inaccurate due to human error.Here, we\ndescribe a semantic segmentation network for tumor subregion segmentation from\n3D MRIs based on encoder-decoder architecture. Due to a limited training\ndataset size, a variational auto-encoder branch is added to reconstruct the\ninput image itself in order to regularize the shared decoder and impose\nadditional constraints on its layers. The current approach won 1st place in the\nBraTS 2018 challenge."},
{"paper": "Multi-scale self-guided attention for medical image segmentation", "abstract": "Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation, standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder architectures, leads to a redundant use of information, where similar low-level features are extracted multiple times at multiple scales.Second, long-range feature dependencies are not efficiently modeled, resulting in non-optimal discriminative feature representations associated with each semantic class. In this paper we attempt to overcome these limitations with the proposed architecture, by capturing richer contextual dependencies based on the use of guided self-attention mechanisms. This approach is able to integrate local features with their corresponding global dependencies, as well as highlight interdependent channel maps in an adaptive manner. Further, the additional loss between different modules guides the attention mechanisms to neglect irrelevant information and focus on more discriminant regions of the image by emphasizing relevant feature associations. We evaluate the proposed model in the context of semantic segmentation on three different datasets: abdominal organs, cardiovascular structures and brain tumors. A series of ablation experiments support the importance of these attention modules in the proposed architecture. In addition, compared to other state-of-the-art segmentation networks our model yields better segmentation performance, increasing the accuracy of the predictions while reducing the standard deviation. This demonstrates the efficiency of our approach to generate precise and reliable automatic segmentations of medical images. Our code is made publicly available at https://github.com/sinAshish/Multi-Scale-Attention"},
{"paper": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently.The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."},
{"paper": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation", "abstract": "Recently, dense connections have attracted substantial attention in computer\nvision because they facilitate gradient flow and implicit deep supervision\nduring training. Particularly, DenseNet, which connects each layer to every\nother layer in a feed-forward fashion, has shown impressive performances in\nnatural image classification tasks.We propose HyperDenseNet, a 3D fully\nconvolutional neural network that extends the definition of dense connectivity\nto multi-modal segmentation problems. Each imaging modality has a path, and\ndense connections occur not only between the pairs of layers within the same\npath, but also between those across different paths. This contrasts with the\nexisting multi-modal CNN approaches, in which modeling several modalities\nrelies entirely on a single joint layer (or level of abstraction) for fusion,\ntypically either at the input or at the output of the network. Therefore, the\nproposed network has total freedom to learn more complex combinations between\nthe modalities, within and in-between all the levels of abstraction, which\nincreases significantly the learning representation. We report extensive\nevaluations over two different and highly competitive multi-modal brain tissue\nsegmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing\non 6-month infant data and the latter on adult images. HyperDenseNet yielded\nsignificant improvements over many state-of-the-art segmentation networks,\nranking at the top on both benchmarks. We further provide a comprehensive\nexperimental analysis of features re-use, which confirms the importance of\nhyper-dense connections in multi-modal representation learning. Our code is\npublicly available at https://www.github.com/josedolz/HyperDenseNet."},
{"paper": "PraNet: Parallel Reverse Attention Network for Polyp Segmentation", "abstract": "Colonoscopy is an effective technique for detecting colorectal polyps, which are highly related to colorectal cancer. In clinical practice, segmenting polyps from colonoscopy images is of great importance since it provides valuable information for diagnosis and surgery.However, accurate polyp segmentation is a challenging task, for two major reasons: (i) the same type of polyps has a diversity of size, color and texture; and (ii) the boundary between a polyp and its surrounding mucosa is not sharp. To address these challenges, we propose a parallel reverse attention network (PraNet) for accurate polyp segmentation in colonoscopy images. Specifically, we first aggregate the features in high-level layers using a parallel partial decoder (PPD). Based on the combined feature, we then generate a global map as the initial guidance area for the following components. In addition, we mine the boundary cues using a reverse attention (RA) module, which is able to establish the relationship between areas and boundary cues. Thanks to the recurrent cooperation mechanism between areas and boundaries, our PraNet is capable of calibrating any misaligned predictions, improving the segmentation accuracy. Quantitative and qualitative evaluations on five challenging datasets across six metrics show that our PraNet improves the segmentation accuracy significantly, and presents a number of advantages in terms of generalizability, and real-time segmentation efficiency."},
{"paper": "Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation", "abstract": "Accurate and automatic organ segmentation from 3D radiological scans is an\nimportant yet challenging problem for medical image analysis. Specifically, the\npancreas demonstrates very high inter-patient anatomical variability in both\nits shape and volume.In this paper, we present an automated system using 3D\ncomputed tomography (CT) volumes via a two-stage cascaded approach: pancreas\nlocalization and segmentation. For the first step, we localize the pancreas\nfrom the entire 3D CT scan, providing a reliable bounding box for the more\nrefined segmentation step. We introduce a fully deep-learning approach, based\non an efficient application of holistically-nested convolutional networks\n(HNNs) on the three orthogonal axial, sagittal, and coronal views. The\nresulting HNN per-pixel probability maps are then fused using pooling to\nreliably produce a 3D bounding box of the pancreas that maximizes the recall. We show that our introduced localizer compares favorably to both a conventional\nnon-deep-learning method and a recent hybrid approach based on spatial\naggregation of superpixels using random forest classification. The second,\nsegmentation, phase operates within the computed bounding box and integrates\nsemantic mid-level cues of deeply-learned organ interior and boundary maps,\nobtained by two additional and separate realizations of HNNs. By integrating\nthese two mid-level cues, our method is capable of generating\nboundary-preserving pixel-wise class label maps that result in the final\npancreas segmentation. Quantitative evaluation is performed on a publicly\navailable dataset of 82 patient CT scans using 4-fold cross-validation (CV). We\nachieve a Dice similarity coefficient (DSC) of 81.27+/-6.27% in validation,\nwhich significantly outperforms previous state-of-the art methods that report\nDSCs of 71.80+/-10.70% and 78.01+/-8.20%, respectively, using the same dataset."},
{"paper": "One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation", "abstract": "Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation.Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws, we propose a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks and design a cross-task guided attention (CGA) module which can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net."},
{"paper": "Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration", "abstract": "Medical images are naturally associated with rich semantics about the human anatomy, reflected in an abundance of recurring anatomical patterns, offering unique potential to foster deep semantic representation learning and yield semantically more powerful models for different medical applications. But how exactly such strong yet free semantics embedded in medical images can be harnessed for self-supervised learning remains largely unexplored.To this end, we train deep models to learn semantically enriched visual representation by self-discovery, self-classification, and self-restoration of the anatomy underneath medical images, resulting in a semantics-enriched, general-purpose, pre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis with all the publicly-available pre-trained models, by either self-supervision or fully supervision, on the six distinct target tasks, covering both classification and segmentation in various medical modalities (i.e.,CT, MRI, and X-ray). Our extensive experiments demonstrate that Semantic Genesis significantly exceeds all of its 3D counterparts as well as the de facto ImageNet-based transfer learning in 2D. This performance is attributed to our novel self-supervised learning framework, encouraging deep models to learn compelling semantic representation from abundant anatomical patterns resulting from consistent anatomies embedded in medical images. Code and pre-trained Semantic Genesis are available at https://github.com/JLiangLab/SemanticGenesis ."},
{"paper": "CE-Net: Context Encoder Network for 2D Medical Image Segmentation", "abstract": "Medical image segmentation is an important step in medical image analysis. With the rapid development of convolutional neural network in image processing,\ndeep learning has been used for medical image segmentation, such as optic disc\nsegmentation, blood vessel detection, lung segmentation, cell segmentation,\netc.Previously, U-net based approaches have been proposed. However, the\nconsecutive pooling and strided convolutional operations lead to the loss of\nsome spatial information. In this paper, we propose a context encoder network\n(referred to as CE-Net) to capture more high-level information and preserve\nspatial information for 2D medical image segmentation. CE-Net mainly contains\nthree major components: a feature encoder module, a context extractor and a\nfeature decoder module. We use pretrained ResNet block as the fixed feature\nextractor. The context extractor module is formed by a newly proposed dense\natrous convolution (DAC) block and residual multi-kernel pooling (RMP) block. We applied the proposed CE-Net to different 2D medical image segmentation\ntasks. Comprehensive results show that the proposed method outperforms the\noriginal U-Net method and other state-of-the-art methods for optic disc\nsegmentation, vessel detection, lung segmentation, cell contour segmentation\nand retinal optical coherence tomography layer segmentation."},
{"paper": "SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation", "abstract": "The precise segmentation of retinal blood vessels is of great significance for early diagnosis of eye-related diseases such as diabetes and hypertension. In this work, we propose a lightweight network named Spatial Attention U-Net (SA-UNet) that does not require thousands of annotated training samples and can be utilized in a data augmentation manner to use the available annotated samples more efficiently.SA-UNet introduces a spatial attention module which infers the attention map along the spatial dimension, and multiplies the attention map by the input feature map for adaptive feature refinement. In addition, the proposed network employs structured dropout convolutional blocks instead of the original convolutional blocks of U-Net to prevent the network from overfitting. We evaluate SA-UNet based on two benchmark retinal datasets: the Vascular Extraction (DRIVE) dataset and the Child Heart and Health Study (CHASE_DB1) dataset. The results show that the proposed SA-UNet achieves state-of-the-art performance on both datasets.The implementation and the trained networks are available on Github1."},
{"paper": "Data augmentation using learned transformations for one-shot medical image segmentation", "abstract": "Image segmentation is an important task in many medical applications. Methods\nbased on convolutional neural networks attain state-of-the-art accuracy;\nhowever, they typically rely on supervised training with large labeled\ndatasets.Labeling medical images requires significant expertise and time, and\ntypical hand-tuned approaches for data augmentation fail to capture the complex\nvariations in such images. We present an automated data augmentation method for synthesizing labeled\nmedical images. We demonstrate our method on the task of segmenting magnetic\nresonance imaging (MRI) brain scans. Our method requires only a single\nsegmented scan, and leverages other unlabeled scans in a semi-supervised\napproach. We learn a model of transformations from the images, and use the\nmodel along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an\nintensity change, enabling the synthesis of complex effects such as variations\nin anatomy and image acquisition procedures. We show that training a supervised\nsegmenter with these new examples provides significant improvements over\nstate-of-the-art methods for one-shot biomedical image segmentation. Our code\nis available at https://github.com/xamyzhao/brainstorm."},
{"paper": "Autonomous Human Activity Classification from Ego-vision Camera and Accelerometer Data", "abstract": "There has been significant amount of research work on human activity classification relying either on Inertial Measurement Unit (IMU) data or data from static cameras providing a third-person view. Using only IMU data limits the variety and complexity of the activities that can be detected.For instance, the sitting activity can be detected by IMU data, but it cannot be determined whether the subject has sat on a chair or a sofa, or where the subject is. To perform fine-grained activity classification from egocentric videos, and to distinguish between activities that cannot be differentiated by only IMU data, we present an autonomous and robust method using data from both ego-vision cameras and IMUs. In contrast to convolutional neural network-based approaches, we propose to employ capsule networks to obtain features from egocentric video data. Moreover, Convolutional Long Short Term Memory framework is employed both on egocentric videos and IMU data to capture temporal aspect of actions. We also propose a genetic algorithm-based approach to autonomously and systematically set various network parameters, rather than using manual settings. Experiments have been performed to perform 9- and 26-label activity classification, and the proposed method, using autonomously set network parameters, has provided very promising results, achieving overall accuracies of 86.6\\% and 77.2\\%, respectively. The proposed approach combining both modalities also provides increased accuracy compared to using only egovision data and only IMU data."},
{"paper": "Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation", "abstract": "We aim at segmenting small organs (e.g., the pancreas) from abdominal CT\nscans. As the target often occupies a relatively small region in the input\nimage, deep neural networks can be easily confused by the complex and variable\nbackground.To alleviate this, researchers proposed a coarse-to-fine approach,\nwhich used prediction from the first (coarse) stage to indicate a smaller input\nregion for the second (fine) stage. Despite its effectiveness, this algorithm\ndealt with two stages individually, which lacked optimizing a global energy\nfunction, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations,\nand that the fine stage sometimes produced even lower segmentation accuracy\nthan the coarse stage. This paper presents a Recurrent Saliency Transformation Network. The key\ninnovation is a saliency transformation module, which repeatedly converts the\nsegmentation probability map from the previous iteration as spatial weights and\napplies these weights to the current iteration. This brings us two-fold\nbenefits. In training, it allows joint optimization over the deep networks\ndealing with different input scales. In testing, it propagates multi-stage\nvisual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the\nstate-of-the-art accuracy, which outperforms the previous best by an average of\nover 2%. Much higher accuracies are also reported on several small organs in a\nlarger dataset collected by ourselves. In addition, our approach enjoys better\nconvergence properties, making it more efficient and reliable in practice."},
{"paper": "Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos", "abstract": "Single modality action recognition on RGB or depth sequences has been\nextensively explored recently. It is generally accepted that each of these two\nmodalities has different strengths and limitations for the task of action\nrecognition.Therefore, analysis of the RGB+D videos can help us to better\nstudy the complementary properties of these two types of modalities and achieve\nhigher levels of performance. In this paper, we propose a new deep autoencoder\nbased shared-specific feature factorization network to separate input\nmultimodal signals into a hierarchy of components. Further, based on the\nstructure of the features, a structured sparsity learning machine is proposed\nwhich utilizes mixed norms to apply regularization within components and group\nselection between them for better classification performance. Our experimental\nresults show the effectiveness of our cross-modality feature analysis framework\nby achieving state-of-the-art accuracy for action classification on five\nchallenging benchmark datasets."},
{"paper": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation", "abstract": "The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks.To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects -- an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus."},
{"paper": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos.We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time."},
{"paper": "HAMLET: A Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm", "abstract": "To fluently collaborate with people, robots need the ability to recognize human activities accurately. Although modern robots are equipped with various sensors, robust human activity recognition (HAR) still remains a challenging task for robots due to difficulties related to multimodal data fusion.To address these challenges, in this work, we introduce a deep neural network-based multimodal HAR algorithm, HAMLET. HAMLET incorporates a hierarchical architecture, where the lower layer encodes spatio-temporal features from unimodal data by adopting a multi-head self-attention mechanism. We develop a novel multimodal attention mechanism for disentangling and fusing the salient unimodal features to compute the multimodal features in the upper layer. Finally, multimodal features are used in a fully connect neural-network to recognize human activities. We evaluated our algorithm by comparing its performance to several state-of-the-art activity recognition algorithms on three human activity datasets. The results suggest that HAMLET outperformed all other evaluated baselines across all datasets and metrics tested, with the highest top-1 accuracy of 95.12% and 97.45% on the UTD-MHAD [1] and the UT-Kinect [2] datasets respectively, and F1-score of 81.52% on the UCSD-MIT [3] dataset. We further visualize the unimodal and multimodal attention maps, which provide us with a tool to interpret the impact of attention mechanisms concerning HAR."},
{"paper": "Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction", "abstract": "We present a method combining affinity prediction with region agglomeration, which improves significantly upon the state of the art of neuron segmentation from electron microscopy (EM) in accuracy and scalability. Our method consists of a 3D U-NET, trained to predict affinities between voxels, followed by iterative region agglomeration.We train using a structured loss based on MALIS, encouraging topologically correct segmentations obtained from affinity thresholding. Our extension consists of two parts: First, we present a quasi-linear method to compute the loss gradient, improving over the original quadratic algorithm. Second, we compute the gradient in two separate passes to avoid spurious gradient contributions in early training stages. Our predictions are accurate enough that simple learning-free percentile-based agglomeration outperforms more involved methods used earlier on inferior predictions. We present results on three diverse EM datasets, achieving relative improvements over previous results of 27%, 15%, and 250%. Our findings suggest that a single method can be applied to both nearly isotropic block-face EM data and anisotropic serial sectioned EM data. The runtime of our method scales linearly with the size of the volume and achieves a throughput of about 2.6 seconds per megavoxel, qualifying our method for the processing of very large datasets."},
{"subfield": "Domain Adaptation", "tasks": {"0": "Image Generation", "1": "Image-to-Image Translation", "2": "Image Inpainting", "3": "Conditional Image Generation", "4": "Face Generation", "5": "Text-to-Image Generation", "6": "Pose Transfer", "7": "Facial Inpainting", "8": "Image Interpolation", "9": "Layout-to-Image Generation", "10": "Pose-Guided Image Generation", "11": "User Constrained Thumbnail Generation", "12": "Handwritten Word Generation", "13": "person reposing"}},
{"paper": "Can a simple approach identify complex nurse care activity?", "abstract": "For the last two decades, more and more complex methods have been developed to identify human activities using various types of sensors, e.g., data from motion capture, accelerometer, and gyroscopes sensors. To date, most of the researches mainly focus on identifying simple human activities, e.g., walking, eating, and running.However, many of our daily life activities are usually more complex than those. To instigate research in complex activity recognition, the \"Nurse Care Activity Recognition Challenge\" [1] is initiated where six nurse activities are to be identified based on location, air pressure, motion capture, and accelerometer data. Our team, \"IITDU\", investigates the use of simple methods for this purpose. We first extract features from the sensor data and use one of the simplest classifiers, namely K-Nearest Neighbors (KNN). Experiment using an ensemble of KNN classifiers demonstrates that it is possible to achieve approximately 87% accuracy on 10-fold cross-validation and 66% accuracy on leave-one-subject-out cross-validation."},
{"subfield": "Decision Making", "tasks": {"0": "Program Repair", "1": "Fault localization", "2": "Variable misuse", "3": "Function-docstring mismatch", "4": "Wrong binary operator", "5": "Swapped operands", "6": "Exception type"}},
{"subfield": "Object Detection", "tasks": {"0": "Object Detection", "1": "3D Object Detection", "2": "RGB Salient Object Detection", "3": "Real-Time Object Detection", "4": "RGB-D Salient Object Detection", "5": "Video Object Detection", "6": "Weakly Supervised Object Detection", "7": "Few-Shot Object Detection", "8": "Object Proposal Generation", "9": "Object Detection In Aerial Images", "10": "Small Object Detection", "11": "Video Salient Object Detection", "12": "Dense Object Detection", "13": "Head Detection", "14": "Robust Object Detection", "15": "Camouflaged Object Segmentation", "16": "One-Shot Object Detection", "17": "Zero-Shot Object Detection", "18": "Co-Salient Object Detection", "19": "Medical Object Detection", "20": "Object Skeleton Detection", "21": "Semantic Part Detection", "22": "Multiple Affordance Detection", "23": "3D Object Detection From Monocular Images", "24": "Fish Detection", "25": "Object Detection In Indoor Scenes"}},
{"subfield": "COVID-19 Diagnosis", "tasks": {"0": "Medical X-Ray Image Segmentation", "1": "Low-Dose X-Ray Ct Reconstruction", "2": "Bone Suppression From Dual Energy Chest X-Rays", "3": "Joint Vertebrae Identification And Localization In Spinal Ct Images", "4": "Finding Pulmonary Nodules In Large-Scale Ct Images", "5": "Mapping Of Lung Nodules In Low-Dose Ct Images", "6": "Cbct Artifact Reduction", "7": "X-Ray"}},
{"subfield": "Electrocardiography (ECG)", "tasks": {"0": "Sleep Stage Detection", "1": "Spindle Detection", "2": "Sleep Micro-event detection", "3": "K-complex detection", "4": "Sleep Quality", "5": "Sleep Arousal Detection", "6": "Sleep apnea detection", "7": "Sleep Quality Prediction", "8": "Multimodal Sleep Stage Detection"}},
{"subfield": "Image Classification", "tasks": {"0": "Image Classification", "1": "Few-Shot Image Classification", "2": "Fine-Grained Image Classification", "3": "Semi-Supervised Image Classification", "4": "Small Data Image Classification", "5": "Hyperspectral Image Classification", "6": "Self-Supervised Image Classification", "7": "Learning with noisy labels", "8": "Sequential Image Classification", "9": "Genre classification", "10": "Unsupervised Image Classification", "11": "Sparse Representation-based Classification", "12": "Document Image Classification", "13": "Satellite Image Classification", "14": "Photo geolocation estimation", "15": "Superpixel Image Classification", "16": "Classification Consistency", "17": "Artistic style classification", "18": "Artist classification"}},
{"paper": "Interpretable 3D Human Action Analysis with Temporal Convolutional Networks", "abstract": "The discriminative power of modern deep learning models for 3D human action\nrecognition is growing ever so potent. In conjunction with the recent\nresurgence of 3D human action representation with 3D skeletons, the quality and\nthe pace of recent progress have been significant.However, the inner workings\nof state-of-the-art learning based methods in 3D human action recognition still\nremain mostly black-box. In this work, we propose to use a new class of models\nknown as Temporal Convolutional Neural Networks (TCN) for 3D human action\nrecognition. Compared to popular LSTM-based Recurrent Neural Network models,\ngiven interpretable input such as 3D skeletons, TCN provides us a way to\nexplicitly learn readily interpretable spatio-temporal representations for 3D\nhuman action recognition. We provide our strategy in re-designing the TCN with\ninterpretability in mind and how such characteristics of the model is leveraged\nto construct a powerful 3D activity recognition method. Through this work, we\nwish to take a step towards a spatio-temporal model that is easier to\nunderstand, explain and interpret. The resulting model, Res-TCN, achieves\nstate-of-the-art results on the largest 3D human action recognition dataset,\nNTU-RGBD."},
{"subfield": "Drug Discovery", "tasks": {"0": "Medical Diagnosis", "1": "Retinal OCT Disease Classification", "2": "Thoracic Disease Classification", "3": "Blood Cell Count", "4": "CBC TEST", "5": "Predicting Drug-Induced Laboratory Test Effects"}},
{"subfield": "3D Action Recognition", "tasks": {"0": "Mortality Prediction", "1": "Electrocardiography (ECG)", "2": "Arrhythmia Detection", "3": "ECG Classification", "4": "Heart Rate Variability", "5": "Heartbeat Classification", "6": "Myocardial infarction detection", "7": "Ventricular fibrillation detection", "8": "ECG Denoising", "9": "Congestive Heart Failure detection", "10": "QRS Complex Detection"}},
{"subfield": "EEG", "tasks": {"0": "EEG", "1": "EEG Artifact Removal", "2": "Attention Score Prediction", "3": "Noise Level Prediction", "4": "Semanticity prediction", "5": "LWR Classification", "6": "EEG Denoising"}},
{"subfield": "3D", "tasks": {"0": "3D Reconstruction", "1": "3D Pose Estimation", "2": "3D Shape Reconstruction", "3": "3D Shape Representation", "4": "No-Reference Image Quality Assessment", "5": "3D Object Classification", "6": "3D Shape Classification", "7": "Neural Rendering", "8": "3D FACE MODELING", "9": "3D Shape Generation", "10": "3D Scene Reconstruction", "11": "3D Point Cloud Matching", "12": "FPS Games", "13": "3D Depth Estimation", "14": "Talking Head Generation", "15": "Face Reenactment", "16": "3D Object Retrieval", "17": "3D Shape Recognition", "18": "3D Shape Reconstruction From A Single 2D Image", "19": "3D Volumetric Reconstruction", "20": "Classify 3D Point Clouds", "21": "Generating 3D Point Clouds", "22": "3D Feature Matching", "23": "3D Geometry Perception", "24": "3D Plane Detection", "25": "3D Surface Generation", "26": "Point Set Upsampling", "27": "3D Object Detection From Monocular Images", "28": "Underwater 3D Scene Reconstruction", "29": "Multi-View 3D Shape Retrieval", "30": "3D"}},
{"subfield": "Medical Image Segmentation", "tasks": {"0": "Medical Image Segmentation", "1": "Lesion Segmentation", "2": "Brain Tumor Segmentation", "3": "Brain Segmentation", "4": "Retinal Vessel Segmentation", "5": "3D Medical Imaging Segmentation", "6": "Cell Segmentation", "7": "Cardiac Segmentation", "8": "Liver Segmentation", "9": "Lung Nodule Segmentation", "10": "Brain Image Segmentation", "11": "Pancreas Segmentation", "12": "Iris Segmentation", "13": "COVID-19 Image Segmentation", "14": "Electron Microscopy Image Segmentation", "15": "Nuclear Segmentation", "16": "Volumetric Medical Image Segmentation", "17": "Skin Cancer Segmentation", "18": "Infant Brain Mri Segmentation", "19": "Brain Lesion Segmentation From Mri", "20": "Ischemic Stroke Lesion Segmentation", "21": "Automatic Liver And Tumor Segmentation", "22": "Acute Stroke Lesion Segmentation", "23": "Cerebrovascular Network Segmentation", "24": "Placenta Segmentation", "25": "Pulmorary Vessel Segmentation", "26": "Automated Pancreas Segmentation", "27": "Brain Ventricle Localization And Segmentation In 3D Ultrasound Images", "28": "Semantic Segmentation Of Orthoimagery"}},
{"subfield": "Word Embeddings", "tasks": {"0": "Mortality Prediction", "1": "Electrocardiography (ECG)", "2": "Arrhythmia Detection", "3": "ECG Classification", "4": "Heart Rate Variability", "5": "Heartbeat Classification", "6": "Myocardial infarction detection", "7": "Ventricular fibrillation detection", "8": "ECG Denoising", "9": "Congestive Heart Failure detection", "10": "QRS Complex Detection"}},
{"subfield": "3D Absolute Human Pose Estimation", "tasks": {"0": "Breast Cancer Detection", "1": "Skin Cancer Classification", "2": "Lung Cancer Diagnosis", "3": "Breast Cancer Histology Image Classification", "4": "Prediction Of Cancer Cell Line Sensitivity", "5": "Classification Of Breast Cancer Histology Images", "6": "Oral Cancer Classification", "7": "Discovery Of Integrative Cancer Subtypes", "8": "Colon Cancer Detection In Confocal Laser Microscopy Images", "9": "Cancer"}},
{"subfield": "Domain Adaptation", "tasks": {"0": "EEG", "1": "EEG Artifact Removal", "2": "Attention Score Prediction", "3": "Noise Level Prediction", "4": "Semanticity prediction", "5": "LWR Classification", "6": "EEG Denoising"}},
{"subfield": "Clustering", "tasks": {"0": "Anomaly Detection", "1": "Unsupervised Anomaly Detection", "2": "Anomaly Detection In Surveillance Videos", "3": "Abnormal Event Detection In Video", "4": "Group Anomaly Detection", "5": "Anomaly Detection in Edge Streams", "6": "Unsupervised Anomaly Detection In Sound"}},
{"subfield": "Transfer Learning", "tasks": {"0": "Few-Shot Learning", "1": "Few-Shot Image Classification", "2": "One-Shot Learning", "3": "Cross-Domain Few-Shot", "4": "Few-Shot Relation Classification", "5": "Few-Shot Imitation Learning", "6": "Few-Shot Camera-Adaptive Color Constancy"}},
{"subfield": "Text Summarization", "tasks": {"0": "Dialogue Generation", "1": "Dialogue State Tracking", "2": "Task-Oriented Dialogue Systems", "3": "Visual Dialog", "4": "Goal-Oriented Dialog", "5": "Dialogue Management", "6": "Dialogue Understanding", "7": "Short-Text Conversation", "8": "Goal-Oriented Dialogue Systems", "9": "Dialogue Act Classification", "10": "Task-Completion Dialogue Policy Learning", "11": "Dialogue Interpretation", "12": "Dialog Learning", "13": "Dialogue"}},
{"subfield": "Named Entity Recognition", "tasks": {"0": "Relation Extraction", "1": "Relation Classification", "2": "Joint Entity and Relation Extraction", "3": "Relationship Extraction (Distant Supervised)", "4": "Dialog Relation Extraction", "5": "Multi-Labeled Relation Extraction", "6": "Relation Mention Extraction"}},
{"subfield": "Semantic Segmentation", "tasks": {"0": "Semantic Segmentation", "1": "Tumor Segmentation", "2": "Real-Time Semantic Segmentation", "3": "Scene Segmentation", "4": "3D Semantic Segmentation", "5": "Panoptic Segmentation", "6": "Weakly-Supervised Semantic Segmentation", "7": "3D Part Segmentation", "8": "Semi-Supervised Semantic Segmentation", "9": "One-Shot Segmentation", "10": "indoor scene understanding", "11": "Unsupervised Semantic Segmentation", "12": "4D Spatio Temporal Semantic Segmentation", "13": "Room Layout Estimation", "14": "Spinal Cord Gray Matter - Segmentation", "15": "Attentive segmentation networks", "16": "UNET Segmentation", "17": "Histopathological Segmentation", "18": "Road Segementation"}},
{"task": "Multimodal Sleep Stage Detection", "papers": {"0": "Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning", "1": "Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning", "2": "Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning", "3": "Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning"}},
{"task": "person reposing", "papers": {}},
{"task": "Object Detection In Indoor Scenes", "papers": {}},
{"task": "QRS Complex Detection", "papers": {"0": "Impact of ECG Dataset Diversity on Generalization of CNN Model for Detecting QRS Complex", "1": "Impact of ECG Dataset Diversity on Generalization of CNN Model for Detecting QRS Complex", "2": "Impact of ECG Dataset Diversity on Generalization of CNN Model for Detecting QRS Complex"}},
{"task": "Artist classification", "papers": {}},
{"task": "Artistic style classification", "papers": {"0": "Recognizing Art Style Automatically in painting with deep learning"}},
{"task": "Classification Consistency", "papers": {"0": "Making Convolutional Networks Shift-Invariant Again"}},
{"task": "Congestive Heart Failure detection", "papers": {"0": "A convolutional neural network approach to detect congestive heart failure", "1": "Regularized HessELM and Inclined Entropy Measurement for Congestive Heart Failure Prediction"}},
{"task": "Superpixel Image Classification", "papers": {"0": "Probabilistic Numeric Convolutional Neural Networks"}},
{"task": "Cancer", "papers": {}},
{"task": "Photo geolocation estimation", "papers": {"0": "Geolocation Estimation of Photos using a Hierarchical Model and Scene Classification", "1": "Geolocation Estimation of Photos using a Hierarchical Model and Scene Classification"}},
{"task": "Colon Cancer Detection In Confocal Laser Microscopy Images", "papers": {}},
{"task": "ECG Denoising", "papers": {"0": "Deep Network for Capacitive ECG Denoising"}},
{"task": "Discovery Of Integrative Cancer Subtypes", "papers": {}},
{"task": "Oral Cancer Classification", "papers": {}},
{"task": "Classification Of Breast Cancer Histology Images", "papers": {}},
{"task": "Relation Mention Extraction", "papers": {}},
{"task": "Multi-Labeled Relation Extraction", "papers": {}},
{"paper": "Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning", "abstract": "Background: Despite recent significant progress in the development of automatic sleep staging methods, building a good model still remains a big challenge for sleep studies with a small cohort due to the data-variability and data-inefficiency issues. This work presents a deep transfer learning approach to overcome these issues and enable transferring knowledge from a large dataset to a small cohort for automatic sleep staging.Methods: We start from a generic end-to-end deep learning framework for sequence-to-sequence sleep staging and derive two networks as the means for transfer learning. The networks are first trained in the source domain (i.e. the large database). The pretrained networks are then finetuned in the target domain (i.e. the small cohort) to complete knowledge transfer. We employ the Montreal Archive of Sleep Studies (MASS) database consisting of 200 subjects as the source domain and study deep transfer learning on three different target domains: the Sleep Cassette subset and the Sleep Telemetry subset of the Sleep-EDF Expanded database, and the Surrey-cEEGrid database. The target domains are purposely adopted to cover different degrees of data mismatch to the source domains. Results: Our experimental results show significant performance improvement on automatic sleep staging on the target domains achieved with the proposed deep transfer learning approach. Conclusions: These results suggest the efficacy of the proposed approach in addressing the above-mentioned data-variability and data-inefficiency issues. Significance: As a consequence, it would enable one to improve the quality of automatic sleep staging models when the amount of data is relatively small. The source code and the pretrained models are available at http://github.com/pquochuy/sleep_transfer_learning."},
{"task": "Dialog Relation Extraction", "papers": {"0": "Dialogue-Based Relation Extraction"}},
{"task": "Relationship Extraction (Distant Supervised)", "papers": {"0": "RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information"}},
{"paper": "Recognizing Art Style Automatically in painting with deep learning", "abstract": "The artistic style (or artistic movement) of a painting is a rich descriptor that captures both\r\nvisual and historical information about the painting. Correctly identifying the artistic style\r\nof a paintings is crucial for indexing large artistic databases.In this paper, we investigate\r\nthe use of deep residual neural to solve the problem of detecting the artistic style of a\r\npainting and outperform existing approaches by almost 10% on the Wikipaintings dataset\r\n(for 25 dierent style). To achieve this result, the network is rst pre-trained on ImageNet,\r\nand deeply retrained for artistic style. We empirically evaluate that to achieve the best\r\nperformance, one need to retrain about 20 layers. This suggests that the two tasks are as\r\nsimilar as expected, and explain the previous success of hand crafted features. We also\r\ndemonstrate that the style detected on the Wikipaintings dataset are consistent with styles\r\ndetected on an independent dataset and describe a number of experiments we conducted\r\nto validate this approach both qualitatively and quantitatively."},
{"paper": "Impact of ECG Dataset Diversity on Generalization of CNN Model for Detecting QRS Complex", "abstract": "Detection of QRS complexes in electrocardiogram (ECG) signal is crucial for automated cardiac diagnosis. Automated QRS detection has been a research topic for over three decades and several of the traditional QRS detection methods show acceptable detection accuracy, however, the applicability of these methods beyond their study-specific databases was not explored.The non-stationary nature of ECG and signal variance of intra and inter-patient recordings impose significant challenges on single QRS detectors to achieve reasonable performance. In real life, a promising QRS detector may be expected to achieve acceptable accuracy over diverse ECG recordings and, thus, investigation of the model's generalization capability is crucial. This paper investigates the generalization capability of convolutional neural network (CNN) based-models from intra (subject wise leave-one-out and five-fold cross validation) and inter-database (training with single and multiple databases) points-of-view over three publicly available ECG databases, namely MIT-BIH Arrhythmia, INCART, and QT. Leave-one-out test accuracy reports 99.22%, 97.13%, and 96.25% for these databases accordingly and inter-database tests report more than 90% accuracy with the single exception of INCART. The performance variation reveals the fact that a CNN model's generalization capability does not increase simply by adding more training samples, rather the inclusion of samples from a diverse range of subjects is necessary for reasonable QRS detection accuracy."},
{"task": "Relation Classification", "papers": {}},
{"task": "Dialogue", "papers": {}},
{"paper": "Regularized HessELM and Inclined Entropy Measurement for Congestive Heart Failure Prediction", "abstract": "Our study concerns with automated predicting of congestive heart failure (CHF) through the analysis of electrocardiography (ECG) signals. A novel machine learning approach, regularized hessenberg decomposition based extreme learning machine (R-HessELM), and feature models; squared, circled, inclined and grid entropy measurement were introduced and used for prediction of CHF.This study proved that inclined entropy measurements features well represent characteristics of ECG signals and together with R-HessELM approach overall accuracy of 98.49% was achieved."},
{"paper": "A convolutional neural network approach to detect congestive heart failure", "abstract": "Congestive Heart Failure (CHF) is a severe pathophysiological condition associated with high prevalence, high mortality rates, and sustained healthcare costs, therefore demanding efficient methods for its detection. Despite recent research has provided methods focused on advanced signal processing and machine learning, the potential of applying Convolutional Neural Network (CNN) approaches to the automatic detection of CHF has been largely overlooked thus far.This study addresses this important gap by presenting a CNN model that accurately identifies CHF on the basis of one raw electrocardiogram (ECG) heartbeat only, also juxtaposing existing methods typically grounded on Heart Rate Variability. We trained and tested the model on publicly available ECG datasets, comprising a total of 490,505 heartbeats, to achieve 100% CHF detection accuracy. Importantly, the model also identifies those heartbeat sequences and ECG\u2019s morphological characteristics which are class-discriminative and thus prominent for CHF detection. Overall, our contribution substantially advances the current methodology for detecting CHF and caters to clinical practitioners\u2019 needs by providing an accurate and fully transparent tool to support decisions concerning CHF detection."},
{"task": "Relation Extraction", "papers": {"0": "Relation Extraction as Two-way Span-Prediction", "1": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling", "2": "Joint Entity and Relation Extraction with Set Prediction Networks", "3": "Relation Extraction as Two-way Span-Prediction", "4": "Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders", "5": "Downstream Model Design of Pre-trained Language Model for Relation Extraction Task", "6": "Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders", "7": "Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders", "8": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "9": "SciBERT: A Pretrained Language Model for Scientific Text", "10": "Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy", "11": "SciBERT: A Pretrained Language Model for Scientific Text", "12": "Generalizing Natural Language Analysis through Span-relation Representations", "13": "Context-Aware Representations for Knowledge Base Relation Extraction", "14": "ERNIE: Enhanced Language Representation with Informative Entities", "15": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "16": "BiTT: Bidirectional Tree Tagging for Joint Extraction of Overlapping Entities and Relations"}},
{"paper": "Making Convolutional Networks Shift-Invariant Again", "abstract": "Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem.The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe \\textit{increased accuracy} in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe \\textit{better generalization}, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/ ."},
{"paper": "Deep Network for Capacitive ECG Denoising", "abstract": "Continuous monitoring of cardiac health under free living condition is\ncrucial to provide effective care for patients undergoing post operative\nrecovery and individuals with high cardiac risk like the elderly. Capacitive\nElectrocardiogram (cECG) is one such technology which allows comfortable and\nlong term monitoring through its ability to measure biopotential in conditions\nwithout having skin contact.cECG monitoring can be done using many household\nobjects like chairs, beds and even car seats allowing for seamless monitoring\nof individuals. This method is unfortunately highly susceptible to motion\nartifacts which greatly limits its usage in clinical practice. The current use\nof cECG systems has been limited to performing rhythmic analysis. In this paper\nwe propose a novel end-to-end deep learning architecture to perform the task of\ndenoising capacitive ECG. The proposed network is trained using motion\ncorrupted three channel cECG and a reference LEAD I ECG collected on\nindividuals while driving a car. Further, we also propose a novel joint loss\nfunction to apply loss on both signal and frequency domain. We conduct\nextensive rhythmic analysis on the model predictions and the ground truth. We\nfurther evaluate the signal denoising using Mean Square Error(MSE) and Cross\nCorrelation between model predictions and ground truth. We report MSE of 0.167\nand Cross Correlation of 0.476. The reported results highlight the feasibility\nof performing morphological analysis using the filtered cECG. The proposed\napproach can allow for continuous and comprehensive monitoring of the\nindividuals in free living conditions."},
{"paper": "Probabilistic Numeric Convolutional Neural Networks", "abstract": "Continuous input signals like images and time series that are irregularly sampled or have missing values are challenging for existing deep learning methods. Coherently defined feature representations must depend on the values in unobserved regions of the input.Drawing from the work in probabilistic numerics, we propose Probabilistic Numeric Convolutional Neural Networks which represent features as Gaussian processes (GPs), providing a probabilistic description of discretization error. We then define a convolutional layer as the evolution of a PDE defined on this GP, followed by a nonlinearity. This approach also naturally admits steerable equivariant convolutions under e.g. the rotation group. In experiments we show that our approach yields a $3\\times$ reduction of error from the previous state of the art on the SuperPixel-MNIST dataset and competitive performance on the medical time series dataset PhysioNet2012."},
{"task": "Dialog Learning", "papers": {}},
{"task": "Dialogue Interpretation", "papers": {}},
{"task": "Task-Completion Dialogue Policy Learning", "papers": {}},
{"task": "Goal-Oriented Dialogue Systems", "papers": {}},
{"task": "Dialogue Act Classification", "papers": {"0": "Dialogue Act Classification with Context-Aware Self-Attention", "1": "A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification"}},
{"paper": "Geolocation Estimation of Photos using a Hierarchical Model and Scene Classification", "abstract": "While the successful estimation of a photo's geolocation enables a number of interesting applications, it is also a very challenging task. Due to the complexity of the problem, most existing approaches are restricted to specific areas, imagery, or worldwide landmarks.Only a few proposals predict GPS coordinates without any limitations. In this paper, we introduce several deep learning methods, which pursue the latter approach and treat geolocalization as a classification problem where the earth is subdivided into geographical cells. We propose to exploit hierarchical knowledge of multiple partitionings and additionally extract and take the photo's scene content into account, i.e., indoor, natural, or urban setting etc. As a result, contextual information at different spatial resolutions as well as more specific features for various environmental settings are incorporated in the learning process of the convolutional neural network. Experimental results on two benchmarks demonstrate the effectiveness of our approach outperforming the state of the art while using a significant lower number of training images and without relying on retrieval methods that require an appropriate reference dataset."},
{"paper": "Dialogue-Based Relation Extraction", "abstract": "We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences.We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https://dataset.org/dialogre/."},
{"paper": "RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information", "abstract": "Distantly-supervised Relation Extraction (RE) methods train an extractor by\nautomatically aligning relation instances in a Knowledge Base (KB) with\nunstructured text. In addition to relation instances, KBs often contain other\nrelevant side information, such as aliases of relations (e.g., founded and\nco-founded are aliases for the relation founderOfCompany).RE models usually\nignore such readily available side information. In this paper, we propose\nRESIDE, a distantly-supervised neural relation extraction method which utilizes\nadditional side information from KBs for improved relation extraction. It uses\nentity type and relation alias information for imposing soft constraints while\npredicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode\nsyntactic information from text and improves performance even when limited side\ninformation is available. Through extensive experiments on benchmark datasets,\nwe demonstrate RESIDE's effectiveness. We have made RESIDE's source code\navailable to encourage reproducible research."},
{"task": "Short-Text Conversation", "papers": {}},
{"task": "Dialogue Understanding", "papers": {}},
{"task": "Dialogue Management", "papers": {}},
{"paper": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "abstract": "NoneNone"},
{"paper": "Context-Aware Representations for Knowledge Base Relation Extraction", "abstract": "We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence.We combine the context representations with an attention mechanism to make the final prediction. We use the Wikidata knowledge base to construct a dataset of multiple relations per sentence and to evaluate our approach. Compared to a baseline system, our method results in an average error reduction of 24 on a held-out set of relations. The code and the dataset to replicate the experiments are made available at \\url{https://github.com/ukplab/}."},
{"paper": "BiTT: Bidirectional Tree Tagging for Joint Extraction of Overlapping Entities and Relations", "abstract": "Joint extraction refers to extracting triples, composed of entities and relations, simultaneously from the text with a single model. However, most existing methods fail to extract all triples accurately and efficiently from sentences with overlapping issue, i.e., the same entity is included in multiple triples.In this paper, we propose a novel scheme called Bidirectional Tree Tagging (BiTT) to label overlapping triples in text. In BiTT, the triples with the same relation category in a sentence are especially represented as two binary trees, each of which is converted into a word-level tags sequence to label each word. Based on BiTT scheme, we develop an end-to-end extraction framework to predict the BiTT tags and further extract triples efficiently. We adopt the Bi-LSTM and the BERT as the encoder in our framework respectively, and obtain promising results in public English as well as Chinese datasets."},
{"task": "Task-Oriented Dialogue Systems", "papers": {}},
{"task": "Goal-Oriented Dialog", "papers": {"0": "Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning"}},
{"task": "Visual Dialog", "papers": {"0": "Factor Graph Attention", "1": "Factor Graph Attention", "2": "Factor Graph Attention"}},
{"paper": "Generalizing Natural Language Analysis through Span-relation Representations", "abstract": "Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks.We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis."},
{"paper": "ERNIE: Enhanced Language Representation with Informative Entities", "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE."},
{"paper": "Downstream Model Design of Pre-trained Language Model for Relation Extraction Task", "abstract": "Supervised relation extraction methods based on deep neural network play an important role in the recent information extraction field. However, at present, their performance still fails to reach a good level due to the existence of complicated relations.On the other hand, recently proposed pre-trained language models (PLMs) have achieved great success in multiple tasks of natural language processing through fine-tuning when combined with the model of downstream tasks. However, original standard tasks of PLM do not include the relation extraction task yet. We believe that PLMs can also be used to solve the relation extraction problem, but it is necessary to establish a specially designed downstream task model or even loss function for dealing with complicated relations. In this paper, a new network architecture with a special loss function is designed to serve as a downstream model of PLMs for supervised relation extraction. Experiments have shown that our method significantly exceeded the current optimal baseline models across multiple public datasets of relation extraction."},
{"paper": "A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification", "abstract": "Recognising dialogue acts (DA) is important for many natural language processing tasks such as dialogue generation and intention recognition. In this paper, we propose a dual-attention hierarchical recurrent neural network for DA classification.Our model is partially inspired by the observation that conversational utterances are normally associated with both a DA and a topic, where the former captures the social act and the latter describes the subject matter. However, such a dependency between DAs and topics has not been utilised by most existing systems for DA classification. With a novel dual task-specific attention mechanism, our model is able, for utterances, to capture information about both DAs and topics, as well as information about the interactions between them. Experimental results show that by modelling topic as an auxiliary task, our model can significantly improve DA classification, yielding better or comparable performance to the state-of-the-art method on three public datasets."},
{"paper": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "abstract": "Multi-task learning (MTL) is an effective method for learning related tasks, but designing MTL models necessitates deciding which and how many parameters should be task-specific, as opposed to shared between tasks. We investigate this issue for the problem of jointly learning named entity recognition (NER) and relation extraction (RE) and propose a novel neural architecture that allows for deeper task-specificity than does prior work.In particular, we introduce additional task-specific bidirectional RNN layers for both the NER and RE tasks and tune the number of shared and task-specific layers separately for different datasets. We achieve state-of-the-art (SOTA) results for both tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of correctly balancing the number of shared and task-specific parameters for MTL approaches in general."},
{"paper": "Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy", "abstract": "Joint extraction of entities and relations aims to detect entity pairs along with their relations using a single model. Prior work typically solves this task in the extract-then-classify or unified labeling manner.However, these methods either suffer from the redundant entity pairs, or ignore the important inner structure in the process of extracting entities and relations. To address these limitations, in this paper, we first decompose the joint extraction task into two interrelated subtasks, namely HE extraction and TER extraction. The former subtask is to distinguish all head-entities that may be involved with target relations, and the latter is to identify corresponding tail-entities and relations for each extracted head-entity. Next, these two subtasks are further deconstructed into several sequence labeling problems based on our proposed span-based tagging scheme, which are conveniently solved by a hierarchical boundary tagger and a multi-span decoding algorithm. Owing to the reasonable decomposition strategy, our model can fully capture the semantic interdependency between different steps, as well as reduce noise from irrelevant entity pairs. Experimental results show that our method outperforms previous work by 5.2%, 5.9% and 21.5% (F1 score), achieving a new state-of-the-art on three public datasets"},
{"paper": "Dialogue Act Classification with Context-Aware Self-Attention", "abstract": "Recent work in Dialogue Act classification has treated the task as a sequence labeling problem using hierarchical deep neural networks. We build on this prior work by leveraging the effectiveness of a context-aware self-attention mechanism coupled with a hierarchical recurrent neural network.We conduct extensive evaluations on standard Dialogue Act classification datasets and show significant improvement over state-of-the-art results on the Switchboard Dialogue Act (SwDA) Corpus. We also investigate the impact of different utterance-level representation learning methods and show that our method is effective at capturing utterance-level semantic text representations while maintaining high accuracy."},
{"paper": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling", "abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level RE counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations.In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multilabel and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work by a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDR and GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4; and also significantly outperforms existing models on both CDR and GDA."},
{"paper": "Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders", "abstract": "Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem.However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel {\\em table-sequence encoders} where two different encoders -- a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having {\\em two} encoders over {\\em one} encoder. On several standard datasets, our model shows significant improvements over existing approaches."},
{"paper": "SciBERT: A Pretrained Language Model for Scientific Text", "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data.SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/."},
{"paper": "Relation Extraction as Two-way Span-Prediction", "abstract": "The current supervised relation classification (RC) task uses a single embedding to represent the relation between a pair of entities. We argue that a better approach is to treat the RC task as a Question answering (QA) like span prediction problem.We present a span-prediction based system for RC and evaluate its performance compared to the embedding based system. We achieve state-of-the-art results on the TACRED and SemEval task 8 datasets."},
{"task": "Prediction Of Cancer Cell Line Sensitivity", "papers": {}},
{"task": "Breast Cancer Histology Image Classification", "papers": {}},
{"task": "Lung Cancer Diagnosis", "papers": {}},
{"task": "Dialogue State Tracking", "papers": {"0": "Towards Universal Dialogue State Tracking", "1": "Towards Universal Dialogue State Tracking"}},
{"paper": "Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning", "abstract": "Attention-based encoder-decoder neural network models have recently shown promising results in goal-oriented dialogue systems. However, these models struggle to reason over and incorporate state-full knowledge while preserving their end-to-end text generation functionality.Since such models can greatly benefit from user intent and knowledge graph integration, in this paper we propose an RNN-based end-to-end encoder-decoder architecture which is trained with joint embeddings of the knowledge graph and the corpus as input. The model provides an additional integration of user intent along with text generation, trained with a multi-task learning paradigm along with an additional regularization technique to penalize generating the wrong entity as output. The model further incorporates a Knowledge Graph entity lookup during inference to guarantee the generated output is state-full based on the local knowledge graph provided. We finally evaluated the model using the BLEU score, empirical evaluation depicts that our proposed architecture can aid in the betterment of task-oriented dialogue system`s performance."},
{"task": "Dialogue Generation", "papers": {"0": "You Impress Me: Dialogue Generation via Mutual Persona Perception", "1": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "2": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "3": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "4": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "5": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "6": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "7": "Jointly Optimizing Diversity and Relevance in Neural Response Generation", "8": "Adversarial Learning for Neural Dialogue Generation"}},
{"paper": "Factor Graph Attention", "abstract": "Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge.Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%."},
{"task": "Skin Cancer Classification", "papers": {}},
{"task": "Ventricular fibrillation detection", "papers": {}},
{"task": "Breast Cancer Detection", "papers": {}},
{"task": "Myocardial infarction detection", "papers": {"0": "Deep Learning for Cardiologist-level Myocardial Infarction Detection in Electrocardiograms", "1": "Deep Learning for Cardiologist-level Myocardial Infarction Detection in Electrocardiograms"}},
{"task": "Heartbeat Classification", "papers": {"0": "A Fast Machine Learning Model for ECG-Based Heartbeat Classification and Arrhythmia Detection", "1": "A Fast Machine Learning Model for ECG-Based Heartbeat Classification and Arrhythmia Detection", "2": "A convolutional neural network approach to detect congestive heart failure"}},
{"task": "Heart Rate Variability", "papers": {}},
{"task": "ECG Classification", "papers": {"0": "Automatic diagnosis of the 12-lead ECG using a deep neural network"}},
{"task": "Arrhythmia Detection", "papers": {"0": "Inter- and intra- patient ECG heartbeat classification for arrhythmia detection: a sequence to sequence deep learning approach", "1": "An Open-source Toolbox for Analysing and Processing PhysioNet Databases in MATLAB and Octave", "2": "Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network", "3": "Detection and Classification of Cardiac Arrhythmias by a Challenge-Best Deep Learning Neural Network Model"}},
{"task": "Electrocardiography (ECG)", "papers": {}},
{"task": "Mortality Prediction", "papers": {"0": "Early hospital mortality prediction using vital signals"}},
{"task": "Satellite Image Classification", "papers": {"0": "DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification", "1": "DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification"}},
{"paper": "Towards Universal Dialogue State Tracking", "abstract": "Dialogue state tracking is the core part of a spoken dialogue system. It\nestimates the beliefs of possible user's goals at every dialogue turn.However,\nfor most current approaches, it's difficult to scale to large dialogue domains. They have one or more of following limitations: (a) Some models don't work in\nthe situation where slot values in ontology changes dynamically; (b) The number\nof model parameters is proportional to the number of slots; (c) Some models\nextract features based on hand-crafted lexicons. To tackle these challenges, we\npropose StateNet, a universal dialogue state tracker. It is independent of the\nnumber of values, shares parameters across all slots, and uses pre-trained word\nvectors instead of explicit semantic dictionaries. Our experiments on two\ndatasets show that our approach not only overcomes the limitations, but also\nsignificantly outperforms the performance of state-of-the-art approaches."},
{"paper": "Adversarial Learning for Neural Dialogue Generation", "abstract": "In this paper, drawing intuition from the Turing test, we propose using\nadversarial training for open-domain dialogue generation: the system is trained\nto produce sequences that are indistinguishable from human-generated dialogue\nutterances. We cast the task as a reinforcement learning (RL) problem where we\njointly train two systems, a generative model to produce response sequences,\nand a discriminator---analagous to the human evaluator in the Turing test--- to\ndistinguish between the human-generated dialogues and the machine-generated\nones.The outputs from the discriminator are then used as rewards for the\ngenerative model, pushing the system to generate dialogues that mostly resemble\nhuman dialogues. In addition to adversarial training we describe a model for adversarial {\\em\nevaluation} that uses success in fooling an adversary as a dialogue evaluation\nmetric, while avoiding a number of potential pitfalls. Experimental results on\nseveral metrics, including adversarial evaluation, demonstrate that the\nadversarially-trained system generates higher-quality responses than previous\nbaselines."},
{"task": "Document Image Classification", "papers": {"0": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding", "1": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters", "2": "Pixel-level Reconstruction and Classification for Noisy Handwritten Bangla Characters", "3": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters"}},
{"paper": "Jointly Optimizing Diversity and Relevance in Neural Response Generation", "abstract": "Although recent neural conversation models have shown great potential, they\noften generate bland and generic responses. While various approaches have been\nexplored to diversify the output of the conversation model, the improvement\noften comes at the cost of decreased relevance.In this paper, we propose a\nSpaceFusion model to jointly optimize diversity and relevance that essentially\nfuses the latent space of a sequence-to-sequence model and that of an\nautoencoder model by leveraging novel regularization terms. As a result, our\napproach induces a latent space in which the distance and direction from the\npredicted response vector roughly match the relevance and diversity,\nrespectively. This property also lends itself well to an intuitive\nvisualization of the latent space. Both automatic and human evaluation results\ndemonstrate that the proposed approach brings significant improvement compared\nto strong baselines in both diversity and relevance."},
{"task": "Sparse Representation-based Classification", "papers": {"0": "Deep Sparse Representation-based Classification"}},
{"task": "Unsupervised Image Classification", "papers": {"0": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "1": "SCAN: Learning to Classify Images without Labels", "2": "SCAN: Learning to Classify Images without Labels", "3": "SCAN: Learning to Classify Images without Labels", "4": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization", "5": "Self-Supervised Learning for Large-Scale Unsupervised Image Clustering"}},
{"task": "Genre classification", "papers": {}},
{"paper": "You Impress Me: Dialogue Generation via Mutual Persona Perception", "abstract": "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation.Motivated by this, we propose P^2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, P^2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations."},
{"paper": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "abstract": "We introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens. There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics.Such procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\nthe standard log- likelihood objective w.r.t. natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions. We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy. On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics. Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure."},
{"paper": "Deep Learning for Cardiologist-level Myocardial Infarction Detection in Electrocardiograms", "abstract": "Myocardial infarction is the leading cause of death worldwide. In this paper, we design domain-inspired neural network models to detect myocardial infarction.First, we study the contribution of various leads. This systematic analysis, first of its kind in the literature, indicates that out of 15 ECG leads, data from the v6, vz, and ii leads are critical to correctly identify myocardial infarction. Second, we use this finding and adapt the ConvNetQuake neural network model--originally designed to identify earthquakes--to attain state-of-the-art classification results for myocardial infarction, achieving $99.43\\%$ classification accuracy on a record-wise split, and $97.83\\%$ classification accuracy on a patient-wise split. These two results represent cardiologist-level performance level for myocardial infarction detection after feeding only 10 seconds of raw ECG data into our model. Third, we show that our multi-ECG-channel neural network achieves cardiologist-level performance without the need of any kind of manual feature extraction or data pre-processing."},
{"paper": "A Fast Machine Learning Model for ECG-Based Heartbeat Classification and Arrhythmia Detection", "abstract": "We present a fully automatic and fast ECG arrhythmia classifier based on a simple brain-inspired machine learning approach known as Echo State Networks. Our classifier has a low-demanding feature processing that only requires a single ECG lead.Its training and validation follows an inter-patient procedure. Our approach is compatible with an online classification that aligns well with recent advances in health-monitoring wireless devices and wearables. The use of a combination of ensembles allows us to exploit parallelism to train the classifier with remarkable speeds. The heartbeat classifier is evaluated over two ECG databases, the MIT-BIH AR and the AHA. In the MIT-BIH AR database, our classification approach provides a sensitivity of 92.7% and positive predictive value of 86.1% for the ventricular ectopic beats, using the single lead II, and a sensitivity of 95.7% and positive predictive value of 75.1% when using the lead V1'. These results are comparable with the state of the art in fully automatic ECG classifiers and even outperform other ECG classifiers that follow more complex feature-selection approaches."},
{"paper": "Inter- and intra- patient ECG heartbeat classification for arrhythmia detection: a sequence to sequence deep learning approach", "abstract": "Electrocardiogram (ECG) signal is a common and powerful tool to study heart\nfunction and diagnose several abnormal arrhythmia. While there have been\nremarkable improvements in cardiac arrhythmia classification methods, they\nstill cannot offer an acceptable performance in detecting different heart\nconditions, especially when dealing with imbalanced datasets.In this paper, we\npropose a solution to address this limitation of current classification\napproaches by developing an automatic heartbeat classification method using\ndeep convolutional neural networks and sequence to sequence models. We\nevaluated the proposed method on the MIT-BIH arrhythmia database, considering\nthe intra-patient and inter-patient paradigms, and the AAMI EC57 standard. The\nevaluation results for both paradigms show that our method achieves the best\nperformance in the literature (a positive predictive value of 96.46% and\nsensitivity of 100% for the category S, and a positive predictive value of\n98.68% and sensitivity of 97.40% for the category F for the intra-patient\nscheme; a positive predictive value of 92.57% and sensitivity of 88.94% for the\ncategory S, and a positive predictive value of 99.50% and sensitivity of 99.94%\nfor the category V for the inter-patient scheme.)"},
{"paper": "DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification", "abstract": "Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets.The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. In a preliminary version of this work, we introduced two new high resolution satellite imagery datasets (SAT-4 and SAT-6) and proposed DeepSat framework for classification based on \"handcrafted\" features and a deep belief network (DBN). The present paper is an extended version, we present an end-to-end framework leveraging an improved architecture that augments a convolutional neural network (CNN) with handcrafted features (instead of using DBN-based architecture) for classification. Our framework, having access to fused spatial information obtained from handcrafted features as well as CNN feature maps, have achieved accuracies of 99.90% and 99.84% respectively, on SAT-4 and SAT-6, surpassing all the other state-of-the-art results. A statistical analysis based on Distribution Separability Criterion substantiates the robustness of our approach in learning better representations for satellite imagery."},
{"task": "Sequential Image Classification", "papers": {"0": "Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies", "1": "Trellis Networks for Sequence Modeling"}},
{"paper": "Detection and Classification of Cardiac Arrhythmias by a Challenge-Best Deep Learning Neural Network Model", "abstract": "Electrocardiograms (ECGs) are widely used to clinically detect cardiac arrhythmias (CAs). They are also being used to develop computer-assisted methods for heart disease diagnosis.We have developed a convolution neural network model to detect and classify CAs, using a large 12-lead ECG dataset (6,877 recordings) provided by the China Physiological Signal Challenge (CPSC) 2018. Our model, which was ranked first in the challenge competition, achieved a median overall F1-score of 0.84 for the nine-type CA classification of CPSC2018's hidden test set of 2,954 ECG recordings. Further analysis showed that concurrent CAs were adequately predictive for 476 patients with multiple types of CA diagnoses in the dataset. Using only single-lead data yielded a performance that was only slightly worse than using the full 12-lead data, with leads aVR and V1 being the most prominent. We extensively consider these results in the context of their agreement with and relevance to clinical observations."},
{"task": "Learning with noisy labels", "papers": {}},
{"paper": "Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network", "abstract": "Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow. Widely available digital ECG data and the algorithmic paradigm of deep learning present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis.However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions. Dataset available from: https://irhythm.github.io/cardiol_test_set/"},
{"paper": "Automatic diagnosis of the 12-lead ECG using a deep neural network", "abstract": "The role of automatic electrocardiogram (ECG) analysis in clinical practice is limited by the accuracy of existing models. Deep Neural Networks (DNNs) are models composed of stacked transformations that learn tasks by examples.This technology has recently achieved striking success in a variety of task and there are great expectations on how it might improve clinical practice. Here we present a DNN model trained in a dataset with more than 2 million labeled exams analyzed by the Telehealth Network of Minas Gerais and collected under the scope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The DNN outperform cardiology resident medical doctors in recognizing 6 types of abnormalities in 12-lead ECG recordings, with F1 scores above 80% and specificity over 99%. These results indicate ECG analysis based on DNNs, previously studied in a single-lead setup, generalizes well to 12-lead exams, taking the technology closer to the standard clinical practice."},
{"paper": "Early hospital mortality prediction using vital signals", "abstract": "Early hospital mortality prediction is critical as intensivists strive to\nmake efficient medical decisions about the severely ill patients staying in\nintensive care units. As a result, various methods have been developed to\naddress this problem based on clinical records.However, some of the laboratory\ntest results are time-consuming and need to be processed. In this paper, we\npropose a novel method to predict mortality using features extracted from the\nheart signals of patients within the first hour of ICU admission. In order to\npredict the risk, quantitative features have been computed based on the heart\nrate signals of ICU patients. Each signal is described in terms of 12\nstatistical and signal-based features. The extracted features are fed into\neight classifiers: decision tree, linear discriminant, logistic regression,\nsupport vector machine (SVM), random forest, boosted trees, Gaussian SVM, and\nK-nearest neighborhood (K-NN). To derive insight into the performance of the\nproposed method, several experiments have been conducted using the well-known\nclinical dataset named Medical Information Mart for Intensive Care III\n(MIMIC-III). The experimental results demonstrate the capability of the\nproposed method in terms of precision, recall, F1-score, and area under the\nreceiver operating characteristic curve (AUC). The decision tree classifier\nsatisfies both accuracy and interpretability better than the other classifiers,\nproducing an F1-score and AUC equal to 0.91 and 0.93, respectively. It\nindicates that heart rate signals can be used for predicting mortality in\npatients in the ICU, achieving a comparable performance with existing\npredictions that rely on high dimensional features from clinical records which\nneed to be processed and may contain missing information."},
{"paper": "An Open-source Toolbox for Analysing and Processing PhysioNet Databases in MATLAB and Octave", "abstract": "The WaveForm DataBase (WFDB) Toolbox for MATLAB/Octave enables integrated access to PhysioNet's software and databases. Using the WFDB Toolbox for MATLAB/Octave, users have access to over 50 physiological databases in PhysioNet.The toolbox provides access over 4 TB of biomedical signals including ECG, EEG, EMG, and PLETH. Additionally, most signals are accompanied by metadata such as medical annotations of clinical events: arrhythmias, sleep stages, seizures, hypotensive episodes, etc. Users of this toolbox should easily be able to reproduce, validate, and compare results published based on PhysioNet's software and databases."},
{"paper": "Deep Sparse Representation-based Classification", "abstract": "We present a transductive deep learning-based formulation for the sparse\nrepresentation-based classification (SRC) method. The proposed network consists\nof a convolutional autoencoder along with a fully-connected layer.The role of\nthe autoencoder network is to learn robust deep features for classification. On\nthe other hand, the fully-connected layer, which is placed in between the\nencoder and the decoder networks, is responsible for finding the sparse\nrepresentation. The estimated sparse codes are then used for classification. Various experiments on three different datasets show that the proposed network\nleads to sparse representations that give better classification results than\nstate-of-the-art SRC methods. The source code is available at:\ngithub.com/mahdiabavisani/DSRC."},
{"paper": "Pixel-level Reconstruction and Classification for Noisy Handwritten Bangla Characters", "abstract": "Classification techniques for images of handwritten characters are\nsusceptible to noise. Quadtrees can be an efficient representation for learning\nfrom sparse features.In this paper, we improve the effectiveness of\nprobabilistic quadtrees by using a pixel level classifier to extract the\ncharacter pixels and remove noise from handwritten character images. The pixel\nlevel denoiser (a deep belief network) uses the map responses obtained from a\npretrained CNN as features for reconstructing the characters eliminating noise. We experimentally demonstrate the effectiveness of our approach by\nreconstructing and classifying a noisy version of handwritten Bangla Numeral\nand Basic Character datasets."},
{"paper": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters", "abstract": "Due to the sparsity of features, noise has proven to be a great inhibitor in the classification of handwritten characters. To combat this, most techniques perform denoising of the data before classification.In this paper, we consolidate the approach by training an all-in-one model that is able to classify even noisy characters. For classification, we progressively train a classifier generative adversarial network on the characters from low to high resolution. We show that by learning the features at each resolution independently a trained model is able to accurately classify characters even in the presence of noise. We experimentally demonstrate the effectiveness of our approach by classifying noisy versions of MNIST, handwritten Bangla Numeral, and Basic Character datasets."},
{"paper": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization", "abstract": "In this paper, we propose a novel unsupervised clustering approach exploiting\nthe hidden information that is indirectly introduced through a pseudo\nclassification objective. Specifically, we randomly assign a pseudo\nparent-class label to each observation which is then modified by applying the\ndomain specific transformation associated with the assigned label.Generated\npseudo observation-label pairs are subsequently used to train a neural network\nwith Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes\nfor each pseudo parent-class. Due to the unsupervised objective based on\nGraph-based Activity Regularization (GAR) terms, softmax duplicates of each\nparent-class are specialized as the hidden information captured through the\nhelp of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we\ndemonstrate how the chosen transformation type impacts performance and helps\npropagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks\non MNIST, SVHN and USPS datasets, with the highest accuracies reported to date\nin the literature."},
{"paper": "Self-Supervised Learning for Large-Scale Unsupervised Image Clustering", "abstract": "Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts.Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at https://github.com/Randl/kmeans_selfsuper"},
{"paper": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding", "abstract": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding.In this paper, we propose the \\textbf{LayoutLM} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at \\url{https://aka.ms/layoutlm}."},
{"task": "Self-Supervised Image Classification", "papers": {"0": "Big Self-Supervised Models are Strong Semi-Supervised Learners"}},
{"paper": "Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies", "abstract": "Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks.Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data."},
{"paper": "Trellis Networks for Sequence Modeling", "abstract": "We present trellis networks, a new architecture for sequence modeling. On the\none hand, a trellis network is a temporal convolutional network with special\nstructure, characterized by weight tying across depth and direct injection of\nthe input into deep layers.On the other hand, we show that truncated recurrent\nnetworks are equivalent to trellis networks with special sparsity structure in\ntheir weight matrices. Thus trellis networks with general weight matrices\ngeneralize truncated recurrent networks. We leverage these connections to\ndesign high-performing trellis networks that absorb structural and algorithmic\nelements from both recurrent and convolutional models. Experiments demonstrate\nthat trellis networks outperform the current state of the art methods on a\nvariety of challenging benchmarks, including word-level language modeling and\ncharacter-level language modeling tasks, and stress tests designed to evaluate\nlong-term memory retention. The code is available at\nhttps://github.com/locuslab/trellisnet ."},
{"paper": "SCAN: Learning to Classify Images without Labels", "abstract": "Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision.Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is made publicly available at https://github.com/wvangansbeke/Unsupervised-Classification."},
{"task": "Hyperspectral Image Classification", "papers": {"0": "FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification", "1": "HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification", "2": "HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification", "3": "FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification", "4": "FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification"}},
{"task": "Sleep Quality Prediction", "papers": {}},
{"task": "Sleep apnea detection", "papers": {"0": "DOSED: a deep learning approach to detect multiple sleep micro-events in EEG signal"}},
{"task": "Small Data Image Classification", "papers": {"0": "Performance Analysis of Semi-supervised Learning in the Small-data Regime using VAEs", "1": "Generative Latent Implicit Conditional Optimization when Learning from Small Sample", "2": "Generative Latent Implicit Conditional Optimization when Learning from Small Sample", "3": "Generative Latent Implicit Conditional Optimization when Learning from Small Sample", "4": "Generative Latent Implicit Conditional Optimization when Learning from Small Sample", "5": "Generative Latent Implicit Conditional Optimization when Learning from Small Sample"}},
{"task": "Semi-Supervised Image Classification", "papers": {"0": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "1": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "2": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "3": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "4": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "5": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "6": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "7": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "8": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring", "9": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "10": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "11": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "12": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "13": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "14": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "15": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "16": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "17": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "18": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "19": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "20": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "21": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "22": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "23": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank", "24": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank", "25": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank", "26": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank"}},
{"task": "Sleep Quality", "papers": {"0": "Sleep quality prediction in caregivers using physiological signals"}},
{"task": "Sleep Arousal Detection", "papers": {"0": "DOSED: a deep learning approach to detect multiple sleep micro-events in EEG signal", "1": "Deepsleep: Fast and Accurate Delineation of Sleep Arousals at Millisecond Resolution by Deep Learning"}},
{"task": "Fine-Grained Image Classification", "papers": {"0": "Domain Adaptive Transfer Learning with Specialist Models", "1": "End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition", "2": "Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization", "3": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "4": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "5": "Fixing the train-test resolution discrepancy", "6": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "7": "SpinalNet: Deep Neural Network with Gradual Input", "8": "Learning Attentive Pairwise Interaction for Fine-Grained Classification", "9": "Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization", "10": "Fine-Tuning DARTS for Image Classification", "11": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "12": "SpinalNet: Deep Neural Network with Gradual Input", "13": "SpinalNet: Deep Neural Network with Gradual Input", "14": "SpinalNet: Deep Neural Network with Gradual Input", "15": "Rethinking Softmax with Cross-Entropy: Neural Network Classifier as Mutual Information Estimator", "16": "Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network", "17": "Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features", "18": "Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features"}},
{"task": "Image Classification", "papers": {"0": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "1": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "2": "SpinalNet: Deep Neural Network with Gradual Input", "3": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "4": "A Branching and Merging Convolutional Network with Homogeneous Filter Capsules", "5": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "6": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "7": "SpinalNet: Deep Neural Network with Gradual Input", "8": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "9": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "10": "Fine-Tuning DARTS for Image Classification", "11": "Neural Architecture Transfer", "12": "SpinalNet: Deep Neural Network with Gradual Input", "13": "Edge-labeling Graph Neural Network for Few-shot Learning", "14": "Big Transfer (BiT): General Visual Representation Learning", "15": "SpinalNet: Deep Neural Network with Gradual Input", "16": "SpinalNet: Deep Neural Network with Gradual Input", "17": "Big Transfer (BiT): General Visual Representation Learning", "18": "Fixing the train-test resolution discrepancy", "19": "Null-sampling for Interpretable and Fair Representations", "20": "Webly Supervised Image Classification with Self-Contained Confidence", "21": "CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise", "22": "Direction Concentration Learning: Enhancing Congruency in Machine Learning", "23": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters", "24": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters", "25": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters", "26": "SpinalNet: Deep Neural Network with Gradual Input", "27": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "28": "Dynamic Routing Between Capsules", "29": "Rethinking Softmax with Cross-Entropy: Neural Network Classifier as Mutual Information Estimator", "30": "Capsule Routing via Variational Bayes", "31": "Rethinking Recurrent Neural Networks and other Improvements for Image Classification", "32": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup"}},
{"task": "Sleep Micro-event detection", "papers": {}},
{"task": "K-complex detection", "papers": {"0": "RED: Deep Recurrent Neural Networks for Sleep EEG Event Detection"}},
{"task": "X-Ray", "papers": {}},
{"task": "Spindle Detection", "papers": {"0": "RED: Deep Recurrent Neural Networks for Sleep EEG Event Detection", "1": "DOSED: a deep learning approach to detect multiple sleep micro-events in EEG signal", "2": "DOSED: a deep learning approach to detect multiple sleep micro-events in EEG signal"}},
{"task": "Cbct Artifact Reduction", "papers": {}},
{"task": "Mapping Of Lung Nodules In Low-Dose Ct Images", "papers": {}},
{"task": "Sleep Stage Detection", "papers": {"0": "Intra- and Inter-epoch Temporal Context Network (IITNet) Using Sub-epoch Features for Automatic Sleep Scoring on Raw Single-channel EEG", "1": "Intra- and Inter-epoch Temporal Context Network (IITNet) Using Sub-epoch Features for Automatic Sleep Scoring on Raw Single-channel EEG"}},
{"task": "Finding Pulmonary Nodules In Large-Scale Ct Images", "papers": {}},
{"paper": "HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification", "abstract": "Hyperspectral image (HSI) classification is widely used for the analysis of remotely sensed images. Hyperspectral imagery includes varying bands of images.Convolutional Neural Network (CNN) is one of the most frequently used deep learning based methods for visual data processing. The use of CNN for HSI classification is also visible in recent works. These approaches are mostly based on 2D CNN. Whereas, the HSI classification performance is highly dependent on both spatial and spectral information. Very few methods have utilized the 3D CNN because of increased computational complexity. This letter proposes a Hybrid Spectral Convolutional Neural Network (HybridSN) for HSI classification. Basically, the HybridSN is a spectral-spatial 3D-CNN followed by spatial 2D-CNN. The 3D-CNN facilitates the joint spatial-spectral feature representation from a stack of spectral bands. The 2D-CNN on top of the 3D-CNN further learns more abstract level spatial representation. Moreover, the use of hybrid CNNs reduces the complexity of the model compared to 3D-CNN alone. To test the performance of this hybrid approach, very rigorous HSI classification experiments are performed over Indian Pines, Pavia University and Salinas Scene remote sensing datasets. The results are compared with the state-of-the-art hand-crafted as well as end-to-end deep learning based methods. A very satisfactory performance is obtained using the proposed HybridSN for HSI classification. The source code can be found at \\url{https://github.com/gokriznastic/HybridSN}."},
{"paper": "FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification", "abstract": "Deep learning techniques have provided significant improvements in hyperspectral image (HSI) classification. The current deep learning-based HSI classifiers follow a patch-based learning framework by dividing the image into overlapping patches.As such, these methods are local learning methods, which have a high computational cost. In this article, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. The proposed framework consists of three main parts: 1) a designed sampling strategy; 2) an encoder-decoder-based fully convolutional network (FCN); and 3) lateral connections between the encoder and decoder. In FPGA, an encoder-decoder-based FCN is utilized to consider the global spatial information by processing the whole image, which results in fast inference. However, it is difficult to directly utilize the encoder-decoder-based FCN for HSI classification as it always fails to converge due to the insufficiently diverse gradients caused by the limited training samples. To solve the divergence problem and maintain the FCNs abilities of fast inference and global spatial information mining, a global stochastic stratified (GS 2 ) sampling strategy is first proposed by transforming all the training samples into a stochastic sequence of stratified samples. This strategy can obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. For a better design of FCN architecture, FreeNet, which is a fully end-to-end network for HSI classification, is proposed to maximize the exploitation of the global spatial information and boost the performance via a spectral attention-based encoder and a lightweight decoder. A lateral connection module is also designed to connect the encoder and decoder, fusing the spatial details in the encoder and the semantic features in the decoder. The experimental results obtained using three public benchmark data sets suggest that the FPGA framework is superior to the patch-based framework in both speed and accuracy for HSI classification."},
{"paper": "Performance Analysis of Semi-supervised Learning in the Small-data Regime using VAEs", "abstract": "Extracting large amounts of data from biological samples is not feasible due to radiation issues, and image processing in the small-data regime is one of the critical challenges when working with a limited amount of data. In this work, we applied an existing algorithm named Variational Auto Encoder (VAE) that pre-trains a latent space representation of the data to capture the features in a lower-dimension for the small-data regime input.The fine-tuned latent space provides constant weights that are useful for classification. Here we will present the performance analysis of the VAE algorithm with different latent space sizes in the semi-supervised learning using the CIFAR-10 dataset."},
{"paper": "Generative Latent Implicit Conditional Optimization when Learning from Small Sample", "abstract": "We revisit the long-standing problem of learning from small sample, to which end we propose a novel method called GLICO (Generative Latent Implicit Conditional Optimization). GLICO learns a mapping from the training examples to a latent space, and a generator that generates images from vectors in the latent space.Unlike most recent works, which rely on access to large amounts of unlabeled data, GLICO does not require access to any additional data other than the small set of labeled points. In fact, GLICO learns to synthesize completely new samples for every class using as little as 5 or 10 examples per class, with as few as 10 such classes without imposing any prior. GLICO is then used to augment the small training set while training a classifier on the small sample. To this end, our proposed method samples the learned latent space using spherical interpolation, and generates new examples using the trained generator. Empirical results show that the new sampled set is diverse enough, leading to improvement in image classification in comparison with the state of the art, when trained on small samples obtained from CIFAR-10, CIFAR-100, and CUB-200."},
{"task": "Joint Vertebrae Identification And Localization In Spinal Ct Images", "papers": {}},
{"paper": "Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features", "abstract": "Text contained in an image carries high-level semantics that can be exploited to achieve richer image understanding. In particular, the mere presence of text provides strong guiding content that should be employed to tackle a diversity of computer vision tasks such as image retrieval, fine-grained classification, and visual question answering.In this paper, we address the problem of fine-grained classification and image retrieval by leveraging textual information along with visual cues to comprehend the existing intrinsic relation between the two modalities. The novelty of the proposed model consists of the usage of a PHOC descriptor to construct a bag of textual words along with a Fisher Vector Encoding that captures the morphology of text. This approach provides a stronger multimodal representation for this task and as our experiments demonstrate, it achieves state-of-the-art results on two different tasks, fine-grained classification and image retrieval."},
{"paper": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "abstract": "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet.A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels."},
{"paper": "DOSED: a deep learning approach to detect multiple sleep micro-events in EEG signal", "abstract": "Background: Electroencephalography (EEG) monitors brain activity during sleep\nand is used to identify sleep disorders. In sleep medicine, clinicians\ninterpret raw EEG signals in so-called sleep stages, which are assigned by\nexperts to every 30s window of signal.For diagnosis, they also rely on shorter\nprototypical micro-architecture events which exhibit variable durations and\nshapes, such as spindles, K-complexes or arousals. Annotating such events is\ntraditionally performed by a trained sleep expert, making the process time\nconsuming, tedious and subject to inter-scorer variability. To automate this\nprocedure, various methods have been developed, yet these are event-specific\nand rely on the extraction of hand-crafted features. New method: We propose a novel deep learning architecure called Dreem One\nShot Event Detector (DOSED). DOSED jointly predicts locations, durations and\ntypes of events in EEG time series. The proposed approach, applied here on\nsleep related micro-architecture events, is inspired by object detectors\ndeveloped for computer vision such as YOLO and SSD. It relies on a\nconvolutional neural network that builds a feature representation from raw EEG\nsignals, as well as two modules performing localization and classification\nrespectively. Results and comparison with other methods: The proposed approach is tested on\n4 datasets and 3 types of events (spindles, K-complexes, arousals) and compared\nto the current state-of-the-art detection algorithms. Conclusions: Results demonstrate the versatility of this new approach and\nimproved performance compared to the current state-of-the-art detection\nmethods."},
{"paper": "Fine-Tuning DARTS for Image Classification", "abstract": "Neural Architecture Search (NAS) has gained attraction due to superior classification performance. Differential Architecture Search (DARTS) is a computationally light method.To limit computational resources DARTS makes numerous approximations. These approximations result in inferior performance. We propose to fine-tune DARTS using fixed operations as they are independent of these approximations. Our method offers a good trade-off between the number of parameters and classification accuracy. Our approach improves the top-1 accuracy on Fashion-MNIST, CompCars, and MIO-TCD datasets by 0.56%, 0.50%, and 0.39%, respectively compared to the state-of-the-art approaches. Our approach performs better than DARTS, improving the accuracy by 0.28%, 1.64%, 0.34%, 4.5%, and 3.27% compared to DARTS, on CIFAR-10, CIFAR-100, Fashion-MNIST, CompCars, and MIO-TCD datasets, respectively."},
{"paper": "Learning Attentive Pairwise Interaction for Fine-Grained Classification", "abstract": "Fine-grained classification is a challenging problem, due to subtle differences among highly-confused categories. Most approaches address this difficulty by learning discriminative representation of individual input image.On the other hand, humans can effectively identify contrastive clues by comparing image pairs. Inspired by this fact, this paper proposes a simple but effective Attentive Pairwise Interaction Network (API-Net), which can progressively recognize a pair of fine-grained images by interaction. Specifically, API-Net first learns a mutual feature vector to capture semantic differences in the input pair. It then compares this mutual vector with individual vectors to generate gates for each input image. These distinct gate vectors inherit mutual context on semantic differences, which allow API-Net to attentively capture contrastive clues by pairwise interaction between two images. Additionally, we train API-Net in an end-to-end manner with a score ranking regularization, which can further generalize API-Net by taking feature priorities into account. We conduct extensive experiments on five popular benchmarks in fine-grained classification. API-Net outperforms the recent SOTA methods, i.e., CUB-200-2011 (90.0%), Aircraft(93.9%), Stanford Cars (95.3%), Stanford Dogs (90.3%), and NABirds (88.1%)."},
{"paper": "RED: Deep Recurrent Neural Networks for Sleep EEG Event Detection", "abstract": "The brain electrical activity presents several short events during sleep that can be observed as distinctive micro-structures in the electroencephalogram (EEG), such as sleep spindles and K-complexes. These events have been associated with biological processes and neurological disorders, making them a research topic in sleep medicine.However, manual detection limits their study because it is time-consuming and affected by significant inter-expert variability, motivating automatic approaches. We propose a deep learning approach based on convolutional and recurrent neural networks for sleep EEG event detection called Recurrent Event Detector (RED). RED uses one of two input representations: a) the time-domain EEG signal, or b) a complex spectrogram of the signal obtained with the Continuous Wavelet Transform (CWT). Unlike previous approaches, a fixed time window is avoided and temporal context is integrated to better emulate the visual criteria of experts. When evaluated on the MASS dataset, our detectors outperform the state of the art in both sleep spindle and K-complex detection with a mean F1-score of at least 80.9% and 82.6%, respectively. Although the CWT-domain model obtained a similar performance than its time-domain counterpart, the former allows in principle a more interpretable input representation due to the use of a spectrogram. The proposed approach is event-agnostic and can be used directly to detect other types of sleep events."},
{"paper": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup", "abstract": "While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed.However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available at https://github.com/snu-mllab/PuzzleMix."},
{"paper": "Rethinking Recurrent Neural Networks and other Improvements for Image Classification", "abstract": "For a long history of Machine Learning which dates back to several decades, Recurrent Neural Networks (RNNs) have been mainly used for sequential data and time series or generally 1D information. Even in some rare researches on 2D images, the networks merely learn and generate data sequentially rather than for recognition of images.In this research, we propose to integrate RNN as an additional layer in designing image recognition's models. Moreover, we develop End-to-End Ensemble Multi-models that are able to learn experts' predictions from several models. Besides, we extend training strategy and softmax pruning which overall leads our designs to perform comparably to top models on several datasets. The source code of the methods provided in this article is available in https://github.com/leonlha/e2e-3m and http://nguyenhuuphong.me."},
{"paper": "Capsule Routing via Variational Bayes", "abstract": "Capsule networks are a recently proposed type of neural network shown to outperform alternatives in challenging shape recognition tasks. In capsule networks, scalar neurons are replaced with capsule vectors or matrices, whose entries represent different properties of objects.The relationships between objects and their parts are learned via trainable viewpoint-invariant transformation matrices, and the presence of a given object is decided by the level of agreement among votes from its parts. This interaction occurs between capsule layers and is a process called routing-by-agreement. In this paper, we propose a new capsule routing algorithm derived from Variational Bayes for fitting a mixture of transforming gaussians, and show it is possible transform our capsule network into a Capsule-VAE. Our Bayesian approach addresses some of the inherent weaknesses of MLE based models such as the variance-collapse by modelling uncertainty over capsule pose parameters. We outperform the state-of-the-art on smallNORB using 50% fewer capsules than previously reported, achieve competitive performances on CIFAR-10, Fashion-MNIST, SVHN, and demonstrate significant improvement in MNIST to affNIST generalisation over previous works."},
{"paper": "Direction Concentration Learning: Enhancing Congruency in Machine Learning", "abstract": "One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency.Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency."},
{"paper": "Dynamic Routing Between Capsules", "abstract": "A capsule is a group of neurons whose activity vector represents the\ninstantiation parameters of a specific type of entity such as an object or an\nobject part. We use the length of the activity vector to represent the\nprobability that the entity exists and its orientation to represent the\ninstantiation parameters.Active capsules at one level make predictions, via\ntransformation matrices, for the instantiation parameters of higher-level\ncapsules. When multiple predictions agree, a higher level capsule becomes\nactive. We show that a discrimininatively trained, multi-layer capsule system\nachieves state-of-the-art performance on MNIST and is considerably better than\na convolutional net at recognizing highly overlapping digits. To achieve these\nresults we use an iterative routing-by-agreement mechanism: A lower-level\ncapsule prefers to send its output to higher level capsules whose activity\nvectors have a big scalar product with the prediction coming from the\nlower-level capsule."},
{"paper": "Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network", "abstract": "Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon.In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g. ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3\\% to 82.78\\%, mCE from 76.0\\% to 48.9\\% and mFR from 57.7\\% to 32.3\\% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at https://github.com/clovaai/assembled-cnn"},
{"paper": "Intra- and Inter-epoch Temporal Context Network (IITNet) Using Sub-epoch Features for Automatic Sleep Scoring on Raw Single-channel EEG", "abstract": "A deep learning model, named IITNet, is proposed to learn intra- and inter-epoch temporal contexts from raw single-channel EEG for automatic sleep scoring. To classify the sleep stage from half-minute EEG, called an epoch, sleep experts investigate sleep-related events and consider the transition rules between the found events.Similarly, IITNet extracts representative features at a sub-epoch level by a residual neural network and captures intra- and inter-epoch temporal contexts from the sequence of the features via bidirectional LSTM. The performance was investigated for three datasets as the sequence length (L) increased from one to ten. IITNet achieved the comparable performance with other state-of-the-art results. The best accuracy, MF1, and Cohen's kappa ($\\kappa$) were 83.9%, 77.6%, 0.78 for SleepEDF (L=10), 86.5%, 80.7%, 0.80 for MASS (L=9), and 86.7%, 79.8%, 0.81 for SHHS (L=10), respectively. Even though using four epochs, the performance was still comparable. Compared to using a single epoch, on average, accuracy and MF1 increased by 2.48%p and 4.90%p and F1 of N1, N2, and REM increased by 16.1%p, 1.50%p, and 6.42%p, respectively. Above four epochs, the performance improvement was not significant. The results support that considering the latest two-minute raw single-channel EEG can be a reasonable choice for sleep scoring via deep neural networks with efficiency and reliability. Furthermore, the experiments with the baselines showed that introducing intra-epoch temporal context learning with a deep residual network contributes to the improvement in the overall performance and has the positive synergy effect with the inter-epoch temporal context learning."},
{"paper": "CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise", "abstract": "In this paper, we study the problem of learning image classification models\nwith label noise. Existing approaches depending on human supervision are\ngenerally not scalable as manually identifying correct or incorrect labels is\ntime-consuming, whereas approaches not relying on human supervision are\nscalable but less effective.To reduce the amount of human supervision for\nlabel noise cleaning, we introduce CleanNet, a joint neural embedding network,\nwhich only requires a fraction of the classes being manually verified to\nprovide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network\nclassifier into one framework for image classification learning. We demonstrate\nthe effectiveness of the proposed algorithm on both of the label noise\ndetection task and the image classification on noisy data task on several\nlarge-scale datasets. Experimental results show that CleanNet can reduce label\nnoise detection error rate on held-out classes where no human supervision\navailable by 41.5% compared to current weakly supervised methods. It also\nachieves 47% of the performance gain of verifying all images with only 3.2%\nimages verified on an image classification task. Source code and dataset will\nbe available at kuanghuei.github.io/CleanNetProject."},
{"paper": "Webly Supervised Image Classification with Self-Contained Confidence", "abstract": "This paper focuses on webly supervised learning (WSL), where datasets are built by crawling samples from the Internet and directly using search queries as web labels. Although WSL benefits from fast and low-cost data collection, noises in web labels hinder better performance of the image classification model.To alleviate this problem, in recent works, self-label supervised loss $\\mathcal{L}_s$ is utilized together with webly supervised loss $\\mathcal{L}_w$. $\\mathcal{L}_s$ relies on pseudo labels predicted by the model itself. Since the correctness of the web label or pseudo label is usually on a case-by-case basis for each web sample, it is desirable to adjust the balance between $\\mathcal{L}_s$ and $\\mathcal{L}_w$ on sample level. Inspired by the ability of Deep Neural Networks (DNNs) in confidence prediction, we introduce Self-Contained Confidence (SCC) by adapting model uncertainty for WSL setting, and use it to sample-wisely balance $\\mathcal{L}_s$ and $\\mathcal{L}_w$. Therefore, a simple yet effective WSL framework is proposed. A series of SCC-friendly regularization approaches are investigated, among which the proposed graph-enhanced mixup is the most effective method to provide high-quality confidence to enhance our framework. The proposed WSL framework has achieved the state-of-the-art results on two large-scale WSL datasets, WebVision-1000 and Food101-N. Code is available at https://github.com/bigvideoresearch/SCC."},
{"paper": "Null-sampling for Interpretable and Fair Representations", "abstract": "We propose to learn invariant representations, in the data domain, to achieve interpretability in algorithmic fairness. Invariance implies a selectivity for high level, relevant correlations w.r.t.class label annotations, and a robustness to irrelevant correlations with protected characteristics such as race or gender. We introduce a non-trivial setup in which the training set exhibits a strong bias such that class label annotations are irrelevant and spurious correlations cannot be distinguished. To address this problem, we introduce an adversarially trained model with a null-sampling procedure to produce invariant representations in the data domain. To enable disentanglement, a partially-labelled representative set is used. By placing the representations into the data domain, the changes made by the model are easily examinable by human auditors. We show the effectiveness of our method on both image and tabular datasets: Coloured MNIST, the CelebA and the Adult dataset."},
{"paper": "Edge-labeling Graph Neural Network for Few-shot Learning", "abstract": "In this paper, we propose a novel edge-labeling graph neural network (EGNN), which adapts a deep neural network on the edge-labeling graph, for few-shot learning. The previous graph neural network (GNN) approaches in few-shot learning have been based on the node-labeling framework, which implicitly models the intra-cluster similarity and the inter-cluster dissimilarity.In contrast, the proposed EGNN learns to predict the edge-labels rather than the node-labels on the graph that enables the evolution of an explicit clustering by iteratively updating the edge-labels with direct exploitation of both intra-cluster similarity and the inter-cluster dissimilarity. It is also well suited for performing on various numbers of classes without retraining, and can be easily extended to perform a transductive inference. The parameters of the EGNN are learned by episodic training with an edge-labeling loss to obtain a well-generalizable model for unseen low-data problem. On both of the supervised and semi-supervised few-shot image classification tasks with two benchmark datasets, the proposed EGNN significantly improves the performances over the existing GNNs."},
{"paper": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks.Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix ."},
{"paper": "A Branching and Merging Convolutional Network with Homogeneous Filter Capsules", "abstract": "We present a convolutional neural network design with additional branches after certain convolutions so that we can extract features with differing effective receptive fields and levels of abstraction. From each branch, we transform each of the final filters into a pair of homogeneous vector capsules.As the capsules are formed from entire filters, we refer to them as filter capsules. We then compare three methods for merging the branches--merging with equal weight and merging with learned weights, with two different weight initialization strategies. This design, in combination with a domain-specific set of randomly applied augmentation techniques, establishes a new state of the art for the MNIST dataset with an accuracy of 99.84% for an ensemble of these models, as well as establishing a new state of the art for a single model (99.79% accurate). These accuracies were achieved with a 75% reduction in both the number of parameters and the number of epochs of training relative to the previously best performing capsule network on MNIST. All training was performed using the Adam optimizer and experienced no overfitting."},
{"paper": "Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization", "abstract": "ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is one of the most authoritative academic competitions in the field of Computer Vision (CV) in recent years. But applying ILSVRC's annual champion directly to fine-grained visual categorization (FGVC) tasks does not achieve good performance.To FGVC tasks, the small inter-class variations and the large intra-class variations make it a challenging problem. Our attention object location module (AOLM) can predict the position of the object and attention part proposal module (APPM) can propose informative part regions without the need of bounding-box or part annotations. The obtained object images not only contain almost the entire structure of the object, but also contains more details, part images have many different scales and more fine-grained features, and the raw images contain the complete object. The three kinds of training images are supervised by our multi-branch network. Therefore, our multi-branch and multi-scale learning network(MMAL-Net) has good classification ability and robustness for images of different scales. Our approach can be trained end-to-end, while provides short inference time. Through the comprehensive experiments demonstrate that our approach can achieves state-of-the-art results on CUB-200-2011, FGVC-Aircraft and Stanford Cars datasets. Our code will be available at https://github.com/ZF1044404254/MMAL-Net"},
{"paper": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "abstract": "In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality.Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels."},
{"paper": "End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition", "abstract": "Part-based approaches for fine-grained recognition do not show the expected performance gain over global methods, although being able to explicitly focus on small details that are relevant for distinguishing highly similar classes. We assume that part-based methods suffer from a missing representation of local features, which is invariant to the order of parts and can handle a varying number of visible parts appropriately.The order of parts is artificial and often only given by ground-truth annotations, whereas viewpoint variations and occlusions result in parts that are not observable. Therefore, we propose integrating a Fisher vector encoding of part features into convolutional neural networks. The parameters for this encoding are estimated jointly with those of the neural network in an end-to-end manner. Our approach improves state-of-the-art accuracies for bird species classification on CUB-200-2011 from 90.40\\% to 90.95\\%, on NA-Birds from 89.20\\% to 90.30\\%, and on Birdsnap from 84.30\\% to 86.97\\%."},
{"paper": "Big Transfer (BiT): General Visual Representation Learning", "abstract": "Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task.We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance."},
{"paper": "Domain Adaptive Transfer Learning with Specialist Models", "abstract": "Transfer learning is a widely used method to build high performing computer\nvision models. In this paper, we study the efficacy of transfer learning by\nexamining how the choice of data impacts performance.We find that more\npre-training data does not always help, and transfer performance depends on a\njudicious choice of pre-training data. These findings are important given the\ncontinued increase in dataset sizes. We further propose domain adaptive\ntransfer learning, a simple and effective pre-training method using importance\nweights computed based on the target dataset. Our method to compute importance\nweights follow from ideas in domain adaptation, and we show a novel application\nto transfer learning. Our methods achieve state-of-the-art results on multiple\nfine-grained classification datasets and are well-suited for use in practice."},
{"paper": "Deepsleep: Fast and Accurate Delineation of Sleep Arousals at Millisecond Resolution by Deep Learning", "abstract": "Background: Sleep arousals are transient periods of wakefulness punctuated into sleep. Excessive sleep arousals are associated with many negative effects including daytime sleepiness and sleep disorders.High-quality annotation of polysomnographic recordings is crucial for the diagnosis of sleep arousal disorders. Currently, sleep arousals are mainly annotated by human experts through looking at millions of data points manually, which requires considerable time and effort. Methods: We used the polysomnograms of 2,994 individuals from two independent datasets (i) PhysioNet Challenge dataset (n=994), and (ii) Sleep Heart Health Study dataset (n=2000) for model training (60%), validation (15%), and testing (25%). We developed a deep convolutional neural network approach, DeepSleep, to automatically segment sleep arousal events. Our method captured the long-range and short-range interactions among physiological signals at multiple time scales to empower the detection of sleep arousals. A novel augmentation strategy by randomly swapping similar physiological channels was further applied to improve the prediction accuracy. Findings: Compared with other computational methods in sleep study, DeepSleep features accurate (area under receiver operating characteristic curve of 0.93 and area under the precision recall curve of 0.55), high-resolution (5-millisecond resolution), and fast (10 seconds per sleep record) delineation of sleep arousals. This method ranked first in segmenting non-apenic arousals when evaluated on a large held-out dataset (n=989) in the 2018 PhysioNet Challenge. We found that DeepSleep provided more detailed delineations than humans, especially at the low-confident boundary regions between arousal and non-arousal events. This indicates that in silico annotations is a complement to human annotations and potentially advances the current binary label system and scoring criteria for sleep arousals. Interpretation: The proposed deep learning model achieved state-of-the-art performance in detection of sleep arousals. By introducing the probability of annotation confidence, this model would provide more accurate information for the diagnosis of sleep disorders and the evaluation of sleep quality."},
{"paper": "Sleep quality prediction in caregivers using physiological signals", "abstract": "Most caregivers of people with dementia (CPWD) experience a high degree of stress due to the demands of providing care, especially when addressing unpredictable behavioral and psychological symptoms of dementia. Such challenging responsibilities make caregivers susceptible to poor sleep quality with detrimental effects on their overall health.Hence, monitoring caregivers\u2019 sleep quality can provide important CPWD stress assessment. Most current sleep studies are based on polysomnography, which is expensive and potentially disrupts the caregiving routine. To address these issues, we propose a clinical decision support system to predict sleep quality based on trends of physiological signals in the deep sleep stage. This system utilizes four raw physiological signals using a wearable device (E4 wristband): heart rate variability, electrodermal activity, body movement, and skin temperature. To evaluate the performance of the proposed method, analyses were conducted on a two-week period of sleep monitored on eight CPWD. The best performance is achieved using the random forest classifier with an accuracy of 75% for sleep quality, and 73% for restfulness, respectively. We found that the most important features to detect these measures are sleep efficiency (ratio of amount of time asleep to the amount of time in bed) and skin temperature. The results from our sleep analysis system demonstrate the capability of using wearable sensors to measure sleep quality and restfulness in CPWD."},
{"paper": "SpinalNet: Deep Neural Network with Gradual Input", "abstract": "Over the past few years, deep neural networks (DNNs) have garnered remarkable success in a diverse range of real-world applications. However, DNNs consider a large number of inputs and consist of a large number of parameters, resulting in high computational demand.We study the human somatosensory system and propose the SpinalNet to achieve higher accuracy with less computational resources. In a typical neural network (NN) architecture, the hidden layers receive inputs in the first layer and then transfer the intermediate outcomes to the next layer. In the proposed SpinalNet, the structure of hidden layers allocates to three sectors: 1) Input row, 2) Intermediate row, and 3) output row. The intermediate row of the SpinalNet contains a few neurons. The role of input segmentation is in enabling each hidden layer to receive a part of the inputs and outputs of the previous layer. Therefore, the number of incoming weights in a hidden layer is significantly lower than traditional DNNs. As all layers of the SpinalNet directly contributes to the output row, the vanishing gradient problem does not exist. We also investigate the SpinalNet fully-connected layer to several well-known DNN models and perform traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. We have also obtained the state-of-the-art (SOTA) performance for QMNIST, Kuzushiji-MNIST, EMNIST (Letters, Digits, and Balanced), STL-10, Bird225, Fruits 360, and Caltech-101 datasets. The scripts of the proposed SpinalNet are available with the following link: https://github.com/dipuk0506/SpinalNet"},
{"paper": "Fixing the train-test resolution discrepancy", "abstract": "Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time.We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time. We then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images. Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date."},
{"paper": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc. ), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."},
{"paper": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank", "abstract": "Deep Convolutional features extracted from a comprehensive labeled dataset,\ncontain substantial representations which could be effectively used in a new\ndomain. Despite the fact that generic features achieved good results in many\nvisual tasks, fine-tuning is required for pretrained deep CNN models to be more\neffective and provide state-of-the-art performance.Fine tuning using the\nbackpropagation algorithm in a supervised setting, is a time and resource\nconsuming process. In this paper, we present a new architecture and an approach\nfor unsupervised object recognition that addresses the above mentioned problem\nwith fine tuning associated with pretrained CNN-based supervised deep learning\napproaches while allowing automated feature extraction. Unlike existing works,\nour approach is applicable to general object recognition tasks. It uses a\npretrained (on a related domain) CNN model for automated feature extraction\npipelined with a Hopfield network based associative memory bank for storing\npatterns for classification purposes. The use of associative memory bank in our\nframework allows eliminating backpropagation while providing competitive\nperformance on an unseen dataset."},
{"task": "Bone Suppression From Dual Energy Chest X-Rays", "papers": {}},
{"task": "Low-Dose X-Ray Ct Reconstruction", "papers": {}},
{"paper": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "abstract": "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling.Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch."},
{"paper": "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", "abstract": "Deep neural networks have been successfully applied to many real-world applications. However, these successes rely heavily on large amounts of labeled data, which is expensive to obtain.Recently, Auto-Encoding Transformation (AET) and MixMatch have been proposed and achieved state-of-the-art results for unsupervised and semi-supervised learning, respectively. In this study, we train an Ensemble of Auto-Encoding Transformations (EnAET) to learn from both labeled and unlabeled data based on the embedded representations by decoding both spatial and non-spatial transformations. This distinguishes EnAET from conventional semi-supervised methods that focus on improving prediction consistency and confidence by different models on both unlabeled and labeled examples. In contrast, we propose to explore the role of self-supervised representations in semi-supervised learning under a rich family of transformations. Experiment results on CIFAR-10, CIFAR-100, SVHN and STL10 demonstrate that the proposed EnAET outperforms the state-of-the-art semi-supervised methods by significant margins. In particular, we apply the proposed method to extremely challenging scenarios with only 10 images per class, and show that EnAET can achieve an error rate of 9.35% on CIFAR-10 and 16.92% on SVHN. In addition, EnAET achieves the best result when compared with fully supervised learning using all labeled data with the same network architecture. The performance on CIFAR-10, CIFAR-100 and SVHN with a smaller network is even more competitive than the state-of-the-art of supervised learning methods based on a larger network. We also set a new performance record with an error rate of 1.99% on CIFAR-10 and 4.52% on STL10. The code and experiment records are released at https://github.com/maple-research-lab/EnAET."},
{"task": "Medical X-Ray Image Segmentation", "papers": {}},
{"paper": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "abstract": "Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp.We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success."},
{"paper": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring", "abstract": "We improve the recently-proposed \"MixMatch\" semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels.Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between $5\\times$ and $16\\times$ less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\\%$ accuracy (compared to MixMatch's accuracy of $93.58\\%$ with $4{,}000$ examples) and a median accuracy of $84.92\\%$ with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch."},
{"task": "Fish Detection", "papers": {}},
{"task": "Multiple Affordance Detection", "papers": {}},
{"task": "Semantic Part Detection", "papers": {"0": "Attention-based Joint Detection of Object and Semantic Part"}},
{"task": "Object Skeleton Detection", "papers": {"0": "DeepFlux for Skeletons in the Wild"}},
{"task": "Medical Object Detection", "papers": {"0": "Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides"}},
{"task": "Zero-Shot Object Detection", "papers": {"0": "Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts", "1": "Polarity Loss for Zero-shot Object Detection"}},
{"task": "One-Shot Object Detection", "papers": {"0": "One-Shot Object Detection with Co-Attention and Co-Excitation"}},
{"task": "Camouflaged Object Segmentation", "papers": {"0": "MirrorNet: Bio-Inspired Camouflaged Object Segmentation", "1": "Camouflaged Object Detection"}},
{"task": "Head Detection", "papers": {"0": "Segmentation is All You Need"}},
{"task": "Co-Salient Object Detection", "papers": {"0": "Gradient-Induced Co-Saliency Detection", "1": "Gradient-Induced Co-Saliency Detection", "2": "Taking a Deeper Look at Co-Salient Object Detection", "3": "Taking a Deeper Look at Co-Salient Object Detection"}},
{"task": "Robust Object Detection", "papers": {"0": "Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming", "1": "Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming", "2": "Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming"}},
{"task": "Dense Object Detection", "papers": {"0": "A Solution to Product detection in Densely Packed Scenes"}},
{"task": "Small Object Detection", "papers": {}},
{"paper": "Attention-based Joint Detection of Object and Semantic Part", "abstract": "In this paper, we address the problem of joint detection of objects like dog and its semantic parts like face, leg, etc. Our model is created on top of two Faster-RCNN models that share their features to perform a novel Attention-based feature fusion of related Object and Part features to get enhanced representations of both.These representations are used for final classification and bounding box regression separately for both models. Our experiments on the PASCAL-Part 2010 dataset show that joint detection can simultaneously improve both object detection and part detection in terms of mean Average Precision (mAP) at IoU=0.5."},
{"task": "Video Salient Object Detection", "papers": {"0": "Shifting More Attention to Video Salient Object Detection", "1": "Shifting More Attention to Video Salient Object Detection", "2": "Shifting More Attention to Video Salient Object Detection", "3": "Shifting More Attention to Video Salient Object Detection", "4": "Shifting More Attention to Video Salient Object Detection", "5": "Shifting More Attention to Video Salient Object Detection", "6": "Pyramid Dilated Deeper ConvLSTM for Video Salient Object Detection", "7": "Pyramid Dilated Deeper ConvLSTM for Video Salient Object Detection", "8": "Shifting More Attention to Video Salient Object Detection", "9": "Pyramid Dilated Deeper ConvLSTM for Video Salient Object Detection"}},
{"task": "Object Detection In Aerial Images", "papers": {"0": "Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery"}},
{"task": "Weakly Supervised Object Detection", "papers": {"0": "Instance-aware, Context-focused, and Memory-efficient Weakly Supervised Object Detection", "1": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection", "2": "Activity Driven Weakly Supervised Object Detection", "3": "PCL: Proposal Cluster Learning for Weakly Supervised Object Detection", "4": "Few-Example Object Detection with Model Communication", "5": "Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation", "6": "Activity Driven Weakly Supervised Object Detection", "7": "Instance-aware, Context-focused, and Memory-efficient Weakly Supervised Object Detection", "8": "Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts", "9": "Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts", "10": "Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts", "11": "Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts", "12": "Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts"}},
{"task": "Object Proposal Generation", "papers": {"0": "Recurrent Pixel Embedding for Instance Grouping"}},
{"task": "Few-Shot Object Detection", "papers": {"0": "Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild"}},
{"paper": "Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides", "abstract": "Deep learning-based methods, such as the sliding window approach for cropped-image classification and heuristic aggregation for whole-slide inference, for analyzing histological patterns in high-resolution microscopy images have shown promising results. These approaches, however, require a laborious annotation process and are fragmented.This diagnostic study collected deidentified high-resolution histological images (N = 379) for training a new model composed of a convolutional neural network and a grid-based attention network, trainable without region-of-interest annotations. Histological images of patients who underwent endoscopic esophagus and gastroesophageal junction mucosal biopsy between January 1, 2016, and December 31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were collected. The method achieved a mean accuracy of 0.83 in classifying 123 test images. These results were comparable with or better than the performance from the current state-of-the-art sliding window approach, which was trained with regions of interest. Results of this study suggest that the proposed attention-based deep neural network framework for Barrett esophagus and esophageal adenocarcinoma detection is important because it is based solely on tissue-level annotations, unlike existing methods that are based on regions of interest. This new model is expected to open avenues for applying deep learning to digital pathology."},
{"paper": "Polarity Loss for Zero-shot Object Detection", "abstract": "Conventional object detection models require large amounts of training data. In comparison, humans can recognize previously unseen objects by merely knowing their semantic description.To mimic similar behaviour, zero-shot object detection aims to recognize and localize 'unseen' object instances by using only their semantic information. The model is first trained to learn the relationships between visual and semantic domains for seen objects, later transferring the acquired knowledge to totally unseen objects. This setting gives rise to the need for correct alignment between visual and semantic concepts, so that the unseen objects can be identified using only their semantic attributes. In this paper, we propose a novel loss function called 'Polarity loss', that promotes correct visual-semantic alignment for an improved zero-shot object detection. On one hand, it refines the noisy semantic embeddings via metric learning on a 'Semantic vocabulary' of related concepts to establish a better synergy between visual and semantic domains. On the other hand, it explicitly maximizes the gap between positive and negative predictions to achieve better discrimination between seen, unseen and background objects. Our approach is inspired by embodiment theories in cognitive science, that claim human semantic understanding to be grounded in past experiences (seen objects), related linguistic concepts (word vocabulary) and visual perception (seen/unseen object images). We conduct extensive evaluations on MS-COCO and Pascal VOC datasets, showing significant improvements over state of the art."},
{"task": "Video Object Detection", "papers": {"0": "Mining Inter-Video Proposal Relations for Video Object Detection"}},
{"paper": "One-Shot Object Detection with Co-Attention and Co-Excitation", "abstract": "This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image.To this end, we develop a novel {\\em co-attention and co-excitation} (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection."},
{"paper": "Neural Architecture Transfer", "abstract": "Neural architecture search (NAS) has emerged as a promising avenue for automatically designing task-specific neural networks. Most existing NAS approaches require one complete search for each deployment specification of hardware or objective.This is a computationally impractical endeavor given the potentially large number of application scenarios. In this paper, we propose Neural Architecture Transfer (NAT) to overcome this limitation. NAT is designed to efficiently generate task-specific custom models that are competitive even under multiple conflicting objectives. To realize this goal we learn task-specific supernets from which specialized subnets can be sampled without any additional training. The key to our approach is an integrated online transfer learning and many-objective evolutionary search procedure. A pre-trained supernet is iteratively adapted while simultaneously searching for task-specific subnets. We demonstrate the efficacy of NAT on 11 benchmark image classification tasks ranging from large-scale multi-class to small-scale fine-grained datasets. In all cases, including ImageNet, NATNets improve upon the state-of-the-art under mobile settings ($\\leq$ 600M Multiply-Adds). Surprisingly, small-scale fine-grained datasets benefit the most from NAT. At the same time, the architecture search and transfer is orders of magnitude more efficient than existing NAS methods. Overall, experimental evaluation indicates that across diverse image classification tasks and computational objectives, NAT is an appreciably more effective alternative to fine-tuning based transfer learning. Code is available at https://github.com/human-analysis/neural-architecture-transfer"},
{"paper": "Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts", "abstract": "Current Zero-Shot Learning (ZSL) approaches are restricted to recognition of\na single dominant unseen object category in a test image. We hypothesize that\nthis setting is ill-suited for real-world applications where unseen objects\nappear only as a part of a complex scene, warranting both the `recognition' and\n`localization' of an unseen category.To address this limitation, we introduce\na new \\emph{`Zero-Shot Detection'} (ZSD) problem setting, which aims at\nsimultaneously recognizing and locating object instances belonging to novel\ncategories without any training examples. We also propose a new experimental\nprotocol for ZSD based on the highly challenging ILSVRC dataset, adhering to\npractical issues, e.g., the rarity of unseen objects. To the best of our\nknowledge, this is the first end-to-end deep network for ZSD that jointly\nmodels the interplay between visual and semantic domain information. To\novercome the noise in the automatically derived semantic descriptions, we\nutilize the concept of meta-classes to design an original loss function that\nachieves synergy between max-margin class separation and semantic space\nclustering. Furthermore, we present a baseline approach extended from\nrecognition to detection setting. Our extensive experiments show significant\nperformance boost over the baseline on the imperative yet difficult ZSD\nproblem."},
{"paper": "MirrorNet: Bio-Inspired Camouflaged Object Segmentation", "abstract": "Camouflaged objects are generally difficult to be detected in their natural environment even for human beings. In this paper, we propose a novel bio-inspired network, named the MirrorNet, that leverages both instance segmentation and mirror stream for the camouflaged object segmentation.Differently from existing networks for segmentation, our proposed network possesses two segmentation streams: the main stream and the mirror stream corresponding with the original image and its flipped image, respectively. The output from the mirror stream is then fused into the main stream's result for the final camouflage map to boost up the segmentation accuracy. Extensive experiments conducted on the public CAMO dataset demonstrate the effectiveness of our proposed network. Our proposed method achieves 89% in accuracy, outperforming the state-of-the-arts. Project Page: https://sites.google.com/view/ltnghia/research/camo"},
{"paper": "A Solution to Product detection in Densely Packed Scenes", "abstract": "This work is a solution to densely packed scenes dataset SKU-110k. Our work is modified from Cascade R-CNN.To solve the problem, we proposed a random crop strategy to ensure both the sampling rate and input scale is relatively sufficient as a contrast to the regular random crop. And we adopted some of trick and optimized the hyper-parameters. To grasp the essential feature of the densely packed scenes, we analysis the stages of a detector and investigate the bottleneck which limits the performance. As a result, our method obtains 58.7 mAP on test set of SKU-110k."},
{"paper": "Camouflaged Object Detection", "abstract": "We present a comprehensive study on a new task named camouflaged object detection (COD), which aims to identify objects that are \"seamlessly\" embedded in their surroundings. The high intrinsic similarities between the target object and the background make COD far more challenging than the traditional object detection task.To address this issue, we elaborately collect a novel dataset, called COD10K, which comprises 10,000 images covering camouflaged objects in various natural scenes, over 78 object categories. All the images are densely annotated with category, bounding-box, object-/instance-level, and matting-level labels. This dataset could serve as a catalyst for progressing many vision tasks, e.g., localization, segmentation, and alpha-matting, etc. In addition, we develop a simple but effective framework for COD, termed Search Identification Network (SINet). Without any bells and whistles, SINet outperforms various state-of-the-art object detection baselines on all datasets tested, making it a robust, general framework that can help facilitate future research in COD. Finally, we conduct a large-scale COD study, evaluating 13 cutting-edge models, providing some interesting findings, and showing several potential applications. Our research offers the community an opportunity to explore more in this new field. The code will be available at https://github.com/DengPingFan/SINet/."},
{"paper": "Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming", "abstract": "The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades.The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30--60\\% of the original performance). However, a simple data augmentation trick---stylizing the training images---leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available."},
{"paper": "Segmentation is All You Need", "abstract": "Region proposal mechanisms are essential for existing deep learning approaches to object detection in images. Although they can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases is unacceptably low.This is mainly because bounding box annotations contain much environment noise information, and non-maximum suppression (NMS) is required to select target boxes. Therefore, in this paper, we propose the first anchor-free and NMS-free object detection model called weakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes segmentation models to achieve an accurate and robust object detection without NMS. In WSMA-Seg, multimodal annotations are proposed to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects. In addition, we propose a multi-scale pooling segmentation (MSP-Seg) as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors."},
{"paper": "Gradient-Induced Co-Saliency Detection", "abstract": "Co-saliency detection (Co-SOD) aims to segment the common salient foreground in a group of relevant images. In this paper, inspired by human behavior, we propose a gradient-induced co-saliency detection (GICD) method.We first abstract a consensus representation for the grouped images in the embedding space; then, by comparing the single image with consensus representation, we utilize the feedback gradient information to induce more attention to the discriminative co-salient features. In addition, due to the lack of Co-SOD training data, we design a jigsaw training strategy, with which Co-SOD networks can be trained on general saliency datasets without extra pixel-level annotations. To evaluate the performance of Co-SOD methods on discovering the co-salient object among multiple foregrounds, we construct a challenging CoCA dataset, where each image contains at least one extraneous foreground along with the co-salient object. Experiments demonstrate that our GICD achieves state-of-the-art performance. Our codes and dataset are available at https://mmcheng.net/gicd/."},
{"paper": "Taking a Deeper Look at Co-Salient Object Detection", "abstract": "Co-salient object detection (CoSOD) is a newly emerging and rapidly growing branch of salient object detection (SOD), which aims to detect the co-occurring salient objects in multiple images. However, existing CoSOD datasets often have a serious data bias, which assumes that each group of images contains salient objects of similar visual appearances.This bias results in the ideal settings and the effectiveness of the models, trained on existing datasets, may be impaired in real-life situations, where the similarity is usually semantic or conceptual. To tackle this issue, we first collect a new high-quality dataset, named CoSOD3k, which contains 3,316 images divided into 160 groups with multiple level annotations, i.e., category, bounding box, object, and instance levels. CoSOD3k makes a significant leap in terms of diversity, difficulty and scalability, benefiting related vision tasks. Besides, we comprehensively summarize 34 cutting-edge algorithms, benchmarking 19 of them over four existing CoSOD datasets (MSRC, iCoSeg, Image Pair and CoSal2015) and our CoSOD3k with a total of  61K images (largest scale), and reporting group-level performance analysis. Finally, we discuss the challenge and future work of CoSOD. Our study would give a strong boost to growth in the CoSOD community. Benchmark toolbox and results are available on our project page."},
{"paper": "Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery", "abstract": "Automatic multi-class object detection in remote sensing images in\nunconstrained scenarios is of high interest for several applications including\ntraffic monitoring and disaster management. The huge variation in object scale,\norientation, category, and complex backgrounds, as well as the different camera\nsensors pose great challenges for current algorithms.In this work, we propose\na new method consisting of a novel joint image cascade and feature pyramid\nnetwork with multi-size convolution kernels to extract multi-scale strong and\nweak semantic features. These features are fed into rotation-based region\nproposal and region of interest networks to produce object detections. Finally,\nrotational non-maximum suppression is applied to remove redundant detections. During training, we minimize joint horizontal and oriented bounding box loss\nfunctions, as well as a novel loss that enforces oriented boxes to be\nrectangular. Our method achieves 68.16% mAP on horizontal and 72.45% mAP on\noriented bounding box detection tasks on the challenging DOTA dataset,\noutperforming all published methods by a large margin (+6% and +12% absolute\nimprovement, respectively). Furthermore, it generalizes to two other datasets,\nNWPU VHR-10 and UCAS-AOD, and achieves competitive results with the baselines\neven when trained on DOTA. Our method can be deployed in multi-class object\ndetection applications, regardless of the image and object scales and\norientations, making it a great choice for unconstrained aerial and satellite\nimagery."},
{"paper": "Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts", "abstract": "Weakly supervised object detection (WSOD) using only image-level annotations has attracted a growing attention over the past few years. Whereas such task is typically addressed with a domain-specific solution focused on natural images, we show that a simple multiple instance approach applied on pre-trained deep features yields excellent performances on non-photographic datasets, possibly including new classes.The approach does not include any fine-tuning or cross-domain learning and is therefore efficient and possibly applicable to arbitrary datasets and classes. We investigate several flavors of the proposed approach, some including multi-layers perceptron and polyhedral classifiers. Despite its simplicity, our method shows competitive results on a range of publicly available datasets, including paintings (People-Art, IconArt), watercolors, cliparts and comics and allows to quickly learn unseen visual categories."},
{"paper": "Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild", "abstract": "Detecting objects and estimating their viewpoint in images are key tasks of 3D scene understanding. Recent approaches have achieved excellent results on very large benchmarks for object detection and viewpoint estimation.However, performances are still lagging behind for novel object categories with few samples. In this paper, we tackle the problems of few-shot object detection and few-shot viewpoint estimation. We propose a meta-learning framework that can be applied to both tasks, possibly including 3D data. Our models improve the results on objects of novel classes by leveraging on rich feature information originating from base classes with many samples. A simple joint feature embedding module is proposed to make the most of this feature sharing. Despite its simplicity, our method outperforms state-of-the-art methods by a large margin on a range of datasets, including PASCAL VOC and MS COCO for few-shot object detection, and Pascal3D+ and ObjectNet3D for few-shot viewpoint estimation. And for the first time, we tackle the combination of both few-shot tasks, on Object- Net3D, showing promising results. Our code and data are available at http://imagine.enpc.fr/~xiaoy/FSDetView/."},
{"paper": "Recurrent Pixel Embedding for Instance Grouping", "abstract": "We introduce a differentiable, end-to-end trainable framework for solving\npixel-level grouping problems such as instance segmentation consisting of two\nnovel components. First, we regress pixels into a hyper-spherical embedding\nspace so that pixels from the same group have high cosine similarity while\nthose from different groups have similarity below a specified margin.We\nanalyze the choice of embedding dimension and margin, relating them to\ntheoretical results on the problem of distributing points uniformly on the\nsphere. Second, to group instances, we utilize a variant of mean-shift\nclustering, implemented as a recurrent neural network parameterized by kernel\nbandwidth. This recurrent grouping module is differentiable, enjoys convergent\ndynamics and probabilistic interpretability. Backpropagating the group-weighted\nloss through this module allows learning to focus on only correcting embedding\nerrors that won't be resolved during subsequent clustering. Our framework,\nwhile conceptually simple and theoretically abundant, is also practically\neffective and computationally efficient. We demonstrate substantial\nimprovements over state-of-the-art instance segmentation for object proposal\ngeneration, as well as demonstrating the benefits of grouping loss for\nclassification tasks such as boundary detection and semantic segmentation."},
{"paper": "Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation", "abstract": "Can we detect common objects in a variety of image domains without\ninstance-level annotations? In this paper, we present a framework for a novel\ntask, cross-domain weakly supervised object detection, which addresses this\nquestion.For this paper, we have access to images with instance-level\nannotations in a source domain (e.g., natural image) and images with\nimage-level annotations in a target domain (e.g., watercolor). In addition, the\nclasses to be detected in the target domain are all or a subset of those in the\nsource domain. Starting from a fully supervised object detector, which is\npre-trained on the source domain, we propose a two-step progressive domain\nadaptation technique by fine-tuning the detector on two types of artificially\nand automatically generated samples. We test our methods on our newly collected\ndatasets containing three image domains, and achieve an improvement of\napproximately 5 to 20 percentage points in terms of mean average precision\n(mAP) compared to the best-performing baselines."},
{"paper": "Few-Example Object Detection with Model Communication", "abstract": "In this paper, we study object detection using a large pool of unlabeled\nimages and only a few labeled images per category, named \"few-example object\ndetection\". The key challenge consists in generating trustworthy training\nsamples as many as possible from the pool.Using few training examples as\nseeds, our method iterates between model training and high-confidence sample\nselection. In training, easy samples are generated first and, then the poorly\ninitialized model undergoes improvement. As the model becomes more\ndiscriminative, challenging but reliable samples are selected. After that,\nanother round of model improvement takes place. To further improve the\nprecision and recall of the generated training samples, we embed multiple\ndetection models in our framework, which has proven to outperform the single\nmodel baseline and the model ensemble method. Experiments on PASCAL VOC'07, MS\nCOCO'14, and ILSVRC'13 indicate that by using as few as three or four samples\nselected for each category, our method produces very competitive results when\ncompared to the state-of-the-art weakly-supervised approaches using a large\nnumber of image-level labels."},
{"paper": "PCL: Proposal Cluster Learning for Weakly Supervised Object Detection", "abstract": "Weakly Supervised Object Detection (WSOD), using only image-level annotations\nto train object detectors, is of growing importance in object recognition. In\nthis paper, we propose a novel deep network for WSOD.Unlike previous networks\nthat transfer the object detection problem to an image classification problem\nusing Multiple Instance Learning (MIL), our strategy generates proposal\nclusters to learn refined instance classifiers by an iterative process. The\nproposals in the same cluster are spatially adjacent and associated with the\nsame object. This prevents the network from concentrating too much on parts of\nobjects instead of whole objects. We first show that instances can be assigned\nobject or background labels directly based on proposal clusters for instance\nclassifier refinement, and then show that treating each cluster as a small new\nbag yields fewer ambiguities than the directly assigning label method. The\niterative instance classifier refinement is implemented online using multiple\nstreams in convolutional neural networks, where the first is an MIL network and\nthe others are for instance classifier refinement supervised by the preceding\none. Experiments are conducted on the PASCAL VOC, ImageNet detection, and\nMS-COCO benchmarks for WSOD. Results show that our method outperforms the\nprevious state of the art significantly."},
{"paper": "Mining Inter-Video Proposal Relations for Video Object Detection", "abstract": "Recent studies have shown that, context aggregating information from proposals in different frames can clearly enhance the performance of video object detection. However, these approaches mainly exploit the intra-proposal relation within single video, while ignoring the intra-proposal relation among different videos, which can provide important discriminative cues for recognizing confusing objects.To address the limitation, we propose a novel Inter-Video Proposal Relation module. Based on a concise multi-level triplet selection scheme, this module can learn effective object representations via modeling relations of hard proposals among different videos. Moreover, we design a Hierarchical Video Relation Network (HVR-Net), by integrating intra-video and inter-video proposal relations in a hierarchical fashion. This design can progressively exploit both intra and inter contexts to boost video object detection. We examine our method on the large-scale video object detection benchmark, i.e., ImageNet VID, where HVR-Net achieves the SOTA results. Codes and models will be released afterwards."},
{"paper": "Activity Driven Weakly Supervised Object Detection", "abstract": "Weakly supervised object detection aims at reducing the amount of supervision\nrequired to train detection models. Such models are traditionally learned from\nimages/videos labelled only with the object class and not the object bounding\nbox.In our work, we try to leverage not only the object class labels but also\nthe action labels associated with the data. We show that the action depicted in\nthe image/video can provide strong cues about the location of the associated\nobject. We learn a spatial prior for the object dependent on the action (e.g.\n\"ball\" is closer to \"leg of the person\" in \"kicking ball\"), and incorporate\nthis prior to simultaneously train a joint object detection and action\nclassification model. We conducted experiments on both video datasets and image\ndatasets to evaluate the performance of our weakly supervised object detection\nmodel. Our approach outperformed the current state-of-the-art (SOTA) method by\nmore than 6% in mAP on the Charades video dataset."},
{"paper": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection", "abstract": "Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts.Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO."},
{"paper": "DeepFlux for Skeletons in the Wild", "abstract": "Computing object skeletons in natural images is challenging, owing to large\nvariations in object appearance and scale, and the complexity of handling\nbackground clutter. Many recent methods frame object skeleton detection as a\nbinary pixel classification problem, which is similar in spirit to\nlearning-based edge detection, as well as to semantic segmentation methods.In\nthe present article, we depart from this strategy by training a CNN to predict\na two-dimensional vector field, which maps each scene point to a candidate\nskeleton pixel, in the spirit of flux-based skeletonization algorithms. This\n\"image context flux\" representation has two major advantages over previous\napproaches. First, it explicitly encodes the relative position of skeletal\npixels to semantically meaningful entities, such as the image points in their\nspatial context, and hence also the implied object boundaries. Second, since\nthe skeleton detection context is a region-based vector field, it is better\nable to cope with object parts of large width. We evaluate the proposed method\non three benchmark datasets for skeleton detection and two for symmetry\ndetection, achieving consistently superior performance over state-of-the-art\nmethods."},
{"paper": "Pyramid Dilated Deeper ConvLSTM for Video Salient Object Detection", "abstract": "This paper proposes a fast video salient object detection model, based on a novel recurrent network architecture, named Pyramid Dilated Bidirectional ConvLSTM (PDB-ConvLSTM). A Pyramid Dilated Convolution (PDC) module is first designed for simultaneously extracting spatial features at multiple scales.These spatial features are then concatenated and fed into an extended Deeper Bidirectional ConvLSTM (DB-ConvLSTM) to learn spatiotemporal information. Forward and backward ConvLSTM units are placed in two layers and connected in a cascaded way, encouraging information flow between the bi-directional streams and leading to deeper feature extraction. We further augment DB-ConvLSTM with a PDC-like structure, by adopting several dilated DB-ConvLSTMs to extract multi-scale spatiotemporal information. Extensive experimental results show that our method outperforms previous video saliency models in a large margin, with a real-time speed of 20 fps on a single GPU. With unsupervised video object segmentation as an example application, the proposed model (with a CRF-based post-process) achieves state-of-the-art results on two popular benchmarks, well demonstrating its superior performance and high applicability."},
{"paper": "Instance-aware, Context-focused, and Memory-efficient Weakly Supervised Object Detection", "abstract": "Weakly supervised learning has emerged as a compelling tool for object detection by reducing the need for strong supervision during training. However, major challenges remain: (1) differentiation of object instances can be ambiguous; (2) detectors tend to focus on discriminative parts rather than entire objects; (3) without ground truth, object proposals have to be redundant for high recalls, causing significant memory consumption.Addressing these challenges is difficult, as it often requires to eliminate uncertainties and trivial solutions. To target these issues we develop an instance-aware and context-focused unified framework. It employs an instance-aware self-training algorithm and a learnable Concrete DropBlock while devising a memory-efficient sequential batch back-propagation. Our proposed method achieves state-of-the-art results on COCO ($12.1\\% ~AP$, $24.8\\% ~AP_{50}$), VOC 2007 ($54.9\\% ~AP$), and VOC 2012 ($52.1\\% ~AP$), improving baselines by great margins. In addition, the proposed method is the first to benchmark ResNet based models and weakly supervised video object detection. Code, models, and more details will be made available at: https://github.com/NVlabs/wetectron."},
{"task": "RGB-D Salient Object Detection", "papers": {"0": "Bifurcated backbone strategy for RGB-D salient object detection", "1": "Siamese Network for RGB-D Salient Object Detection and Beyond", "2": "Bifurcated backbone strategy for RGB-D salient object detection", "3": "Is Depth Really Necessary for Salient Object Detection?", "4": "Uncertainty Inspired RGB-D Saliency Detection", "5": "Uncertainty Inspired RGB-D Saliency Detection", "6": "Is Depth Really Necessary for Salient Object Detection?"}},
{"paper": "Shifting More Attention to Video Salient Object Detection", "abstract": "The last decade has witnessed a growing interest in video salient object detection (VSOD). However, the research community long-term lacked a well-established VSOD dataset representative of real dynamic scenes with high-quality annotations.To address this issue, we elaborately collected a visual-attention-consistent Densely Annotated VSOD (DAVSOD) dataset, which contains 226 videos with 23,938 frames that cover diverse realistic-scenes, objects, instances and motions. With corresponding real human eye-fixation data, we obtain precise ground-truths. This is the first work that explicitly emphasizes the challenge of saliency shift, i.e., the video salient object(s) may dynamically change. To further contribute the community a complete benchmark, we systematically assess 17 representative VSOD algorithms over seven existing VSOD datasets and our DAVSOD with totally  84K frames (largest-scale). Utilizing three famous metrics, we then present a comprehensive and insightful performance analysis. Furthermore, we propose a baseline model. It is equipped with a saliency shift- aware convLSTM, which can efficiently capture video saliency dynamics through learning human attention-shift behavior. Extensive experiments open up promising future directions for model development and comparison."},
{"task": "Handwritten Word Generation", "papers": {}},
{"task": "User Constrained Thumbnail Generation", "papers": {}},
{"task": "RGB Salient Object Detection", "papers": {"0": "Uncertainty Inspired RGB-D Saliency Detection", "1": "Cascaded Partial Decoder for Fast and Accurate Salient Object Detection", "2": "Cascaded Partial Decoder for Fast and Accurate Salient Object Detection", "3": "Cascaded Partial Decoder for Fast and Accurate Salient Object Detection", "4": "Uncertainty Inspired RGB-D Saliency Detection", "5": "Uncertainty Inspired RGB-D Saliency Detection", "6": "Uncertainty Inspired RGB-D Saliency Detection", "7": "Uncertainty Inspired RGB-D Saliency Detection", "8": "A Simple Pooling-Based Design for Real-Time Salient Object Detection", "9": "A Simple Pooling-Based Design for Real-Time Salient Object Detection", "10": "Cascaded Partial Decoder for Fast and Accurate Salient Object Detection"}},
{"task": "Real-Time Object Detection", "papers": {"0": "EfficientDet: Scalable and Efficient Object Detection", "1": "You Only Look Once: Unified, Real-Time Object Detection", "2": "Mask R-CNN"}},
{"task": "3D Object Detection", "papers": {"0": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds", "1": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds", "2": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "3": "Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection", "4": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds", "5": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "6": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds", "7": "IPOD: Intensive Point-based Object Detector for Point Cloud", "8": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds", "9": "3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation", "10": "A Hierarchical Graph Network for 3D Object Detection on Point Clouds", "11": "Center-based 3D Object Detection and Tracking", "12": "Point-Voxel CNN for Efficient 3D Deep Learning", "13": "Point-Voxel CNN for Efficient 3D Deep Learning", "14": "Point-Voxel CNN for Efficient 3D Deep Learning", "15": "Point-Voxel CNN for Efficient 3D Deep Learning", "16": "Point-Voxel CNN for Efficient 3D Deep Learning", "17": "Point-Voxel CNN for Efficient 3D Deep Learning", "18": "Point-Voxel CNN for Efficient 3D Deep Learning", "19": "Point-Voxel CNN for Efficient 3D Deep Learning", "20": "Point-Voxel CNN for Efficient 3D Deep Learning", "21": "ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes", "22": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation", "23": "RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles", "24": "RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles", "25": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "26": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "27": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "28": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection"}},
{"task": "Pose-Guided Image Generation", "papers": {}},
{"task": "Object Detection", "papers": {"0": "CSPNet: A New Backbone that can Enhance Learning Capability of CNN", "1": "EfficientDet: Scalable and Efficient Object Detection", "2": "RODEO: Replay for Online Object Detection", "3": "Patch Refinement -- Localized 3D Object Detection", "4": "IterDet: Iterative Scheme for ObjectDetection in Crowded Environments", "5": "Patch Refinement -- Localized 3D Object Detection", "6": "Patch Refinement -- Localized 3D Object Detection", "7": "IterDet: Iterative Scheme for ObjectDetection in Crowded Environments", "8": "SSD: Single Shot MultiBox Detector", "9": "Scene Graph Generation from Objects, Phrases and Region Captions", "10": "Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous Vehicles", "11": "Look-into-Object: Self-supervised Structure Modeling for Object Recognition", "12": "Detecting People in Artwork with CNNs", "13": "On Generalizing Detection Models for Unconstrained Environments", "14": "On Generalizing Detection Models for Unconstrained Environments", "15": "Attention-based Joint Detection of Object and Semantic Part", "16": "How To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning", "17": "Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors", "18": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks", "19": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks", "20": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks", "21": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks", "22": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks", "23": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks"}},
{"task": "Layout-to-Image Generation", "papers": {"0": "Object-Centric Image Generation from Layouts", "1": "Object-Centric Image Generation from Layouts", "2": "Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis", "3": "Object-Centric Image Generation from Layouts"}},
{"task": "Image Interpolation", "papers": {}},
{"task": "Pose Transfer", "papers": {"0": "Region-adaptive Texture Enhancement for Detailed Person Image Synthesis", "1": "Progressive Pose Attention Transfer for Person Image Generation"}},
{"task": "Text-to-Image Generation", "papers": {"0": "DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation", "1": "DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation", "2": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks", "3": "Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction", "4": "Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction"}},
{"task": "Conditional Image Generation", "papers": {"0": "Lessons Learned from the Training of GANs on Artificial Datasets", "1": "LOGAN: Latent Optimisation for Generative Adversarial Networks", "2": "Feature Quantization Improves GAN Training", "3": "A U-Net Based Discriminator for Generative Adversarial Networks", "4": "Feature Quantization Improves GAN Training"}},
{"task": "Image Generation", "papers": {"0": "Denoising Diffusion Probabilistic Models", "1": "Image Transformer", "2": "Efficient Content-Based Sparse Attention with Routing Transformers", "3": "Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling", "4": "A Style-Based Generator Architecture for Generative Adversarial Networks", "5": "Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search", "6": "Analyzing and Improving the Image Quality of StyleGAN", "7": "Locally Masked Convolution for Autoregressive Models", "8": "Locally Masked Convolution for Autoregressive Models", "9": "A Style-Based Generator Architecture for Generative Adversarial Networks", "10": "A U-Net Based Discriminator for Generative Adversarial Networks", "11": "Analyzing and Improving the Image Quality of StyleGAN", "12": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "13": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "14": "Generative Latent Flow", "15": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "16": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery", "17": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery", "18": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery", "19": "Analyzing and Improving the Image Quality of StyleGAN", "20": "NCP-VAE: Variational Autoencoders with Noise Contrastive Priors", "21": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "22": "The relativistic discriminator: a key element missing from standard GAN", "23": "Twin Auxilary Classifiers GAN", "24": "COCO-GAN: Generation by Parts via Conditional Coordinating", "25": "A U-Net Based Discriminator for Generative Adversarial Networks", "26": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "27": "A U-Net Based Discriminator for Generative Adversarial Networks", "28": "Semantic Bottleneck Scene Generation", "29": "Semantic Bottleneck Scene Generation", "30": "Semantic Bottleneck Scene Generation", "31": "MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks", "32": "MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks", "33": "Analyzing and Improving the Image Quality of StyleGAN", "34": "Analyzing and Improving the Image Quality of StyleGAN", "35": "Analyzing and Improving the Image Quality of StyleGAN", "36": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "37": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations"}},
{"task": "Image Inpainting", "papers": {"0": "Free-Form Image Inpainting with Gated Convolution", "1": "Coherent Semantic Attention for Image Inpainting", "2": "Global and Local Attention-Based Free-Form Image Inpainting", "3": "Editing Text in the Wild"}},
{"paper": "Is Depth Really Necessary for Salient Object Detection?", "abstract": "Salient object detection (SOD) is a crucial and preliminary task for many computer vision applications, which have made progress with deep CNNs. Most of the existing methods mainly rely on the RGB information to distinguish the salient objects, which faces difficulties in some complex scenarios.To solve this, many recent RGBD-based networks are proposed by adopting the depth map as an independent input and fuse the features with RGB information. Taking the advantages of RGB and RGBD methods, we propose a novel depth-aware salient object detection framework, which has following superior designs: 1) It only takes the depth information as training data while only relies on RGB information in the testing phase. 2) It comprehensively optimizes SOD features with multi-level depth-aware regularizations. 3) The depth information also serves as error-weighted map to correct the segmentation process. With these insightful designs combined, we make the first attempt in realizing an unified depth-aware framework with only RGB information as input for inference, which not only surpasses the state-of-the-art performances on five public RGB SOD benchmarks, but also surpasses the RGBD-based methods on five benchmarks by a large margin, while adopting less information and implementation light-weighted. The code and model will be publicly available."},
{"subfield": "Text Generation", "tasks": {"0": "Text Summarization", "1": "Abstractive Text Summarization", "2": "Document Summarization", "3": "Multi-Document Summarization", "4": "Sentence Summarization", "5": "Extractive Document Summarization", "6": "Scientific Document Summarization", "7": "Timeline Summarization", "8": "Query-Based Extractive Summarization"}},
{"task": "Image-to-Image Translation", "papers": {"0": "Constructing Self-motivated Pyramid Curriculums for Cross-Domain Semantic Segmentation: A Non-Adversarial Approach", "1": "Image-to-Image Translation with Conditional Adversarial Networks", "2": "SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects", "3": "You Only Need Adversarial Supervision for Semantic Image Synthesis", "4": "You Only Need Adversarial Supervision for Semantic Image Synthesis", "5": "Image-to-Image Translation with Conditional Adversarial Networks", "6": "Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training", "7": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation", "8": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "9": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "10": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "11": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "12": "Progressive Domain Adaptation for Object Detection", "13": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "14": "Image-to-Image Translation with Conditional Adversarial Networks", "15": "Feature Quantization Improves GAN Training", "16": "Feature Quantization Improves GAN Training", "17": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "18": "InstaGAN: Instance-aware Image-to-Image Translation", "19": "Semantic Bottleneck Scene Generation", "20": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "21": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "22": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "23": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "24": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "25": "Towards Multi-pose Guided Virtual Try-on Network", "26": "Editing Text in the Wild"}},
{"paper": "Siamese Network for RGB-D Salient Object Detection and Beyond", "abstract": "Existing RGB-D salient object detection (SOD) models usually treat RGB and depth as independent information and design separate networks for feature extraction from each. Such schemes can easily be constrained by a limited amount of training data or over-reliance on an elaborately designed training process.Inspired by the observation that RGB and depth modalities actually present certain commonality in distinguishing salient objects, a novel joint learning and densely cooperative fusion (JL-DCF) architecture is designed to learn from both RGB and depth inputs through a shared network backbone, known as the Siamese architecture. In this paper, we propose two effective components: joint learning (JL), and densely cooperative fusion (DCF). The JL module provides robust saliency feature learning by exploiting cross-modal commonality via a Siamese network, while the DCF module is introduced for complementary feature discovery. Comprehensive experiments using five popular metrics show that the designed framework yields a robust RGB-D saliency detector with good generalization. As a result, JL-DCF significantly advances the state-of-the-art models by an average of ~2.0% (F-measure) across seven challenging datasets. In addition, we show that JL-DCF is readily applicable to other related multi-modal detection tasks, including RGB-T (thermal infrared) SOD and video SOD (VSOD), achieving comparable or even better performance against state-of-the-art methods. This further confirms that the proposed framework could offer a potential solution for various applications and provide more insight into the cross-modal complementarity task. The code will be available at https://github.com/kerenfu/JLDCF/"},
{"paper": "Uncertainty Inspired RGB-D Saliency Detection", "abstract": "We propose the first stochastic framework to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection models treat this task as a point estimation problem by predicting a single saliency map following a deterministic learning pipeline.We argue that, however, the deterministic solution is relatively ill-posed. Inspired by the saliency data labeling process, we propose a generative architecture to achieve probabilistic RGB-D saliency detection which utilizes a latent variable to model the labeling variations. Our framework includes two main models: 1) a generator model, which maps the input image and latent variable to stochastic saliency prediction, and 2) an inference model, which gradually updates the latent variable by sampling it from the true or approximate posterior distribution. The generator model is an encoder-decoder saliency network. To infer the latent variable, we introduce two different solutions: i) a Conditional Variational Auto-encoder with an extra encoder to approximate the posterior distribution of the latent variable; and ii) an Alternating Back-Propagation technique, which directly samples the latent variable from the true posterior distribution. Qualitative and quantitative results on six challenging RGB-D benchmark datasets show our approach's superior performance in learning the distribution of saliency maps. The source code is publicly available via our project page: https://github.com/JingZhang617/UCNet."},
{"paper": "Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis", "abstract": "With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable inputs. This paper focuses on a recent emerged task, layout-to-image, to learn generative models that are capable of synthesizing photo-realistic images from spatial layout (i.e., object bounding boxes configured in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors).This paper first proposes an intuitive paradigm for the task, layout-to-mask-to-image, to learn to unfold object masks of given bounding boxes in an input layout to bridge the gap between the input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks for the proposed layout-to-mask-to-image with style control at both image and mask levels. Object masks are learned from the input layout and iteratively refined along stages in the generator network. Style control at the image level is the same as in vanilla GANs, while style control at the object mask level is realized by a proposed novel feature normalization scheme, Instance-Sensitive and Layout-Aware Normalization. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained."},
{"paper": "RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles", "abstract": "Region proposal algorithms play an important role in most state-of-the-art two-stage object detection networks by hypothesizing object locations in the image. Nonetheless, region proposal algorithms are known to be the bottleneck in most two-stage object detection networks, increasing the processing time for each image and resulting in slow networks not suitable for real-time applications such as autonomous driving vehicles.In this paper we introduce RRPN, a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for each mapped Radar detection point. These anchor boxes are then transformed and scaled based on the object's distance from the vehicle, to provide more accurate proposals for the detected objects. We evaluate our method on the newly released NuScenes dataset [1] using the Fast R-CNN object detection network [2]. Compared to the Selective Search object proposal algorithm [3], our model operates more than 100x faster while at the same time achieves higher detection precision and recall. Code has been made publicly available at https://github.com/mrnabati/RRPN ."},
{"paper": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks", "abstract": "This paper proposes a computationally efficient approach to detecting objects\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\nparticular, this is achieved by leveraging a feature-centric voting scheme to\nimplement novel convolutional layers which explicitly exploit the sparsity\nencountered in the input.To this end, we examine the trade-off between\naccuracy and speed for different architectures and additionally propose to use\nan L1 penalty on the filter activations to further encourage sparsity in the\nintermediate representations. To the best of our knowledge, this is the first\nwork to propose sparse convolutional layers and L1 regularisation for efficient\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\non the KITTI object detection benchmark and show that Vote3Deep models with as\nfew as three layers outperform the previous state of the art in both laser and\nlaser-vision based approaches by margins of up to 40% while remaining highly\ncompetitive in terms of processing time."},
{"paper": "Object-Centric Image Generation from Layouts", "abstract": "Despite recent impressive results on single-object and single-domain image generation, the generation of complex scenes with multiple objects remains challenging. In this paper, we start with the idea that a model must be able to understand individual objects and relationships between objects in order to generate complex scenes well.Our layout-to-image-generation method, which we call Object-Centric Generative Adversarial Network (or OC-GAN), relies on a novel Scene-Graph Similarity Module (SGSM). The SGSM learns representations of the spatial relationships between objects in the scene, which lead to our model's improved layout-fidelity. We also propose changes to the conditioning mechanism of the generator that enhance its object instance-awareness. Apart from improving image quality, our contributions mitigate two failure modes in previous approaches: (1) spurious objects being generated without corresponding bounding boxes in the layout, and (2) overlapping bounding boxes in the layout leading to merged objects in images. Extensive quantitative evaluation and ablation studies demonstrate the impact of our contributions, with our model outperforming previous state-of-the-art approaches on both the COCO-Stuff and Visual Genome datasets. Finally, we address an important limitation of evaluation metrics used in previous works by introducing SceneFID -- an object-centric adaptation of the popular Fr{\\'e}chet Inception Distance metric, that is better suited for multi-object images."},
{"paper": "Bifurcated backbone strategy for RGB-D salient object detection", "abstract": "Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales.When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efficient, and backbone-independent. Extensive experiments show that BBS-Net significantly outperforms eighteen SOTA models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach ($\\sim 4 \\%$ improvement in S-measure $vs.$ the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research."},
{"paper": "You Only Look Once: Unified, Real-Time Object Detection", "abstract": "We present YOLO, a new approach to object detection. Prior work on object\ndetection repurposes classifiers to perform detection.Instead, we frame object\ndetection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes\nand class probabilities directly from full images in one evaluation. Since the\nwhole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes\nimages in real-time at 45 frames per second. A smaller version of the network,\nFast YOLO, processes an astounding 155 frames per second while still achieving\ndouble the mAP of other real-time detectors. Compared to state-of-the-art\ndetection systems, YOLO makes more localization errors but is far less likely\nto predict false detections where nothing exists. Finally, YOLO learns very\ngeneral representations of objects. It outperforms all other detection methods,\nincluding DPM and R-CNN, by a wide margin when generalizing from natural images\nto artwork on both the Picasso Dataset and the People-Art Dataset."},
{"paper": "Progressive Pose Attention Transfer for Person Image Generation", "abstract": "This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively.Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency. Codes and models are available at: https://github.com/tengteng95/Pose-Transfer.git."},
{"paper": "Region-adaptive Texture Enhancement for Detailed Person Image Synthesis", "abstract": "The ability to produce convincing textural details is essential for the fidelity of synthesized person images. However, existing methods typically follow a ``warping-based'' strategy that propagates appearance features through the same pathway used for pose transfer.However, most fine-grained features would be lost due to down-sampling, leading to over-smoothed clothes and missing details in the output images. In this paper we presents RATE-Net, a novel framework for synthesizing person images with sharp texture details. The proposed framework leverages an additional texture enhancing module to extract appearance information from the source image and estimate a fine-grained residual texture map, which helps to refine the coarse estimation from the pose transfer module. In addition, we design an effective alternate updating strategy to promote mutual guidance between two modules for better shape and appearance consistency. Experiments conducted on DeepFashion benchmark dataset have demonstrated the superiority of our framework compared with existing networks."},
{"paper": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "abstract": "Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes.Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning."},
{"paper": "Editing Text in the Wild", "abstract": "In this paper, we are interested in editing text in natural images, which aims to replace or modify a word in the source image with another one while maintaining its realistic look. This task is challenging, as the styles of both background and text need to be preserved so that the edited image is visually indistinguishable from the source image.Specifically, we propose an end-to-end trainable style retention network (SRNet) that consists of three modules: text conversion module, background inpainting module and fusion module. The text conversion module changes the text content of the source image into the target text while keeping the original text style. The background inpainting module erases the original text, and fills the text region with appropriate texture. The fusion module combines the information from the two former modules, and generates the edited text images. To our knowledge, this work is the first attempt to edit text in natural images at the word level. Both visual effects and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully confirm the importance and necessity of modular decomposition. We also conduct extensive experiments to validate the usefulness of our method in various real-world applications such as text image synthesis, augmented reality (AR) translation, information hiding, etc."},
{"paper": "MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks", "abstract": "While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to adapt to different datasets, in part due to instability during training and sensitivity to hyperparameters. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator become uninformative when there isn't enough overlap in the supports of the real and fake distributions.In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this by allowing the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique. We show that MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains, as well as different types of loss functions and architectures, all with the same set of fixed hyperparameters. When compared to state-of-the-art GANs, our approach matches or exceeds the performance in most of the cases we tried."},
{"task": "Query-Based Extractive Summarization", "papers": {"0": "Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models"}},
{"paper": "Towards Multi-pose Guided Virtual Try-on Network", "abstract": "Virtual try-on system under arbitrary human poses has huge application\npotential, yet raises quite a lot of challenges, e.g. self-occlusions, heavy\nmisalignment among diverse poses, and diverse clothes textures. Existing\nmethods aim at fitting new clothes into a person can only transfer clothes on\nthe fixed human pose, but still show unsatisfactory performances which often\nfail to preserve the identity, lose the texture details, and decrease the\ndiversity of poses.In this paper, we make the first attempt towards multi-pose\nguided virtual try-on system, which enables transfer clothes on a person image\nunder diverse poses. Given an input person image, a desired clothes image, and\na desired pose, the proposed Multi-pose Guided Virtual Try-on Network (MG-VTON)\ncan generate a new person image after fitting the desired clothes into the\ninput image and manipulating human poses. Our MG-VTON is constructed in three\nstages: 1) a desired human parsing map of the target image is synthesized to\nmatch both the desired pose and the desired clothes shape; 2) a deep Warping\nGenerative Adversarial Network (Warp-GAN) warps the desired clothes appearance\ninto the synthesized human parsing map and alleviates the misalignment problem\nbetween the input human pose and desired human pose; 3) a refinement render\nutilizing multi-pose composition masks recovers the texture details of clothes\nand removes some artifacts. Extensive experiments on well-known datasets and\nour newly collected largest virtual try-on benchmark demonstrate that our\nMG-VTON significantly outperforms all state-of-the-art methods both\nqualitatively and quantitatively with promising multi-pose virtual try-on\nperformances."},
{"paper": "A U-Net Based Discriminator for Generative Adversarial Networks", "abstract": "Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature.The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals dataset."},
{"paper": "Mask R-CNN", "abstract": "We present a conceptually simple, flexible, and general framework for object\ninstance segmentation. Our approach efficiently detects objects in an image\nwhile simultaneously generating a high-quality segmentation mask for each\ninstance.The method, called Mask R-CNN, extends Faster R-CNN by adding a\nbranch for predicting an object mask in parallel with the existing branch for\nbounding box recognition. Mask R-CNN is simple to train and adds only a small\noverhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to\ngeneralize to other tasks, e.g., allowing us to estimate human poses in the\nsame framework. We show top results in all three tracks of the COCO suite of\nchallenges, including instance segmentation, bounding-box object detection, and\nperson keypoint detection. Without bells and whistles, Mask R-CNN outperforms\nall existing, single-model entries on every task, including the COCO 2016\nchallenge winners. We hope our simple and effective approach will serve as a\nsolid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron"},
{"paper": "Progressive Domain Adaptation for Object Detection", "abstract": "Recent deep learning methods for object detection rely on a large amount of bounding box annotations. Collecting these annotations is laborious and costly, yet supervised models do not generalize well when testing on images from a different distribution.Domain adaptation provides a solution by adapting existing labels to the target testing data. However, a large gap between domains could make adaptation a challenging task, which leads to unstable training processes and sub-optimal results. In this paper, we propose to bridge the domain gap with an intermediate domain and progressively solve easier adaptation subtasks. This intermediate domain is constructed by translating the source images to mimic the ones in the target domain. To tackle the domain-shift problem, we adopt adversarial learning to align distributions at the feature level. In addition, a weighted task loss is applied to deal with unbalanced image quality in the intermediate domain. Experimental results show that our method performs favorably against the state-of-the-art method in terms of the performance on the target domain."},
{"paper": "InstaGAN: Instance-aware Image-to-Image Translation", "abstract": "Unsupervised image-to-image translation has gained considerable attention due\nto the recent impressive progress based on generative adversarial networks\n(GANs). However, previous methods often fail in challenging cases, in\nparticular, when an image has multiple target instances and a translation task\ninvolves significant changes in shape, e.g., translating pants to skirts in\nfashion images.To tackle the issues, we propose a novel method, coined\ninstance-aware GAN (InstaGAN), that incorporates the instance information\n(e.g., object segmentation masks) and improves multi-instance transfiguration. The proposed method translates both an image and the corresponding set of\ninstance attributes while maintaining the permutation invariance property of\nthe instances. To this end, we introduce a context preserving loss that\nencourages the network to learn the identity function outside of target\ninstances. We also propose a sequential mini-batch inference/training technique\nthat handles multiple instances with a limited GPU memory and enhances the\nnetwork to generalize better for multiple instances. Our comparative evaluation\ndemonstrates the effectiveness of the proposed method on different image\ndatasets, in particular, in the aforementioned challenging cases. Code and\nresults are available in https://github.com/sangwoomo/instagan"},
{"paper": "Feature Quantization Improves GAN Training", "abstract": "The instability in GAN training has been a long-standing problem despite remarkable research efforts. We identify that instability issues stem from difficulties of performing feature matching with mini-batch statistics, due to a fragile balance between the fixed target distribution and the progressively generated distribution.In this work, we propose Feature Quantization (FQ) for the discriminator, to embed both true and fake data samples into a shared discrete space. The quantized values of FQ are constructed as an evolving dictionary, which is consistent with feature statistics of the recent distribution history. Hence, FQ implicitly enables robust feature matching in a compact space. Our method can be easily plugged into existing GAN models, with little computational overhead in training. We apply FQ to 3 representative GAN models on 9 benchmarks: BigGAN for image generation, StyleGAN for face synthesis, and U-GAT-IT for unsupervised image-to-image translation. Extensive experimental results show that the proposed FQ-GAN can improve the FID scores of baseline methods by a large margin on a variety of tasks, achieving new state-of-the-art performance."},
{"paper": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier.Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch."},
{"paper": "Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training", "abstract": "Recent deep networks achieved state of the art performanceon a variety of semantic segmentation tasks. Despite such progress, thesemodels often face challenges in real world \u00e2\u0080\u009cwild tasks\u00e2\u0080\u009d where large differ-ence between labeled training/source data and unseen test/target dataexists.In particular, such difference is often referred to as \u00e2\u0080\u009cdomain gap\u00e2\u0080\u009d,and  could  cause  significantly  decreased  performance  which  cannot  beeasily remedied by further increasing the representation power. Unsuper-vised domain adaptation (UDA) seeks to overcome such problem withouttarget domain labels. In this paper, we propose a novel UDA frameworkbased  on  an  iterative  self-training  (ST)  procedure,  where  the  problemis formulated as latent variable loss minimization, and can be solved byalternatively generating pseudo labels on target data and re-training themodel with these labels. On top of ST, we also propose a novel class-balanced  self-training  (CBST)  framework  to  avoid  the  gradual  domi-nance of large classes on pseudo-label generation, and introduce spatialpriors to refine generated labels. Comprehensive experiments show thatthe  proposed  methods  achieve  state  of  the  art  semantic  segmentationperformance under multiple major UDA settings."},
{"paper": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation", "abstract": "Recent studies have shown remarkable success in image-to-image translation\nfor two domains. However, existing approaches have limited scalability and\nrobustness in handling more than two domains, since different models should be\nbuilt independently for every pair of image domains.To address this\nlimitation, we propose StarGAN, a novel and scalable approach that can perform\nimage-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of\nmultiple datasets with different domains within a single network. This leads to\nStarGAN's superior quality of translated images compared to existing models as\nwell as the novel capability of flexibly translating an input image to any\ndesired target domain. We empirically demonstrate the effectiveness of our\napproach on a facial attribute transfer and a facial expression synthesis\ntasks."},
{"paper": "You Only Need Adversarial Supervision for Semantic Image Synthesis", "abstract": "Despite their recent successes, semantic image synthesis GAN models still greatly suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis.In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of $6$ FID and $5$ mIoU points over the state of the art across different datasets using only adversarial supervision."},
{"task": "Scientific Document Summarization", "papers": {"0": "ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks"}},
{"paper": "SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects", "abstract": "Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information.They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels."},
{"task": "Timeline Summarization", "papers": {"0": "Learning towards Abstractive Timeline Summarization"}},
{"paper": "Constructing Self-motivated Pyramid Curriculums for Cross-Domain Semantic Segmentation: A Non-Adversarial Approach", "abstract": "We propose a new approach, called self-motivated pyramid curriculum domain adaptation (PyCDA), to facilitate the adaptation of semantic segmentation neural networks from synthetic source domains to real target domains. Our approach draws on an insight connecting two existing works: curriculum domain adaptation and self-training.Inspired by the former, PyCDA constructs a pyramid curriculum which contains various properties about the target domain. Those properties are mainly about the desired label distributions over the target domain images, image regions, and pixels. By enforcing the segmentation neural network to observe those properties, we can improve the network's generalization capability to the target domain. Motivated by the self-training, we infer this pyramid of properties by resorting to the semantic segmentation network itself. Unlike prior work, we do not need to maintain any additional models (e.g., logistic regression or discriminator networks) or to solve minmax problems which are often difficult to optimize. We report state-of-the-art results for the adaptation from both GTAV and SYNTHIA to Cityscapes, two popular settings in unsupervised domain adaptation for semantic segmentation."},
{"paper": "Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models", "abstract": "Query Focused Summarization (QFS) has been addressed mostly using extractive\nmethods. Such methods, however, produce text which suffers from low coherence.We investigate how abstractive methods can be applied to QFS, to overcome such\nlimitations. Recent developments in neural-attention based sequence-to-sequence\nmodels have led to state-of-the-art results on the task of abstractive generic\nsingle document summarization. Such models are trained in an end to end method\non large amounts of training data. We address three aspects to make abstractive\nsummarization applicable to QFS: (a)since there is no training data, we\nincorporate query relevance into a pre-trained abstractive model; (b) since\nexisting abstractive models are trained in a single-document setting, we design\nan iterated method to embed abstractive models within the multi-document\nrequirement of QFS; (c) the abstractive models we adapt are trained to generate\ntext of specific length (about 100 words), while we aim at generating output of\na different size (about 250 words); we design a way to adapt the target size of\nthe generated summaries to a given size ratio. We compare our method (Relevance\nSensitive Attention for QFS) to extractive baselines and with various ways to\ncombine abstractive models on the DUC QFS datasets and demonstrate solid\nimprovements on ROUGE performance."},
{"task": "Extractive Document Summarization", "papers": {"0": "Iterative Document Representation Learning Towards Summarization with Polishing"}},
{"task": "Sentence Summarization", "papers": {"0": "Simple Unsupervised Summarization by Contextual Matching"}},
{"paper": "Image-to-Image Translation with Conditional Adversarial Networks", "abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping.This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either."},
{"paper": "Global and Local Attention-Based Free-Form Image Inpainting", "abstract": "Deep-learning-based image inpainting methods have shown significant promise in both rectangular and irregular holes. However, the inpainting of irregular holes presents numerous challenges owing to uncertainties in their shapes and locations.When depending solely on convolutional neural network (CNN) or adversarial supervision, plausible inpainting results cannot be guaranteed because irregular holes need attention-based guidance for retrieving information for content generation. In this paper, we propose two new attention mechanisms, namely a mask pruning-based global attention module and a global and local attention module to obtain global dependency information and the local similarity information among the features for refined results. The proposed method is evaluated using state-of-the-art methods, and the experimental results show that our method outperforms the existing methods in both quantitative and qualitative measures."},
{"task": "Multi-Document Summarization", "papers": {"0": "Graph-based Neural Multi-Document Summarization"}},
{"task": "Abstractive Text Summarization", "papers": {"0": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation", "1": "Abstractive Summarization of Spoken andWritten Instructions with BERT"}},
{"task": "Text Summarization", "papers": {"0": "Better Fine-Tuning by Reducing Representational Collapse", "1": "Positional Encoding to Control Output Sequence Length", "2": "A Divide-and-Conquer Approach to the Summarization of Long Documents", "3": "Big Bird: Transformers for Longer Sequences", "4": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation", "5": "Abstractive Summarization of Spoken andWritten Instructions with BERT", "6": "Better Fine-Tuning by Reducing Representational Collapse", "7": "Multimodal Abstractive Summarization for How2 Videos", "8": "Big Bird: Transformers for Longer Sequences", "9": "Extractive Summarization as Text Matching", "10": "ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks", "11": "Better Fine-Tuning by Reducing Representational Collapse", "12": "Better Fine-Tuning by Reducing Representational Collapse"}},
{"task": "Document Summarization", "papers": {"0": "Extractive Summarization as Text Matching", "1": "Big Bird: Transformers for Longer Sequences"}},
{"paper": "Coherent Semantic Attention for Image Inpainting", "abstract": "The latest deep learning-based approaches have shown promising results for the challenging task of inpainting missing regions of an image. However, the existing methods often generate contents with blurry textures and distorted structures due to the discontinuity of the local pixels.From a semantic-level perspective, the local pixel discontinuity is mainly because these methods ignore the semantic relevance and feature continuity of hole regions. To handle this problem, we investigate the human behavior in repairing pictures and propose a fined deep generative model-based approach with a novel coherent semantic attention (CSA) layer, which can not only preserve contextual structure but also make more effective predictions of missing parts by modeling the semantic relevance between the holes features. The task is divided into rough, refinement as two steps and model each step with a neural network under the U-Net architecture, where the CSA layer is embedded into the encoder of refinement step. To stabilize the network training process and promote the CSA layer to learn more effective parameters, we propose a consistency loss to enforce the both the CSA layer and the corresponding layer of the CSA in decoder to be close to the VGG feature layer of a ground truth image simultaneously. The experiments on CelebA, Places2, and Paris StreetView datasets have validated the effectiveness of our proposed methods in image inpainting tasks and can obtain images with a higher quality as compared with the existing state-of-the-art approaches."},
{"paper": "Free-Form Image Inpainting with Gated Convolution", "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts.The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting"},
{"paper": "Semantic Bottleneck Scene Generation", "abstract": "Coupling the high-fidelity generation capabilities of label-conditional image synthesis methods with the flexibility of unconditional generative models, we propose a semantic bottleneck GAN model for unconditional synthesis of complex scenes. We assume pixel-wise segmentation labels are available during training and use them to learn the scene structure.During inference, our model first synthesizes a realistic segmentation layout from scratch, then synthesizes a realistic scene conditioned on that layout. For the former, we use an unconditional progressive segmentation generation network that captures the distribution of realistic semantic scene layouts. For the latter, we use a conditional segmentation-to-image synthesis network that captures the distribution of photo-realistic images conditioned on the semantic layout. When trained end-to-end, the resulting model outperforms state-of-the-art generative models in unsupervised image synthesis on two challenging domains in terms of the Frechet Inception Distance and user-study evaluations. Moreover, we demonstrate the generated segmentation maps can be used as additional training data to strongly improve recent segmentation-to-image synthesis networks."},
{"paper": "COCO-GAN: Generation by Parts via Conditional Coordinating", "abstract": "Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment.Inspired by such behavior and the fact that machines also have computational constraints, we propose \\underline{CO}nditional \\underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce \\textbf{state-of-the-art-quality} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called \"beyond-boundary generation\". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand."},
{"paper": "ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks", "abstract": "Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article's impacts on research community. This paper provides novel solutions to these two challenges.We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors' original highlights (abstract) and the article's actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research."},
{"paper": "Twin Auxilary Classifiers GAN", "abstract": "Conditional generative models enjoy significant progress over the past few years. One of the popular conditional models is Auxiliary Classifier GAN (AC-GAN) that generates highly discriminative images by extending the loss function of GAN with an auxiliary classifier.However, the diversity of the generated samples by AC-GAN tends to decrease as the number of classes increases. In this paper, we identify the source of low diversity issue theoretically and propose a practical solution to the problem. We show that the auxiliary classifier in AC-GAN imposes perfect separability, which is disadvantageous when the supports of the class distributions have significant overlap. To address the issue, we propose Twin Auxiliary Classifiers Generative Adversarial Net (TAC-GAN) that adds a new player that interacts with other players (the generator and the discriminator) in GAN. Theoretically, we demonstrate that our TAC-GAN can effectively minimize the divergence between generated and real data distributions. Extensive experimental results show that our TAC-GAN can successfully replicate the true data distributions on simulated data, and significantly improves the diversity of class-conditional image generation on real datasets."},
{"paper": "Learning towards Abstractive Timeline Summarization", "abstract": "Timeline summarization targets at concisely summarizing the evolution trajectory along the timeline and existing timeline summarization approaches are all based on extractive methods.In this paper, we propose the task of abstractive timeline summarization, which tends to concisely paraphrase the information in the time-stamped events.Unlike traditional document summarization, timeline summarization needs to model the time series information of the input events and summarize important events in chronological order.To tackle this challenge, we propose a memory-based timeline summarization model (MTS).Concretely, we propose a time-event memory to establish a timeline, and use the time position of events on this timeline to guide generation process.Besides, in each decoding step, we incorporate event-level information into word-level attention to avoid confusion between events.Extensive experiments are conducted on a large-scale real-world dataset, and the results show that MTS achieves the state-of-the-art performance in terms of both automatic and human evaluations.None"},
{"paper": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved.We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\nfor both the discriminator and the generator. Using the theory of stochastic\napproximation, we prove that the TTUR converges under mild assumptions to a\nstationary local Nash equilibrium. The convergence carries over to the popular\nAdam optimization, for which we prove that it follows the dynamics of a heavy\nball with friction and thus prefers flat minima in the objective landscape. For\nthe evaluation of the performance of GANs at image generation, we introduce the\n\"Fr\\'echet Inception Distance\" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\nBedrooms, and the One Billion Word Benchmark."},
{"paper": "NCP-VAE: Variational Autoencoders with Noise Contrastive Priors", "abstract": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering.One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets."},
{"paper": "The relativistic discriminator: a key element missing from standard GAN", "abstract": "In standard generative adversarial network (SGAN), the discriminator\nestimates the probability that the input data is real. The generator is trained\nto increase the probability that fake data is real.We argue that it should\nalso simultaneously decrease the probability that real data is real because 1)\nthis would account for a priori knowledge that half of the data in the\nmini-batch is fake, 2) this would be observed with divergence minimization, and\n3) in optimal settings, SGAN would be equivalent to integral probability metric\n(IPM) GANs. We show that this property can be induced by using a relativistic\ndiscriminator which estimate the probability that the given real data is more\nrealistic than a randomly sampled fake data. We also present a variant in which\nthe discriminator estimate the probability that the given real data is more\nrealistic than fake data, on average. We generalize both approaches to\nnon-standard GAN loss functions and we refer to them respectively as\nRelativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that\nIPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more\nstable and generate higher quality data samples than their non-relativistic\ncounterparts, 2) Standard RaGAN with gradient penalty generate data of better\nquality than WGAN-GP while only requiring a single discriminator update per\ngenerator update (reducing the time taken for reaching the state-of-the-art by\n400%), and 3) RaGANs are able to generate plausible high resolutions images\n(256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these\nimages are of significantly better quality than the ones generated by WGAN-GP\nand SGAN with spectral normalization."},
{"paper": "Simple Unsupervised Summarization by Contextual Matching", "abstract": "We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain.We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data."},
{"paper": "Iterative Document Representation Learning Towards Summarization with Polishing", "abstract": "In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation.To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans."},
{"paper": "Generative Latent Flow", "abstract": "In this work, we propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise.In contrast to some other Auto-encoder based generative models, which use various regularizers that encourage the encoded latent distribution to match the prior distribution, our model explicitly constructs a mapping between these two distributions, leading to better density matching while avoiding over regularizing the latent variables. We compare our model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. We also study the relationship between our model and its stochastic counterpart, and show that our model can be viewed as a vanishing noise limit of VAEs with flow prior. Quantitatively, under standardized evaluations, our method achieves state-of-the-art sample quality among AE based models on commonly used datasets, and is competitive with GANs' benchmarks."},
{"paper": "Graph-based Neural Multi-Document Summarization", "abstract": "We propose a neural multi-document summarization (MDS) system that\nincorporates sentence relation graphs. We employ a Graph Convolutional Network\n(GCN) on the relation graphs, with sentence embeddings obtained from Recurrent\nNeural Networks as input node features.Through multiple layer-wise\npropagation, the GCN generates high-level hidden sentence features for salience\nestimation. We then use a greedy heuristic to extract salient sentences while\navoiding redundancy. In our experiments on DUC 2004, we consider three types of\nsentence relation graphs and demonstrate the advantage of combining sentence\nrelations in graphs with the representation power of deep neural networks. Our\nmodel improves upon traditional graph-based extractive approaches and the\nvanilla GRU sequence model with no graph, and it achieves competitive results\nagainst other state-of-the-art multi-document summarization systems."},
{"paper": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space.Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection."},
{"paper": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery", "abstract": "We propose FineGAN, a novel unsupervised GAN framework, which disentangles\nthe background, object shape, and object appearance to hierarchically generate\nimages of fine-grained object categories. To disentangle the factors without\nsupervision, our key idea is to use information theory to associate each factor\nto a latent code, and to condition the relationships between the codes in a\nspecific way to induce the desired hierarchy.Through extensive experiments, we\nshow that FineGAN achieves the desired disentanglement to generate realistic\nand diverse images belonging to fine-grained classes of birds, dogs, and cars. Using FineGAN's automatically learned features, we also cluster real images as\na first attempt at solving the novel problem of unsupervised fine-grained\nobject category discovery. Our code/models/demo can be found at\nhttps://github.com/kkanshul/finegan"},
{"paper": "Multimodal Abstractive Summarization for How2 Videos", "abstract": "In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to \"compress\" text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text).We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU."},
{"paper": "Extractive Summarization as Text Matching", "abstract": "This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space.Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/MatchSum."},
{"paper": "Positional Encoding to Control Output Sequence Length", "abstract": "Neural encoder-decoder models have been successful in natural language\ngeneration tasks. However, real applications of abstractive summarization must\nconsider additional constraint that a generated summary should not exceed a\ndesired length.In this paper, we propose a simple but effective extension of a\nsinusoidal positional encoding (Vaswani et al., 2017) to enable neural\nencoder-decoder model to preserves the length constraint. Unlike in previous\nstudies where that learn embeddings representing each length, the proposed\nmethod can generate a text of any length even if the target length is not\npresent in training data. The experimental results show that the proposed\nmethod can not only control the generation length but also improve the ROUGE\nscores."},
{"paper": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "abstract": "Despite recent progress in generative image modeling, successfully generating\nhigh-resolution, diverse samples from complex datasets such as ImageNet remains\nan elusive goal. To this end, we train Generative Adversarial Networks at the\nlargest scale yet attempted, and study the instabilities specific to such\nscale.We find that applying orthogonal regularization to the generator renders\nit amenable to a simple \"truncation trick,\" allowing fine control over the\ntrade-off between sample fidelity and variety by reducing the variance of the\nGenerator's input. Our modifications lead to models which set the new state of\nthe art in class-conditional image synthesis. When trained on ImageNet at\n128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of\n166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous\nbest IS of 52.52 and FID of 18.6."},
{"paper": "A Divide-and-Conquer Approach to the Summarization of Long Documents", "abstract": "We present a novel divide-and-conquer method for the neural summarization of long documents. Our method exploits the discourse structure of the document and uses sentence similarity to split the problem into an ensemble of smaller summarization problems.In particular, we break a long document and its summary into multiple source-target pairs, which are used for training a model that learns to summarize each part of the document separately. These partial summaries are then combined in order to produce a final complete summary. With this approach we can decompose the problem of long document summarization into smaller and simpler problems, reducing computational complexity and creating more training examples, which at the same time contain less noise in the target summaries compared to the standard approach. We demonstrate that this approach paired with different summarization models, including sequence-to-sequence RNNs and Transformers, can lead to improved summarization performance. Our best models achieve results that are on par with the state-of-the-art in two two publicly available datasets of academic articles."},
{"paper": "Abstractive Summarization of Spoken andWritten Instructions with BERT", "abstract": "Summarization of speech is a difficult problem due to the spontaneity of the flow, disfluencies, and other issues that are not usually encountered in written texts. Our work presents the first application of the BERTSum model to conversational language.We generate abstractive summaries of narrated instructional videos across a wide variety of topics, from gardening and cooking to software configuration and sports. In order to enrich the vocabulary, we use transfer learning and pretrain the model on a few large cross-domain datasets in both written and spoken English. We also do preprocessing of transcripts to restore sentence segmentation and punctuation in the output of an ASR system. The results are evaluated with ROUGE and Content-F1 scoring for the How2 and WikiHow datasets. We engage human judges to score a set of summaries randomly selected from a dataset curated from HowTo100M and YouTube. Based on blind evaluation, we achieve a level of textual fluency and utility close to that of summaries written by human content creators. The model beats current SOTA when applied to WikiHow articles that vary widely in style and topic, while showing no performance regression on the canonical CNN/DailyMail dataset. Due to the high generalizability of the model across different styles and domains, it has great potential to improve accessibility and discoverability of internet content. We envision this integrated as a feature in intelligent virtual assistants, enabling them to summarize both written and spoken instructional content upon request."},
{"paper": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism.To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."},
{"paper": "Better Fine-Tuning by Reducing Representational Collapse", "abstract": "Although widely adopted, existing approaches for fine-tuning pre-trained language models have been shown to be unstable across hyper-parameter settings, motivating recent work on trust region methods. In this paper, we present a simplified and efficient method rooted in trust region theory that replaces previously used adversarial objectives with parametric noise (sampling from either a normal or uniform distribution), thereby discouraging representation change during fine-tuning when possible without hurting performance.We also introduce a new analysis to motivate the use of trust region methods more generally, by studying representational collapse; the degradation of generalizable representations from pre-trained models as they are fine-tuned for a specific end task. Extensive experiments show that our fine-tuning method matches or exceeds the performance of previous trust region methods on a range of understanding and generation tasks (including DailyMail/CNN, Gigaword, Reddit TIFU, and the GLUE benchmark), while also being much faster. We also show that it is less prone to representation collapse; the pre-trained models maintain more generalizable representations every time they are fine-tuned."},
{"paper": "Locally Masked Convolution for Autoregressive Models", "abstract": "High-dimensional generative models have many applications including image compression, multimedia generation, anomaly detection and data completion. State-of-the-art estimators for natural images are autoregressive, decomposing the joint distribution over pixels into a product of conditionals parameterized by a deep neural network, e.g. a convolutional neural network such as the PixelCNN.However, PixelCNNs only model a single decomposition of the joint, and only a single generation order is efficient. For tasks such as image completion, these models are unable to use much of the observed context. To generate data in arbitrary orders, we introduce LMConv: a simple modification to the standard 2D convolution that allows arbitrary masks to be applied to the weights at each location in the image. Using LMConv, we learn an ensemble of distribution estimators that share parameters but differ in generation order, achieving improved performance on whole-image density estimation (2.89 bpd on unconditional CIFAR10), as well as globally coherent image completions. Our code is available at https://ajayjain.github.io/lmconv."},
{"paper": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation", "abstract": "Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method.To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves state-of-the-art results with a much smaller amount of pre-training data and parameters on a range of language generation tasks, including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue generation (Persona-Chat) and generative question answering (CoQA)."},
{"paper": "Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search", "abstract": "In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture.To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN."},
{"paper": "Analyzing and Improving the Image Quality of StyleGAN", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them.In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."},
{"paper": "Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling", "abstract": "The unconditional generation of high fidelity images is a longstanding\nbenchmark for testing the performance of image decoders. Autoregressive image\nmodels have been able to generate small images unconditionally, but the\nextension of these methods to large images where fidelity can be more readily\nassessed has remained an open problem.Among the major challenges are the\ncapacity to encode the vast previous context and the sheer difficulty of\nlearning a distribution that preserves both global semantic coherence and\nexactness of detail. To address the former challenge, we propose the Subscale\nPixel Network (SPN), a conditional decoder architecture that generates an image\nas a sequence of sub-images of equal size. The SPN compactly captures\nimage-wide spatial dependencies and requires a fraction of the memory and the\ncomputation required by other fully autoregressive models. To address the\nlatter challenge, we propose to use Multidimensional Upscaling to grow an image\nin both size and depth via intermediate stages utilising distinct SPNs. We\nevaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of\nImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in\nmultiple settings, set up new benchmark results in previously unexplored\nsettings and are able to generate very high fidelity large scale samples on the\nbasis of both datasets."},
{"paper": "A Style-Based Generator Architecture for Generative Adversarial Networks", "abstract": "We propose an alternative generator architecture for generative adversarial\nnetworks, borrowing from style transfer literature. The new architecture leads\nto an automatically learned, unsupervised separation of high-level attributes\n(e.g., pose and identity when trained on human faces) and stochastic variation\nin the generated images (e.g., freckles, hair), and it enables intuitive,\nscale-specific control of the synthesis.The new generator improves the\nstate-of-the-art in terms of traditional distribution quality metrics, leads to\ndemonstrably better interpolation properties, and also better disentangles the\nlatent factors of variation. To quantify interpolation quality and\ndisentanglement, we propose two new, automated methods that are applicable to\nany generator architecture. Finally, we introduce a new, highly varied and\nhigh-quality dataset of human faces."},
{"paper": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence\ngeneration or transformation problem. Recent work has shown that self-attention\nis an effective way of modeling textual sequences.In this work, we generalize\na recently proposed model architecture based on self-attention, the\nTransformer, to a sequence modeling formulation of image generation with a\ntractable likelihood. By restricting the self-attention mechanism to attend to\nlocal neighborhoods we significantly increase the size of images the model can\nprocess in practice, despite maintaining significantly larger receptive fields\nper layer than typical convolutional neural networks. While conceptually\nsimple, our generative models significantly outperform the current state of the\nart in image generation on ImageNet, improving the best published negative\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\nconfiguration of our architecture. In a human evaluation study, we find that\nimages generated by our super-resolution model fool human observers three times\nmore often than the previous state of the art."},
{"paper": "Denoising Diffusion Probabilistic Models", "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding.On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion"},
{"paper": "Lessons Learned from the Training of GANs on Artificial Datasets", "abstract": "Generative Adversarial Networks (GANs) have made great progress in synthesizing realistic images in recent years. However, they are often trained on image datasets with either too few samples or too many classes belonging to different data distributions.Consequently, GANs are prone to underfitting or overfitting, making the analysis of them difficult and constrained. Therefore, in order to conduct a thorough study on GANs while obviating unnecessary interferences introduced by the datasets, we train them on artificial datasets where there are infinitely many samples and the real data distributions are simple, high-dimensional and have structured manifolds. Moreover, the generators are designed such that optimal sets of parameters exist. Empirically, we find that under various distance measures, the generator fails to learn such parameters with the GAN training procedure. We also find that training mixtures of GANs leads to more performance gain compared to increasing the network depth or width when the model complexity is high enough. Our experimental results demonstrate that a mixture of generators can discover different modes or different classes automatically in an unsupervised setting, which we attribute to the distribution of the generation and discrimination tasks across multiple generators and discriminators. As an example of the generalizability of our conclusions to realistic datasets, we train a mixture of GANs on the CIFAR-10 dataset and our method significantly outperforms the state-of-the-art in terms of popular metrics, i.e., Inception Score (IS) and Fr\\'echet Inception Distance (FID)."},
{"paper": "Efficient Content-Based Sparse Attention with Routing Transformers", "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length.Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\\left(n^{1.5}d\\right)$ from $O\\left(n^2d\\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192."},
{"paper": "Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction", "abstract": "Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step.One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/ ."},
{"paper": "DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation", "abstract": "Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: (1) Training multiple networks can increase the run time and affect the convergence and stability of the generative model; (2) These approaches ignore the quality of early-stage generator images; (3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high quality and visually realistic images only employing a single generator/discriminator pair.The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image quality by ensuring the vivid shape and the perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. Visualization of the attention maps shows that the channel-aware attention module is able to localize the discriminative regions, while the pixel-aware attention module has the ability to capture the globally visual contents for the generation of an image."},
{"paper": "LOGAN: Latent Optimisation for Generative Adversarial Networks", "abstract": "Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes.In this work, we improve CS-GAN with natural gradient-based latent optimisation and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet ($128 \\times 128$) dataset. Our model achieves an Inception Score (IS) of $148$ and an Fr\\'echet Inception Distance (FID) of $3.4$, an improvement of $17\\%$ and $32\\%$ in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters."},
{"paper": "Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors", "abstract": "Oriented object detection in aerial images is a challenging task as the objects in aerial images are displayed in arbitrary directions and are usually densely packed. Current oriented object detection methods mainly rely on two-stage anchor-based detectors.However, the anchor-based detectors typically suffer from a severe imbalance issue between the positive and negative anchor boxes. To address this issue, in this work we extend the horizontal keypoint-based object detector to the oriented object detection task. In particular, we first detect the center keypoints of the objects, based on which we then regress the box boundary-aware vectors (BBAVectors) to capture the oriented bounding boxes. The box boundary-aware vectors are distributed in the four quadrants of a Cartesian coordinate system for all arbitrarily oriented objects. To relieve the difficulty of learning the vectors in the corner cases, we further classify the oriented bounding boxes into horizontal and rotational bounding boxes. In the experiment, we show that learning the box boundary-aware vectors is superior to directly predicting the width, height, and angle of an oriented bounding box, as adopted in the baseline method. Besides, the proposed method competes favorably with state-of-the-art methods. Code is available at https://github.com/yijingru/BBAVectors-Oriented-Object-Detection."},
{"paper": "How To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning", "abstract": "With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion.This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset. On 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects."},
{"paper": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks (GANs) have shown remarkable success\nin various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN)\naiming at generating high-resolution photo-realistic images.First, we propose\na two-stage generative adversarial network architecture, StackGAN-v1, for\ntext-to-image synthesis. The Stage-I GAN sketches the primitive shape and\ncolors of the object based on given text description, yielding low-resolution\nimages. The Stage-II GAN takes Stage-I results and text descriptions as inputs,\nand generates high-resolution images with photo-realistic details. Second, an\nadvanced multi-stage generative adversarial network architecture, StackGAN-v2,\nis proposed for both conditional and unconditional generative tasks. Our\nStackGAN-v2 consists of multiple generators and discriminators in a tree-like\nstructure; images at multiple scales corresponding to the same scene are\ngenerated from different branches of the tree. StackGAN-v2 shows more stable\ntraining behavior than StackGAN-v1 by jointly approximating multiple\ndistributions. Extensive experiments demonstrate that the proposed stacked\ngenerative adversarial networks significantly outperform other state-of-the-art\nmethods in generating photo-realistic images."},
{"paper": "Detecting People in Artwork with CNNs", "abstract": "CNNs have massively improved performance in object detection in photographs. However research into object detection in artwork remains limited.We show\nstate-of-the-art performance on a challenging dataset, People-Art, which\ncontains people from photos, cartoons and 41 different artwork movements. We\nachieve this high performance by fine-tuning a CNN for this task, thus also\ndemonstrating that training CNNs on photos results in overfitting for photos:\nonly the first three or four layers transfer from photos to artwork. Although\nthe CNN's performance is the highest yet, it remains less than 60\\% AP,\nsuggesting further work is needed for the cross-depiction problem. The final\npublication is available at Springer via\nhttp://dx.doi.org/10.1007/978-3-319-46604-0_57"},
{"paper": "On Generalizing Detection Models for Unconstrained Environments", "abstract": "Object detection has seen tremendous progress in recent years. However, current algorithms don't generalize well when tested on diverse data distributions.We address the problem of incremental learning in object detection on the India Driving Dataset (IDD). Our approach involves using multiple domain-specific classifiers and effective transfer learning techniques focussed on avoiding catastrophic forgetting. We evaluate our approach on the IDD and BDD100K dataset. Results show the effectiveness of our domain adaptive approach in the case of domain shifts in environments."},
{"paper": "Look-into-Object: Self-supervised Structure Modeling for Object Recognition", "abstract": "Most object recognition approaches predominantly focus on learning discriminative visual patterns while overlooking the holistic object structure. Though important, structure modeling usually requires significant manual annotations and therefore is labor-intensive.In this paper, we propose to \"look into object\" (explicitly yet intrinsically model the object structure) through incorporating self-supervisions into the traditional framework. We show the recognition backbone can be substantially enhanced for more robust representation learning, without any cost of extra annotation and inference speed. Specifically, we first propose an object-extent learning module for localizing the object according to the visual patterns shared among the instances in the same category. We then design a spatial context learning module for modeling the internal structures of the object, through predicting the relative positions within the extent. These two modules can be easily plugged into any backbone networks during training and detached at inference time. Extensive experiments show that our look-into-object approach (LIO) achieves large performance gain on a number of benchmarks, including generic object recognition (ImageNet) and fine-grained object recognition tasks (CUB, Cars, Aircraft). We also show that this learning paradigm is highly generalizable to other tasks such as object detection and segmentation (MS COCO). Project page: https://github.com/JDAI-CV/LIO."},
{"paper": "RODEO: Replay for Online Object Detection", "abstract": "Humans can incrementally learn to do new visual detection tasks, which is a huge challenge for today's computer vision systems. Incrementally trained deep learning models lack backwards transfer to previously seen classes and suffer from a phenomenon known as $\"catastrophic forgetting.\"$ In this paper, we pioneer online streaming learning for object detection, where an agent must learn examples one at a time with severe memory and computational constraints. In object detection, a system must output all bounding boxes for an image with the correct label. Unlike earlier work, the system described in this paper can learn this task in an online manner with new classes being introduced over time. We achieve this capability by using a novel memory replay mechanism that efficiently replays entire scenes. We achieve state-of-the-art results on both the PASCAL VOC 2007 and MS COCO datasets."},
{"paper": "Scene Graph Generation from Objects, Phrases and Region Captions", "abstract": "Object detection, scene graph generation and region captioning, which are\nthree scene understanding tasks at different semantic levels, are tied\ntogether: scene graphs are generated on top of objects detected in an image\nwith their pairwise relationship predicted, while region captioning gives a\nlanguage description of the objects, their attributes, relations, and other\ncontext information. In this work, to leverage the mutual connections across\nsemantic levels, we propose a novel neural network model, termed as Multi-level\nScene Description Network (denoted as MSDN), to solve the three vision tasks\njointly in an end-to-end manner.Objects, phrases, and caption regions are\nfirst aligned with a dynamic graph based on their spatial and semantic\nconnections. Then a feature refining structure is used to pass messages across\nthe three levels of semantic tasks through the graph. We benchmark the learned\nmodel on three tasks, and show the joint learning across three tasks with our\nproposed method can bring mutual improvements over previous models. Particularly, on the scene graph generation task, our proposed method\noutperforms the state-of-art method with more than 3% margin."},
{"paper": "IterDet: Iterative Scheme for ObjectDetection in Crowded Environments", "abstract": "Deep learning-based detectors usually produce a redundant set of object bounding boxes including many duplicate detections of the same object. These boxes are then filtered using non-maximum suppression (NMS) in order to select exactly one bounding box per object of interest.This greedy scheme is simple and provides sufficient accuracy for isolated objects but often fails in crowded environments, since one needs to both preserve boxes for different objects and suppress duplicate detections. In this work we develop an alternative iterative scheme, where a new subset of objects is detected at each iteration. Detected boxes from the previous iterations are passed to the network at the following iterations to ensure that the same object would not be detected twice. This iterative scheme can be applied to both one-stage and two-stage object detectors with just minor modifications of the training and inference procedures. We perform extensive experiments with two different baseline detectors on four datasets and show significant improvement over the baseline, leading to state-of-the-art performance on CrowdHuman and WiderPerson datasets. The source code and the trained models are available at https://github.com/saic-vul/iterdet."},
{"paper": "Patch Refinement -- Localized 3D Object Detection", "abstract": "We introduce Patch Refinement a two-stage model for accurate 3D object detection and localization from point cloud data. Patch Refinement is composed of two independently trained Voxelnet-based networks, a Region Proposal Network (RPN) and a Local Refinement Network (LRN).We decompose the detection task into a preliminary Bird's Eye View (BEV) detection step and a local 3D detection step. Based on the proposed BEV locations by the RPN, we extract small point cloud subsets (\"patches\"), which are then processed by the LRN, which is less limited by memory constraints due to the small area of each patch. Therefore, we can apply encoding with a higher voxel resolution locally. The independence of the LRN enables the use of additional augmentation techniques and allows for an efficient, regression focused training as it uses only a small fraction of each scene. Evaluated on the KITTI 3D object detection benchmark, our submission from January 28, 2019, outperformed all previous entries on all three difficulties of the class car, using only 50 % of the available training data and only LiDAR information."},
{"paper": "Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous Vehicles", "abstract": "This paper presents two variations of architecture referred to as RANet and BIRANet. The proposed architecture aims to use radar signal data along with RGB camera images to form a robust detection network that works efficiently, even in variable lighting and weather conditions such as rain, dust, fog, and others.First, radar information is fused in the feature extractor network. Second, radar points are used to generate guided anchors. Third, a method is proposed to improve region proposal network targets. BIRANet yields 72.3/75.3% average AP/AR on the NuScenes dataset, which is better than the performance of our base network Faster-RCNN with Feature pyramid network(FFPN). RANet gives 69.6/71.9% average AP/AR on the same dataset, which is reasonably acceptable performance. Also, both BIRANet and RANet are evaluated to be robust towards the noise."},
{"paper": "ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes", "abstract": "3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VoteNet).However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VoteNet and propose a 3D detection architecture called ImVoteNet specialized for RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice."},
{"paper": "Center-based 3D Object Detection and Tracking", "abstract": "Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges.Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. We use a keypoint detector to find centers of objects and simply regress to other attributes, including 3D size, 3D orientation, and velocity. In our center-based framework, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. On the nuScenes dataset, our point-based representations perform $3$-$4$ mAP higher than the box-based counterparts for 3D detection, and 6 AMOTA higher for 3D tracking. Our real-time model runs end-to-end 3D detection and tracking at $30$ FPS with $54.2$ AMOTA and $48.3$ mAP while the best single model achieves $60.3$ mAP for 3D detection and $63.8$ AMOTA for 3D tracking. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint."},
{"paper": "SSD: Single Shot MultiBox Detector", "abstract": "We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location.At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd ."},
{"paper": "A Hierarchical Graph Network for 3D Object Detection on Point Clouds", "abstract": "3D object detection on point clouds finds many applications. However, most known point cloud object detection methods did not adequately accommodate the characteristics (e.g., sparsity) of point clouds, and thus some key semantic information (e.g., shape information) is not well captured.In this paper, we propose a new graph convolution (GConv) based hierarchical graph network (HGNet) for 3D object detection, which processes raw point clouds directly to predict 3D bounding boxes. HGNet effectively captures the relationship of the points and utilizes the multi-level semantics for object detection. Specially, we propose a novel shape-attentive GConv (SA-GConv) to capture the local shape features, by modelling the relative geometric positions of points to describe object shapes. An SA-GConv based U-shape network captures the multi-level features, which are mapped into an identical feature space by an improved voting module and then further utilized to generate proposals. Next, a new GConv based Proposal Reasoning Module reasons on the proposals considering the global scene semantics, and the bounding boxes are then predicted. Consequently, our new framework outperforms state-of-the-art methods on two large-scale point cloud datasets, by  4% mean average precision (mAP) on SUN RGB-D and by  3% mAP on ScanNet-V2."},
{"paper": "Point-Voxel CNN for Efficient 3D Deep Learning", "abstract": "We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models.However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80% of the time is wasted on structuring the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10x GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7x measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2x speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4% mAP on average with 1.5x measured speedup and GPU memory reduction."},
{"paper": "IPOD: Intensive Point-based Object Detector for Point Cloud", "abstract": "We present a novel 3D object detection framework, named IPOD, based on raw\npoint cloud. It seeds object proposal for each point, which is the basic\nelement.This paradigm provides us with high recall and high fidelity of\ninformation, leading to a suitable way to process point cloud data. We design\nan end-to-end trainable architecture, where features of all points within a\nproposal are extracted from the backbone network and achieve a proposal feature\nfor final bounding inference. These features with both context information and\nprecise point cloud coordinates yield improved performance. We conduct\nexperiments on KITTI dataset, evaluating our performance in terms of 3D object\ndetection, Bird's Eye View (BEV) detection and 2D object detection. Our method\naccomplishes new state-of-the-art , showing great advantage on the hard set."},
{"paper": "Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection", "abstract": "In this work, we propose a novel method termed \\emph{Frustum ConvNet (F-ConvNet)} for amodal 3D object detection from point clouds. Given 2D region proposals in an RGB image, our method first generates a sequence of frustums for each region proposal, and uses the obtained frustums to group local points.F-ConvNet aggregates point-wise features as frustum-level feature vectors, and arrays these feature vectors as a feature map for use of its subsequent component of fully convolutional network (FCN), which spatially fuses frustum-level features and supports an end-to-end and continuous estimation of oriented boxes in the 3D space. We also propose component variants of F-ConvNet, including an FCN variant that extracts multi-resolution frustum features, and a refined use of F-ConvNet over a reduced 3D space. Careful ablation studies verify the efficacy of these component variants. F-ConvNet assumes no prior knowledge of the working 3D environment and is thus dataset-agnostic. We present experiments on both the indoor SUN-RGBD and outdoor KITTI datasets. F-ConvNet outperforms all existing methods on SUN-RGBD, and at the time of submission it outperforms all published works on the KITTI benchmark. Code has been made available at: {\\url{https://github.com/zhixinwang/frustum-convnet}.}"},
{"paper": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "abstract": "We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features.It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds."},
{"subfield": "Data Augmentation", "tasks": {"0": "Named Entity Recognition", "1": "Chinese Named Entity Recognition", "2": "Nested Named Entity Recognition", "3": "Medical Named Entity Recognition", "4": "Cross-Domain Named Entity Recognition", "5": "Named Entity Recognition In Vietnamese", "6": "Multi-Grained Named Entity Recognition", "7": "Scientific Concept Extraction"}},
{"paper": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds", "abstract": "Accurate 3D object detection from point clouds has become a crucial component in autonomous driving. However, the volumetric representations and the projection methods in previous works fail to establish the relationships between the local point sets.In this paper, we propose Sparse Voxel-Graph Attention Network (SVGA-Net), a novel end-to-end trainable network which mainly contains voxel-graph module and sparse-to-dense regression module to achieve comparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net constructs the local complete graph within each divided 3D spherical voxel and global KNN graph through all voxels. The local and global graphs serve as the attention mechanism to enhance the extracted features. In addition, the novel sparse-to-dense regression module enhances the 3D box estimation accuracy through feature maps aggregation at different levels. Experiments on KITTI detection benchmark demonstrate the efficiency of extending the graph representation to 3D object detection and the proposed SVGA-Net can achieve decent detection accuracy."},
{"subfield": "Sentiment Analysis", "tasks": {"0": "Text Generation", "1": "Dialogue Generation", "2": "Data-to-Text Generation", "3": "Multi-Document Summarization", "4": "Text Style Transfer", "5": "Paraphrase Generation", "6": "Spelling Correction", "7": "Visual Storytelling", "8": "Table-to-Text Generation", "9": "Conditional Text Generation", "10": "Text Infilling", "11": "Paper generation", "12": "Distractor Generation", "13": "Concept-To-Text Generation", "14": "News Generation", "15": "Question-Answer-Generation", "16": "Story Completion", "17": "Sonnet Generation"}},
{"paper": "A Simple Pooling-Based Design for Real-Time Salient Object Detection", "abstract": "We solve the problem of salient object detection by investigating how to\nexpand the role of pooling in convolutional neural networks. Based on the\nU-shape architecture, we first build a global guidance module (GGM) upon the\nbottom-up pathway, aiming at providing layers at different feature levels the\nlocation information of potential salient objects.We further design a feature\naggregation module (FAM) to make the coarse-level semantic information well\nfused with the fine-level features from the top-down pathway. By adding FAMs\nafter the fusion operations in the top-down pathway, coarse-level features from\nthe GGM can be seamlessly merged with features at various scales. These two\npooling-based modules allow the high-level semantic features to be\nprogressively refined, yielding detail enriched saliency maps. Experiment\nresults show that our proposed approach can more accurately locate the salient\nobjects with sharpened details and hence substantially improve the performance\ncompared to the previous state-of-the-arts. Our approach is fast as well and\ncan run at a speed of more than 30 FPS when processing a $300 \\times 400$\nimage. Code can be found at http://mmcheng.net/poolnet/."},
{"subfield": "Text Classification", "tasks": {"0": "Sentiment Analysis", "1": "Aspect-Based Sentiment Analysis", "2": "Multimodal Sentiment Analysis", "3": "Twitter Sentiment Analysis", "4": "Arabic Sentiment Analysis", "5": "Fine-Grained Opinion Analysis", "6": "Aspect Sentiment Triplet Extraction", "7": "Persian Sentiment Analysis"}},
{"subfield": "Language Modelling", "tasks": {"0": "Text Classification", "1": "Topic Models", "2": "Document Classification", "3": "Sentence Classification", "4": "Emotion Classification", "5": "Text Categorization", "6": "Multi-Label Text Classification", "7": "Semi-Supervised Text Classification", "8": "Citation Intent Classification", "9": "Cross-Domain Text Classification", "10": "Hierarchical Text Classification of Blurbs (GermEval 2019)"}},
{"paper": "CSPNet: A New Backbone that can Enhance Learning Capability of CNN", "abstract": "Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology.In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks."},
{"subfield": "Question Answering", "tasks": {"0": "Question Answering", "1": "Open-Domain Question Answering", "2": "Answer Selection", "3": "Community Question Answering", "4": "Knowledge Base Question Answering", "5": "Generative Question Answering", "6": "Mathematical Question Answering", "7": "Multilingual Machine Comprehension in English Hindi", "8": "Question Quality Assessment"}},
{"subfield": "Machine Translation", "tasks": {"0": "Machine Translation", "1": "Transliteration", "2": "Unsupervised Machine Translation", "3": "Low-Resource Neural Machine Translation", "4": "Multimodal Machine Translation", "5": "Clinical Language Translation", "6": "Legal Document Translation"}},
{"paper": "Cascaded Partial Decoder for Fast and Accurate Salient Object Detection", "abstract": "Existing state-of-the-art salient object detection networks rely on\naggregating multi-level features of pre-trained convolutional neural networks\n(CNNs). Compared to high-level features, low-level features contribute less to\nperformance but cost more computations because of their larger spatial\nresolutions.In this paper, we propose a novel Cascaded Partial Decoder (CPD)\nframework for fast and accurate salient object detection. On the one hand, the\nframework constructs partial decoder which discards larger resolution features\nof shallower layers for acceleration. On the other hand, we observe that\nintegrating features of deeper layers obtain relatively precise saliency map. Therefore we directly utilize generated saliency map to refine the features of\nbackbone network. This strategy efficiently suppresses distractors in the\nfeatures and significantly improves their representation ability. Experiments\nconducted on five benchmark datasets exhibit that the proposed model not only\nachieves state-of-the-art performance but also runs much faster than existing\nmodels. Besides, the proposed framework is further applied to improve existing\nmulti-level feature aggregation models and significantly improve their\nefficiency and accuracy."},
{"task": "Scientific Concept Extraction", "papers": {"0": "Domain-independent Extraction of Scientific Concepts from Research Articles"}},
{"paper": "EfficientDet: Scalable and Efficient Object Detection", "abstract": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency.First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet."},
{"task": "Multi-Grained Named Entity Recognition", "papers": {}},
{"task": "Named Entity Recognition In Vietnamese", "papers": {"0": "A Deep Neural Network Model for the Task of Named Entity Recognition"}},
{"task": "Cross-Domain Named Entity Recognition", "papers": {"0": "Zero-Resource Cross-Domain Named Entity Recognition"}},
{"task": "Medical Named Entity Recognition", "papers": {"0": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "1": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "2": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"}},
{"task": "Nested Named Entity Recognition", "papers": {"0": "A Unified MRC Framework for Named Entity Recognition", "1": "A Unified MRC Framework for Named Entity Recognition", "2": "A Unified MRC Framework for Named Entity Recognition"}},
{"task": "Sonnet Generation", "papers": {}},
{"task": "Chinese Named Entity Recognition", "papers": {"0": "Dice Loss for Data-imbalanced NLP Tasks", "1": "FLAT: Chinese NER Using Flat-Lattice Transformer", "2": "Dice Loss for Data-imbalanced NLP Tasks", "3": "Glyce: Glyph-vectors for Chinese Character Representations", "4": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "5": "Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism", "6": "Glyce: Glyph-vectors for Chinese Character Representations"}},
{"task": "Paper generation", "papers": {"0": "Paper Abstract Writing through Editing Mechanism"}},
{"task": "Story Completion", "papers": {}},
{"task": "Question-Answer-Generation", "papers": {}},
{"task": "Named Entity Recognition", "papers": {"0": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention", "1": "Dice Loss for Data-imbalanced NLP Tasks", "2": "A Unified MRC Framework for Named Entity Recognition", "3": "Named Entity Recognition as Dependency Parsing", "4": "Named Entity Recognition as Dependency Parsing", "5": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations", "6": "Contextual String Embeddings for Sequence Labeling", "7": "Reinforcement-based denoising of distantly supervised NER with partial annotation", "8": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining", "9": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining", "10": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training", "11": "Automated Concatenation of Embeddings for Structured Prediction", "12": "Automated Concatenation of Embeddings for Structured Prediction", "13": "Automated Concatenation of Embeddings for Structured Prediction", "14": "Automated Concatenation of Embeddings for Structured Prediction", "15": "A General Framework for Information Extraction using Dynamic Span Graphs", "16": "Using Similarity Measures to Select Pretraining Data for NER", "17": "BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks", "18": "BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks", "19": "Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition", "20": "Dependency-Guided LSTM-CRF for Named Entity Recognition", "21": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms", "22": "CamemBERT: a Tasty French Language Model", "23": "Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach", "24": "LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text"}},
{"task": "News Generation", "papers": {}},
{"task": "Concept-To-Text Generation", "papers": {"0": "Fake News Detection as Natural Language Inference"}},
{"task": "Hierarchical Text Classification of Blurbs (GermEval 2019)", "papers": {}},
{"task": "Legal Document Translation", "papers": {}},
{"paper": "Domain-independent Extraction of Scientific Concepts from Research Articles", "abstract": "We examine the novel task of domain-independent scientific concept extraction from abstracts of scholarly articles and present two contributions. First, we suggest a set of generic scientific concepts that have been identified in a systematic annotation process.This set of concepts is utilised to annotate a corpus of scientific abstracts from 10 domains of Science, Technology and Medicine at the phrasal level in a joint effort with domain experts. The resulting dataset is used in a set of benchmark experiments to (a) provide baseline performance for this task, (b) examine the transferability of concepts between domains. Second, we present two deep learning systems as baselines. In particular, we propose active learning to deal with different domains in our task. The experimental results show that (1) a substantial agreement is achievable by non-experts after consultation with domain experts, (2) the baseline system achieves a fairly high F1 score, (3) active learning enables us to nearly halve the amount of required training data."},
{"task": "Clinical Language Translation", "papers": {}},
{"paper": "Zero-Resource Cross-Domain Named Entity Recognition", "abstract": "Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. However, collecting data for low-resource target domains is not only expensive but also time-consuming.Hence, we propose a cross-domain NER model that does not use any external resources. We first introduce a Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not. We then introduce a framework called Mixture of Entity Experts (MoEE) to improve the robustness for zero-resource domain adaptation. Finally, experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources."},
{"paper": "A Deep Neural Network Model for the Task of Named Entity Recognition", "abstract": "One of the most important factors which directly and significantly affects the quality of the neural sequence labeling is the selection and encoding the input features to generate rich semantic and grammatical representation vectors. In this paper, we propose a deep neural network model to address a particular task of sequence labeling problem, the task of Named Entity Recognition (NER).The model consists of three sub-networks to fully exploit character-level and capitalization features as well as word-level contextual representation. To show the ability of our model to generalize to different languages, we evaluated the model in Russian, Vietnamese, English and Chinese and obtained state-of-the-art performances: 91.10%, 94.43%, 91.22%, 92.95% of F-Measure on Gareev's dataset, VLSP-2016, CoNLL-2003 and MSRA datasets, respectively. Besides that, our model also obtained a good performance (about 70% of F1) with using only 100 samples for training and development sets."},
{"task": "Low-Resource Neural Machine Translation", "papers": {}},
{"paper": "A Unified MRC Framework for Named Entity Recognition", "abstract": "The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not. Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels.In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the \\textsc{per} label is formalized as extracting answer spans to the question \"{\\it which person is mentioned in the text?}\". This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities for different categories requires answering two independent questions. Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER. We conduct experiments on both {\\em nested} and {\\em flat} NER datasets. Experimental results demonstrate the effectiveness of the proposed formulation. We are able to achieve vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0."},
{"task": "Multimodal Machine Translation", "papers": {}},
{"task": "Question Quality Assessment", "papers": {"0": "Predicting Subjective Features of Questions of QA Websites using BERT"}},
{"task": "Multilingual Machine Comprehension in English Hindi", "papers": {"0": "BERT Based Multilingual Machine Comprehension in English and Hindi"}},
{"paper": "LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text", "abstract": "Named entity recognition systems have the untapped potential to extract information from legal documents, which can improve\r\ninformation retrieval and decision-making processes. In this paper, a dataset for named entity recognition in Brazilian legal documents is presented.Unlike other Portuguese language datasets, this dataset is composed entirely of legal documents. In addition to tags for persons, locations, time entities and organizations, the dataset contains specific tags for law and legal cases entities. To establish a set of baseline results, we first performed experiments on another Portuguese dataset: Paramopama. This evaluation demonstrate that LSTM-CRF gives results that are significantly better than those previously reported. We then retrained LSTM-CRF, on our dataset and obtained F 1 scores of 97.04% and 88.82% for Legislation and Legal case entities, respectively. These results show the viability of the proposed dataset for legal applications."},
{"task": "Unsupervised Machine Translation", "papers": {"0": "Language Models are Few-Shot Learners", "1": "Language Models are Few-Shot Learners", "2": "Language Models are Few-Shot Learners", "3": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "4": "Language Models are Few-Shot Learners", "5": "Language Models are Few-Shot Learners", "6": "An Effective Approach to Unsupervised Machine Translation", "7": "An Effective Approach to Unsupervised Machine Translation", "8": "Cross-lingual Language Model Pretraining"}},
{"task": "Transliteration", "papers": {}},
{"paper": "Fake News Detection as Natural Language Inference", "abstract": "This report describes the entry by the Intelligent Knowledge Management (IKM) Lab in the WSDM 2019 Fake News Classification challenge. We treat the task as natural language inference (NLI).We individually train a number of the strongest NLI models as well as BERT. We ensemble these results and retrain with noisy labels in two stages. We analyze transitivity relations in the train and test sets and determine a set of test cases that can be reliably classified on this basis. The remainder of test cases are classified by our ensemble. Our entry achieves test set accuracy of 88.063% for 3rd place in the competition."},
{"task": "Machine Translation", "papers": {"0": "Understanding Back-Translation at Scale", "1": "Very Deep Transformers for Neural Machine Translation", "2": "DeLighT: Very Deep and Light-weight Transformer", "3": "Improving Neural Language Modeling via Adversarial Training", "4": "Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule", "5": "Multi-Agent Dual Learning", "6": "Cross-lingual Language Model Pretraining", "7": "Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule", "8": "BPE-Dropout: Simple and Effective Subword Regularization", "9": "Attention Is All You Need", "10": "Neural Machine Translation in Linear Time", "11": "Edinburgh Neural Machine Translation Systems for WMT 16", "12": "Edinburgh Neural Machine Translation Systems for WMT 16", "13": "Impact of Corpora Quality on Neural Machine Translation", "14": "Pay Less Attention with Lightweight and Dynamic Convolutions", "15": "Subformer: A Parameter Reduced Transformer", "16": "The University of Sydney's Machine Translation System for WMT19", "17": "Neural Machine Translation", "18": "Facebook FAIR's WMT19 News Translation Task Submission", "19": "The Evolved Transformer", "20": "Learning to Encode Position for Transformer with Continuous Dynamical Model", "21": "Neural Machine Translation of Rare Words with Subword Units", "22": "Sequence-Level Knowledge Distillation", "23": "Using LSTM to Translate French to Senegalese Local Languages: Wolof as a Case Study", "24": "Edinburgh Neural Machine Translation Systems for WMT 16", "25": "Edinburgh Neural Machine Translation Systems for WMT 16", "26": "Edinburgh Neural Machine Translation Systems for WMT 16", "27": "Tilde's Machine Translation Systems for WMT 2018", "28": "Impact of Corpora Quality on Neural Machine Translation", "29": "Tilde's Machine Translation Systems for WMT 2018", "30": "Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages", "31": "Impact of Corpora Quality on Neural Machine Translation", "32": "Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages", "33": "The University of Sydney's Machine Translation System for WMT19", "34": "The University of Sydney's Machine Translation System for WMT19", "35": "The University of Sydney's Machine Translation System for WMT19", "36": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning", "37": "Exploiting Monolingual Data at Scale for Neural Machine Translation", "38": "DeLighT: Very Deep and Light-weight Transformer", "39": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "40": "Designing the Business Conversation Corpus", "41": "Designing the Business Conversation Corpus"}},
{"paper": "FLAT: Chinese NER Using Flat-Lattice Transformer", "abstract": "Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, most existing lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference-speed.In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallelization ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency."},
{"paper": "Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach", "abstract": "Knowledge about the software used in scientific investigations is necessary for different reasons, including provenance of the results, measuring software impact to attribute developers, and bibliometric software citation analysis in general. Additionally, providing information about whether and how the software and the source code are available allows an assessment about the state and role of open source software in science in general.While such analyses can be done manually, large scale analyses require the application of automated methods of information extraction and linking. In this paper, we present SoftwareKG - a knowledge graph that contains information about software mentions from more than 51,000 scientific articles from the social sciences. A silver standard corpus, created by a distant and weak supervision approach, and a gold standard corpus, created by manual annotation, were used to train an LSTM based neural network to identify software mentions in scientific articles. The model achieves a recognition rate of .82 F-score in exact matches. As a result, we identified more than 133,000 software mentions. For entity disambiguation, we used the public domain knowledge base DBpedia. Furthermore, we linked the entities of the knowledge graph to other knowledge bases such as the Microsoft Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we illustrate, how SoftwareKG can be used to assess the role of software in the social sciences."},
{"paper": "CamemBERT: a Tasty French Language Model", "abstract": "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages.This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."},
{"paper": "Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition", "abstract": "In countries that speak multiple main languages, mixing up different languages within a conversation is commonly called code-switching. Previous works addressing this challenge mainly focused on word-level aspects such as word embeddings.However, in many cases, languages share common subwords, especially for closely related languages, but also for languages that are seemingly irrelevant. Therefore, we propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic lexical representations. On the task of Named Entity Recognition for English-Spanish code-switching data, our model achieves the state-of-the-art performance in the multilingual settings. We also show that, in cross-lingual settings, our model not only leverages closely related languages, but also learns from languages with different roots. Finally, we show that combining different subunits are crucial for capturing code-switching entities."},
{"paper": "Dependency-Guided LSTM-CRF for Named Entity Recognition", "abstract": "Dependency tree structures capture long-distance and syntactic relationships between words in a sentence. The syntactic relations (e.g., nominal subject, object) can potentially infer the existence of certain named entities.In addition, the performance of a named entity recognizer could benefit from the long-distance dependencies between the words in dependency trees. In this work, we propose a simple yet effective dependency-guided LSTM-CRF model to encode the complete dependency trees and capture the above properties for the task of named entity recognition (NER). The data statistics show strong correlations between the entity types and dependency relations. We conduct extensive experiments on several standard datasets and demonstrate the effectiveness of the proposed model in improving NER and achieving state-of-the-art performance. Our analysis reveals that the significant improvements mainly result from the dependency relations and long-distance interactions provided by dependency trees."},
{"paper": "Using Similarity Measures to Select Pretraining Data for NER", "abstract": "Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve various Natural Language Processing (NLP) tasks. However, the measure and impact of similarity between pretraining data and target task data are left to intuition.We propose three cost-effective measures to quantify different aspects of similarity between source pretraining and target task data. We demonstrate that these measures are good predictors of the usefulness of pretrained models for Named Entity Recognition (NER) over 30 data pairs. Results also suggest that pretrained LMs are more effective and more predictable than pretrained word vectors, but pretrained word vectors are better when pretraining data is dissimilar."},
{"paper": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms", "abstract": "Many deep learning architectures have been proposed to model the\ncompositionality in text sequences, requiring a substantial number of\nparameters and expensive computations. However, there has not been a rigorous\nevaluation regarding the added value of sophisticated compositional functions.In this paper, we conduct a point-by-point comparative study between Simple\nWord-Embedding-based Models (SWEMs), consisting of parameter-free pooling\noperations, relative to word-embedding-based RNN/CNN models. Surprisingly,\nSWEMs exhibit comparable or even superior performance in the majority of cases\nconsidered. Based upon this understanding, we propose two additional pooling\nstrategies over learned word embeddings: (i) a max-pooling operation for\nimproved interpretability; and (ii) a hierarchical pooling operation, which\npreserves spatial (n-gram) information within text sequences. We present\nexperiments on 17 datasets encompassing three tasks: (i) (long) document\nclassification; (ii) text sequence matching; and (iii) short text tasks,\nincluding classification and tagging. The source code and datasets can be\nobtained from https:// github.com/dinghanshen/SWEM."},
{"paper": "A General Framework for Information Extraction using Dynamic Span Graphs", "abstract": "We introduce a general framework for several information extraction tasks\nthat share span representations using dynamically constructed span graphs. The\ngraphs are constructed by selecting the most confident entity spans and linking\nthese nodes with confidence-weighted relation types and coreferences.The\ndynamic span graph allows coreference and relation type confidences to\npropagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in\nwhich the only interaction between tasks is in the shared first-layer LSTM. Our\nframework significantly outperforms the state-of-the-art on multiple\ninformation extraction tasks across multiple datasets reflecting different\ndomains. We further observe that the span enumeration approach is good at\ndetecting nested span entities, with significant F1 score improvement on the\nACE dataset."},
{"paper": "BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks", "abstract": "Biomedical Named Entity Recognition (NER) is a challenging problem in biomedical information processing due to the widespread ambiguity of out of context terms and extensive lexical variations. Performance on bioNER benchmarks continues to improve due to advances like BERT, GPT, and XLNet.FLAIR (1) is an alternative embedding model which is less computationally intensive than the others mentioned. We test FLAIR and its pretrained PubMed embeddings (which we term BioFLAIR) on a variety of bio NER tasks and compare those with results from BERT-type networks. We also investigate the effects of a small amount of additional pretraining on PubMed content, and of combining FLAIR and ELMO models. We find that with the provided embeddings, FLAIR performs on-par with the BERT networks - even establishing a new state of the art on one benchmark. Additional pretraining did not provide a clear benefit, although this might change with even more pretraining being done. Stacking the FLAIR embeddings with others typically does provide a boost in the benchmark results."},
{"paper": "Reinforcement-based denoising of distantly supervised NER with partial annotation", "abstract": "Existing named entity recognition (NER) systems rely on large amounts of human-labeled data for supervision. However, obtaining large-scale annotated data is challenging particularly in specific domains like health-care, e-commerce and so on.Given the availability of domain specific knowledge resources, (e.g., ontologies, dictionaries), distant supervision is a solution to generate automatically labeled training data to reduce human effort. The outcome of distant supervision for NER, however, is often noisy. False positive and false negative instances are the main issues that reduce performance on this kind of auto-generated data. In this paper, we explore distant supervision in a supervised setup. We adopt a technique of partial annotation to address false negative cases and implement a reinforcement learning strategy with a neural network policy to identify false positive instances. Our results establish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain."},
{"paper": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining", "abstract": "Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models.However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert."},
{"paper": "Predicting Subjective Features of Questions of QA Websites using BERT", "abstract": "Community Question-Answering websites, such as StackOverflow and Quora, expect users to follow specific guidelines in order to maintain content quality. These systems mainly rely on community reports for assessing contents, which has serious problems such as the slow handling of violations, the loss of normal and experienced users' time, the low quality of some reports, and discouraging feedback to new users.Therefore, with the overall goal of providing solutions for automating moderation actions in Q&A websites, we aim to provide a model to predict 20 quality or subjective aspects of questions in QA websites. To this end, we used data gathered by the CrowdSource team at Google Research in 2019 and a fine-tuned pre-trained BERT model on our problem. Based on the evaluation by Mean-Squared-Error (MSE), the model achieved a value of 0.046 after 2 epochs of training, which did not improve substantially in the next ones. Results confirm that by simple fine-tuning, we can achieve accurate models in little time and on less amount of data."},
{"paper": "BERT Based Multilingual Machine Comprehension in English and Hindi", "abstract": "Multilingual Machine Comprehension (MMC) is a Question-Answering (QA) sub-task that involves quoting the answer for a question from a given snippet, where the question and the snippet can be in different languages. Recently released multilingual variant of BERT (m-BERT), pre-trained with 104 languages, has performed well in both zero-shot and fine-tuned settings for multilingual tasks; however, it has not been used for English-Hindi MMC yet.We, therefore, present in this article, our experiments with m-BERT for MMC in zero-shot, mono-lingual (e.g. Hindi Question-Hindi Snippet) and cross-lingual (e.g. English QuestionHindi Snippet) fine-tune setups. These model variants are evaluated on all possible multilingual settings and results are compared against the current state-of-the-art sequential QA system for these languages. Experiments show that m-BERT, with fine-tuning, improves performance on all evaluation settings across both the datasets used by the prior model, therefore establishing m-BERT based MMC as the new state-of-the-art for English and Hindi. We also publish our results on an extended version of the recently released XQuAD dataset, which we propose to use as the evaluation benchmark for future research."},
{"paper": "An Effective Approach to Unsupervised Machine Translation", "abstract": "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure.Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014."},
{"paper": "Cross-lingual Language Model Pretraining", "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for\nEnglish natural language understanding. In this work, we extend this approach\nto multiple languages and show the effectiveness of cross-lingual pretraining.We propose two methods to learn cross-lingual language models (XLMs): one\nunsupervised that only relies on monolingual data, and one supervised that\nleverages parallel data with a new cross-lingual language model objective. We\nobtain state-of-the-art results on cross-lingual classification, unsupervised\nand supervised machine translation. On XNLI, our approach pushes the state of\nthe art by an absolute gain of 4.9% accuracy. On unsupervised machine\ntranslation, we obtain 34.3 BLEU on WMT'16 German-English, improving the\nprevious state of the art by more than 9 BLEU. On supervised machine\ntranslation, we obtain a new state of the art of 38.5 BLEU on WMT'16\nRomanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available."},
{"paper": "Automated Concatenation of Embeddings for Structured Prediction", "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings.However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 23 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in the vast majority of evaluations."},
{"paper": "Designing the Business Conversation Corpus", "abstract": "While the progress of machine translation of written text has come far in the past several years thanks to the increasing availability of parallel corpora and corpora-based training technologies, automatic translation of spoken text and dialogues remains challenging even for modern systems. In this paper, we aim to boost the machine translation quality of conversational texts by introducing a newly constructed Japanese-English business conversation parallel corpus.A detailed analysis of the corpus is provided along with challenging examples for automatic translation. We also experiment with adding the corpus in a machine translation training scenario and show how the resulting system benefits from its use."},
{"paper": "Exploiting Monolingual Data at Scale for Neural Machine Translation", "abstract": "While target-side monolingual data has been proven to be very useful to improve neural machine translation (briefly, NMT) through back translation, source-side monolingual data is not well investigated. In this work, we study how to use both the source-side and target-side monolingual data for NMT, and propose an effective strategy leveraging both of them.First, we generate synthetic bitext by translating monolingual data from the two domains into the other domain using the models pretrained on genuine bitext. Next, a model is trained on a noised version of the concatenated synthetic bitext where each source sequence is randomly corrupted. Finally, the model is fine-tuned on the genuine bitext and a clean version of a subset of the synthetic bitext without adding any noise. Our approach achieves state-of-the-art results on WMT16, WMT17, WMT18 English$\\leftrightarrow$German translations and WMT19 German$\\to$French translations, which demonstrate the effectiveness of our method. We also conduct a comprehensive study on how each part in the pipeline works."},
{"paper": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "abstract": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks.MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model."},
{"paper": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "abstract": "In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws.Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at https://github.com/lancopku/MUSE"},
{"paper": "Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages", "abstract": "NoneNone"},
{"paper": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning", "abstract": "The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text.In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields $O(k\\cdot n\\log (n/k))$ connections where $k$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch."},
{"paper": "Using LSTM to Translate French to Senegalese Local Languages: Wolof as a Case Study", "abstract": "In this paper, we propose a neural machine translation system for Wolof, a low-resource Niger-Congo language. First we gathered a parallel corpus of 70000 aligned French-Wolof sentences.Then we developped a baseline LSTM based encoder-decoder architecture which was further extended to bidirectional LSTMs with attention mechanisms. Our models are trained on a limited amount of parallel French-Wolof data of approximately 35000 parallel sentences. Experimental results on French-Wolof translation tasks show that our approach produces promising translations in extremely low-resource conditions. The best model was able to achieve a good performance of 47% BLEU score."},
{"paper": "Tilde's Machine Translation Systems for WMT 2018", "abstract": "The paper describes the development process of the Tilde{'}s NMT systems that were submitted for the WMT 2018 shared task on news translation. We describe the data filtering and pre-processing workflows, the NMT system training architectures, and automatic evaluation results.For the WMT 2018 shared task, we submitted seven systems (both constrained and unconstrained) for English-Estonian and Estonian-English translation directions. The submitted systems were trained using Transformer models."},
{"paper": "Sequence-Level Knowledge Distillation", "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large.In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU."},
{"paper": "Neural Machine Translation of Rare Words with Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed\nvocabulary, but translation is an open-vocabulary problem. Previous work\naddresses the translation of out-of-vocabulary words by backing off to a\ndictionary.In this paper, we introduce a simpler and more effective approach,\nmaking the NMT model capable of open-vocabulary translation by encoding rare\nand unknown words as sequences of subword units. This is based on the intuition\nthat various word classes are translatable via smaller units than words, for\ninstance names (via character copying or transliteration), compounds (via\ncompositional translation), and cognates and loanwords (via phonological and\nmorphological transformations). We discuss the suitability of different word\nsegmentation techniques, including simple character n-gram models and a\nsegmentation based on the byte pair encoding compression algorithm, and\nempirically show that subword models improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks English-German and English-Russian by\n1.1 and 1.3 BLEU, respectively."},
{"paper": "Learning to Encode Position for Transformer with Continuous Dynamical Model", "abstract": "We introduce a new way of learning to encode position information for non-recurrent models, such as Transformer models. Unlike RNN and LSTM, which contain inductive bias by loading the input tokens sequentially, non-recurrent models are less sensitive to position.The main reason is that position information among input units is not inherently encoded, i.e., the models are permutation equivalent; this problem justifies why all of the existing models are accompanied by a sinusoidal encoding/embedding layer at the input. However, this solution has clear limitations: the sinusoidal encoding is not flexible enough as it is manually designed and does not contain any learnable parameters, whereas the position embedding restricts the maximum length of input sequences. It is thus desirable to design a new position layer that contains learnable parameters to adjust to different datasets and different architectures. At the same time, we would also like the encodings to extrapolate in accordance with the variable length of inputs. In our proposed solution, we borrow from the recent Neural ODE approach, which may be viewed as a versatile continuous version of a ResNet. This model is capable of modeling many kinds of dynamical systems. We model the evolution of encoded results along position index by such a dynamical system, thereby overcoming the above limitations of existing methods. We evaluate our new position layers on a variety of neural machine translation and language understanding tasks, the experimental results show consistent improvements over the baselines."},
{"paper": "Neural Machine Translation", "abstract": "Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges.Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation."},
{"paper": "Facebook FAIR's WMT19 News Translation Task Submission", "abstract": "This paper describes Facebook FAIR's submission to the WMT19 shared news translation task. We participate in two language pairs and four language directions, English <-> German and English <-> Russian.Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations. This system improves upon our WMT'18 submission by 4.5 BLEU points."},
{"paper": "The Evolved Transformer", "abstract": "Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer.We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original \"big\" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters."},
{"paper": "Impact of Corpora Quality on Neural Machine Translation", "abstract": "Large parallel corpora that are automatically obtained from the web,\ndocuments or elsewhere often exhibit many corrupted parts that are bound to\nnegatively affect the quality of the systems and models that learn from these\ncorpora. This paper describes frequent problems found in data and such data\naffects neural machine translation systems, as well as how to identify and deal\nwith them.The solutions are summarised in a set of scripts that remove\nproblematic sentences from input corpora."},
{"paper": "The University of Sydney's Machine Translation System for WMT19", "abstract": "This paper describes the University of Sydney's submission of the WMT 2019 shared news translation task. We participated in the Finnish$\\rightarrow$English direction and got the best BLEU(33.0) score among all the participants.Our system is based on the self-attentional Transformer networks, into which we integrated the most recent effective strategies from academic research (e.g., BPE, back translation, multi-features data selection, data augmentation, greedy model ensemble, reranking, ConMBR system combination, and post-processing). Furthermore, we propose a novel augmentation method $Cycle Translation$ and a data mixture strategy $Big$/$Small$ parallel construction to entirely exploit the synthetic corpus. Extensive experiments show that adding the above techniques can make continuous improvements of the BLEU scores, and the best result outperforms the baseline (Transformer ensemble model trained with the original parallel corpus) by approximately 5.3 BLEU score, achieving the state-of-the-art performance."},
{"paper": "Subformer: A Parameter Reduced Transformer", "abstract": "The advent of the Transformer can arguably be described as a driving force behind many of the recent advances in natural language processing. However, despite their sizeable performance improvements, as recently shown, the model is severely over-parameterized, being parameter inefficient and computationally expensive to train.Inspired by the success of parameter-sharing in pre-trained deep contextualized word representation encoders, we explore parameter-sharing methods in Transformers, with a specific focus on encoder-decoder models for sequence-to-sequence tasks such as Machine Translation. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer, a parameter efficient Transformer-based model which combines the newly proposed Sandwich-style parameter sharing technique and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization, and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters. On the WMT'14 English-German test set, we show we can perform equally well, and even sometimes outperform (+0.1 BLEU score) the Transformer-base model while using 40% less parameters. We also perform equally well as Transformer-big with 40% less parameters and outperform the model by 0.7 BLEU with 12M less parameters. We also outperform the standard Transformer-XL model, achieving a significant 3.6 lower perplexity with 37% fewer parameters."},
{"paper": "Neural Machine Translation in Linear Time", "abstract": "We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence.The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens."},
{"paper": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language\nand images. It determines the importance of context elements by comparing each\nelement to the current time step.In this paper, we show that a very\nlightweight convolution can perform competitively to the best reported\nself-attention results. Next, we introduce dynamic convolutions which are\nsimpler and more efficient than self-attention. We predict separate convolution\nkernels based solely on the current time-step in order to determine the\nimportance of context elements. The number of operations required by this\napproach scales linearly in the input length, whereas self-attention is\nquadratic. Experiments on large-scale machine translation, language modeling\nand abstractive summarization show that dynamic convolutions improve over\nstrong self-attention models. On the WMT'14 English-German test set dynamic\nconvolutions achieve a new state of the art of 29.7 BLEU."},
{"paper": "Edinburgh Neural Machine Translation Systems for WMT 16", "abstract": "We participated in the WMT 2016 shared news translation task by building\nneural translation systems for four language pairs, each trained in both\ndirections: English<->Czech, English<->German, English<->Romanian and\nEnglish<->Russian. Our systems are based on an attentional encoder-decoder,\nusing BPE subword segmentation for open-vocabulary translation with a fixed\nvocabulary.We experimented with using automatic back-translations of the\nmonolingual News corpus as additional training data, pervasive dropout, and\ntarget-bidirectional models. All reported methods give substantial\nimprovements, and we see improvements of 4.3--11.2 BLEU over our baseline\nsystems. In the human evaluation, our systems were the (tied) best constrained\nsystem for 7 out of 8 translation directions in which we participated."},
{"paper": "BPE-Dropout: Simple and Effective Subword Regularization", "abstract": "Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens.While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization."},
{"paper": "Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule", "abstract": "Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima.Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) and WMT'14 (DE-EN) datasets by just modifying the learning rate schedule of a high performing model."},
{"paper": "Multi-Agent Dual Learning", "abstract": "Dual learning has attracted much attention in machine learning, computer vision and natural language processing communities. The core idea of dual learning is to leverage the duality between the primal task (mapping from domain X to domain Y) and dual task (mapping from domain Y to X) to boost the performances of both tasks.Existing dual learning framework forms a system with two agents (one primal model and one dual model) to utilize such duality. In this paper, we extend this framework by introducing multiple primal and dual models, and propose the multi-agent dual learning framework. Experiments on neural machine translation and image translation tasks demonstrate the effectiveness of the new framework. In particular, we set a new record on IWSLT 2014 German-to-English translation with a 35.44 BLEU score, achieve a 31.03 BLEU score on WMT 2014 English-to-German translation with over 2.6 BLEU improvement over the strong Transformer baseline, and set a new record of 49.61 BLEU score on the recent WMT 2018 English-to-German translation."},
{"paper": "Improving Neural Language Modeling via Adversarial Training", "abstract": "Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting.In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks."},
{"paper": "Understanding Back-Translation at Scale", "abstract": "An effective method to improve neural machine translation with monolingual\ndata is to augment the parallel training corpus with back-translations of\ntarget language sentences. This work broadens the understanding of\nback-translation and investigates a number of methods to generate synthetic\nsource sentences.We find that in all but resource poor settings\nback-translations obtained via sampling or noised beam outputs are most\neffective. Our analysis shows that sampling or noisy synthetic data gives a\nmuch stronger training signal than data generated by beam or greedy search. We\nalso compare how synthetic data compares to genuine bitext and study various\ndomain effects. Finally, we scale to hundreds of millions of monolingual\nsentences and achieve a new state of the art of 35 BLEU on the WMT'14\nEnglish-German test set."},
{"paper": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism.We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data."},
{"paper": "DeLighT: Very Deep and Light-weight Transformer", "abstract": "We introduce a very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using DExTra, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output.Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on machine translation and language modeling tasks show that DeLighT matches the performance of baseline Transformers with significantly fewer parameters. On the WMT'14 En-Fr high resource dataset, DeLighT requires 1.8 times fewer parameters and 2 times fewer operations and achieves better performance (+0.4 BLEU score) than baseline transformers. On the WMT'16 En-Ro low resource dataset, DeLighT delivers similar performance with 2.8 times fewer parameters than baseline transformers."},
{"paper": "Very Deep Transformers for Neural Machine Translation", "abstract": "We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers.These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14 English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14 English-German (30.1 BLEU).The code and trained models will be publicly available at: https://github.com/namisan/exdeep-nmt."},
{"paper": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations", "abstract": "Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER).Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/ZihanWangKi/CrossWeigh."},
{"paper": "Named Entity Recognition as Dependency Parsing", "abstract": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009).In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points."},
{"paper": "Paper Abstract Writing through Editing Mechanism", "abstract": "We present a paper abstract writing system based on an attentive neural\nsequence-to-sequence model that can take a title as input and automatically\ngenerate an abstract. We design a novel Writing-editing Network that can attend\nto both the title and the previously generated abstract drafts and then\niteratively revise and polish the abstract.With two series of Turing tests,\nwhere the human judges are asked to distinguish the system-generated abstracts\nfrom human-written ones, our system passes Turing tests by junior domain\nexperts at a rate up to 30% and by non-expert at a rate up to 80%."},
{"paper": "Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism", "abstract": "Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available.Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each task. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-of-the-art methods."},
{"paper": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples.By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."},
{"paper": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention", "abstract": "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer.The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke."},
{"paper": "Dice Loss for Data-imbalanced NLP Tasks", "abstract": "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of background examples (or easy-negative examples) overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification."},
{"task": "Mathematical Question Answering", "papers": {}},
{"task": "Generative Question Answering", "papers": {"0": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation"}},
{"task": "Community Question Answering", "papers": {"0": "Predicting Subjective Features of Questions of QA Websites using BERT"}},
{"task": "Knowledge Base Question Answering", "papers": {"0": "Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering"}},
{"paper": "Glyce: Glyph-vectors for Chinese Character Representations", "abstract": "It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found.In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\\% on the Fudan corpus for text classification. Code found at https://github.com/ShannonAI/glyce."},
{"task": "Answer Selection", "papers": {"0": "Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots", "1": "Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering"}},
{"task": "Cross-Domain Text Classification", "papers": {}},
{"task": "Open-Domain Question Answering", "papers": {"0": "Densely Connected Attention Propagation for Reading Comprehension", "1": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering", "2": "Reading Wikipedia to Answer Open-Domain Questions", "3": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "4": "Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering"}},
{"task": "Question Answering", "papers": {"0": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention", "1": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "2": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection", "3": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "4": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "5": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "6": "Self-Attentive Associative Memory", "7": "Multi-style Generative Reading Comprehension", "8": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "9": "Language Models are Unsupervised Multitask Learners", "10": "MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller", "11": "Neural Variational Inference for Text Processing", "12": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection", "13": "Big Bird: Transformers for Longer Sequences", "14": "SentenceMIM: A Latent Variable Language Model", "15": "FQuAD: French Question Answering Dataset", "16": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding", "17": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding", "18": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding", "19": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "20": "Language Models are Few-Shot Learners", "21": "Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering", "22": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "23": "Large-scale Simple Question Answering with Memory Networks", "24": "Multi-style Generative Reading Comprehension", "25": "Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification", "26": "Language Models are Few-Shot Learners", "27": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "28": "CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension", "29": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "30": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension", "31": "Open Question Answering with Weakly Supervised Embedding Models", "32": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "33": "Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks", "34": "Generative Data Augmentation for Commonsense Reasoning", "35": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "36": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "37": "Large-scale Simple Question Answering with Memory Networks", "38": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "39": "Product-Aware Answer Generation in E-Commerce Question-Answering", "40": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference", "41": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "42": "Language Models are Few-Shot Learners", "43": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention"}},
{"task": "Citation Intent Classification", "papers": {"0": "Structural Scaffolds for Citation Intent Classification in Scientific Publications", "1": "SciBERT: A Pretrained Language Model for Scientific Text"}},
{"task": "Semi-Supervised Text Classification", "papers": {"0": "Semi-Supervised Learning with Normalizing Flows", "1": "Semi-Supervised Learning with Normalizing Flows"}},
{"paper": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "abstract": "Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences.However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE."},
{"task": "Text Categorization", "papers": {}},
{"task": "Multi-Label Text Classification", "papers": {"0": "Large-Scale Multi-Label Text Classification on EU Legislation", "1": "VICTOR: a Dataset for Brazilian Legal Documents Classification", "2": "VICTOR: a Dataset for Brazilian Legal Documents Classification", "3": "VICTOR: a Dataset for Brazilian Legal Documents Classification", "4": "Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification", "5": "Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification", "6": "Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification", "7": "Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification", "8": "MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network", "9": "MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network", "10": "MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network", "11": "An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes", "12": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model"}},
{"task": "Emotion Classification", "papers": {"0": "ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation", "1": "Modeling Label Semantics for Predicting Emotional Reactions", "2": "Practical Text Classification With Large Pre-Trained Language Models"}},
{"task": "Sentence Classification", "papers": {"0": "SciBERT: A Pretrained Language Model for Scientific Text", "1": "Structural Scaffolds for Citation Intent Classification in Scientific Publications", "2": "Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts", "3": "SciBERT: A Pretrained Language Model for Scientific Text", "4": "SciBERT: A Pretrained Language Model for Scientific Text"}},
{"task": "Document Classification", "papers": {"0": "Message Passing Attention Networks for Document Understanding", "1": "Adaptively Connected Neural Networks", "2": "Message Passing Attention Networks for Document Understanding", "3": "RMDL: Random Multimodel Deep Learning for Classification", "4": "RMDL: Random Multimodel Deep Learning for Classification", "5": "RMDL: Random Multimodel Deep Learning for Classification", "6": "DocBERT: BERT for Document Classification", "7": "Rep the Set: Neural Networks for Learning Set Representations", "8": "Speeding up Word Mover's Distance and its variants via properties of distances between embeddings", "9": "Rep the Set: Neural Networks for Learning Set Representations", "10": "Rep the Set: Neural Networks for Learning Set Representations", "11": "Rethinking Complex Neural Network Architectures for Document Classification", "12": "DocBERT: BERT for Document Classification", "13": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "14": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "15": "Message Passing Attention Networks for Document Understanding", "16": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"}},
{"task": "Persian Sentiment Analysis", "papers": {}},
{"task": "Topic Models", "papers": {"0": "Neural Variational Inference for Text Processing", "1": "TopicEq: A Joint Topic and Mathematical Equation Model for Scientific Texts"}},
{"paper": "Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering", "abstract": "The most approaches to Knowledge Base Question Answering are based on\nsemantic parsing. In this paper, we address the problem of learning vector\nrepresentations for complex semantic parses that consist of multiple entities\nand relations.Previous work largely focused on selecting the correct semantic\nrelations for a question and disregarded the structure of the semantic parse:\nthe connections between entities and the directions of the relations. We\npropose to use Gated Graph Neural Networks to encode the graph structure of the\nsemantic parse. We show on two data sets that the graph networks outperform all\nbaseline models that do not explicitly model the structure. The error analysis\nconfirms that our approach can successfully process complex semantic parses."},
{"task": "Fine-Grained Opinion Analysis", "papers": {"0": "Enhancing Opinion Role Labeling with Semantic-Aware Word Representations from Semantic Role Labeling"}},
{"task": "Arabic Sentiment Analysis", "papers": {}},
{"task": "Text Classification", "papers": {"0": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "1": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "2": "Universal Sentence Encoder", "3": "Simple Spectral Graph Convolution", "4": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "5": "How to Fine-Tune BERT for Text Classification?", "6": "Neural Attentive Bag-of-Entities Model for Text Classification", "7": "Simplifying Graph Convolutional Networks", "8": "Hierarchical Attentional Hybrid Neural Networks for Document Classification", "9": "Graph Star Net for Generalized Multi-Task Learning", "10": "Inferring the source of official texts: can SVM beat ULMFiT?", "11": "VICTOR: a Dataset for Brazilian Legal Documents Classification", "12": "VICTOR: a Dataset for Brazilian Legal Documents Classification", "13": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "14": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings", "15": "How to Fine-Tune BERT for Text Classification?", "16": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "17": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "18": "High Accuracy Rule-based Question Classification using Question Syntax and Semantics", "19": "BERT of all trades, master of some", "20": "BERT of all trades, master of some", "21": "BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text", "22": "Big Bird: Transformers for Longer Sequences", "23": "Big Bird: Transformers for Longer Sequences", "24": "Big Bird: Transformers for Longer Sequences", "25": "A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "26": "A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "27": "A Comparative Study of Feature Types for Age-Based Text Classification", "28": "Simple Spectral Graph Convolution"}},
{"task": "Aspect Sentiment Triplet Extraction", "papers": {"0": "Position-Aware Tagging for Aspect Sentiment Triplet Extraction"}},
{"paper": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference", "abstract": "Given a partial description like \"she opened the hood of the car,\" humans can\nreason about the situation and anticipate what might come next (\"then, she\nexamined the engine\"). In this paper, we introduce the task of grounded\ncommonsense inference, unifying natural language inference and commonsense\nreasoning.We present SWAG, a new dataset with 113k multiple choice questions about a\nrich spectrum of grounded situations. To address the recurring challenges of\nthe annotation artifacts and human biases found in many existing datasets, we\npropose Adversarial Filtering (AF), a novel procedure that constructs a\nde-biased dataset by iteratively training an ensemble of stylistic classifiers,\nand using them to filter the data. To account for the aggressive adversarial\nfiltering, we use state-of-the-art language models to massively oversample a\ndiverse set of potential counterfactuals. Empirical results demonstrate that\nwhile humans can solve the resulting inference problems with high accuracy\n(88%), various competitive models struggle on our task. We provide\ncomprehensive analysis that indicates significant opportunities for future\nresearch."},
{"paper": "Product-Aware Answer Generation in E-Commerce Question-Answering", "abstract": "In e-commerce portals, generating answers for product-related questions has\nbecome a crucial task. In this paper, we propose the task of product-aware\nanswer generation, which tends to generate an accurate and complete answer from\nlarge-scale unlabeled e-commerce reviews and product attributes.Unlike\nexisting question-answering problems, answer generation in e-commerce confronts\nthree main challenges: (1) Reviews are informal and noisy; (2) joint modeling\nof reviews and key-value product attributes is challenging; (3) traditional\nmethods easily generate meaningless answers. To tackle above challenges, we\npropose an adversarial learning based model, named PAAG, which is composed of\nthree components: a question-aware review representation module, a key-value\nmemory network encoding attributes, and a recurrent neural network as a\nsequence generator. Specifically, we employ a convolutional discriminator to\ndistinguish whether our generated answer matches the facts. To extract the\nsalience part of reviews, an attention-based review reader is proposed to\ncapture the most relevant words given the question. Conducted on a large-scale\nreal-world e-commerce dataset, our extensive experiments verify the\neffectiveness of each module in our proposed model. Moreover, our experiments\nshow that our model achieves the state-of-the-art performance in terms of both\nautomatic metrics and human evaluations."},
{"paper": "Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering", "abstract": "Recently, a simple combination of passage retrieval using off-the-shelf IR\ntechniques and a BERT reader was found to be very effective for question\nanswering directly on Wikipedia, yielding a large improvement over the previous\nstate of the art on a standard benchmark dataset. In this paper, we present a\ndata augmentation technique using distant supervision that exploits positive as\nwell as negative examples.We apply a stage-wise approach to fine tuning BERT\non multiple datasets, starting with data that is \"furthest\" from the test data\nand ending with the \"closest\". Experimental results show large gains in\neffectiveness over previous approaches on English QA datasets, and we establish\nnew baselines on two recent Chinese QA datasets."},
{"paper": "Generative Data Augmentation for Commonsense Reasoning", "abstract": "Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on.We investigate G-DAUG^C, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG^C consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG^C-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG^C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization."},
{"paper": "Structural Scaffolds for Citation Intent Classification in Scientific Publications", "abstract": "Identifying the intent of a citation in scientific papers (e.g., background information, use of methods, comparing results) is critical for machine reading of individual publications and automated analysis of the scientific literature. We propose structural scaffolds, a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents.Our model achieves a new state-of-the-art on an existing ACL anthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods. In addition, we introduce a new dataset of citation intents (SciCite) which is more than five times larger and covers multiple scientific domains compared with existing datasets. Our code and data are available at: https://github.com/allenai/scicite."},
{"paper": "Semi-Supervised Learning with Normalizing Flows", "abstract": "Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model.FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions."},
{"paper": "Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks", "abstract": "Multi-hop reading comprehension focuses on one type of factoid question,\nwhere a system needs to properly integrate multiple pieces of evidence to\ncorrectly answer a question. Previous work approximates global evidence with\nlocal coreference information, encoding coreference chains with DAG-styled GRU\nlayers within a gated-attention reader.However, coreference is limited in\nproviding information for rich inference. We introduce a new method for better\nconnecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph\nneural networks, namely graph convolutional network (GCN) and graph recurrent\nnetwork (GRN). Experiments on two standard datasets show that richer global\ninformation leads to better answers. Our method performs better than all\npublished results on these datasets."},
{"paper": "Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts", "abstract": "Prevalent models based on artificial neural network (ANN) for sentence\nclassification often classify sentences in isolation without considering the\ncontext in which sentences appear. This hampers the traditional sentence\nclassification approaches to the problem of sequential sentence classification,\nwhere structured prediction is needed for better overall classification\nperformance.In this work, we present a hierarchical sequential labeling\nnetwork to make use of the contextual information within surrounding sentences\nto help classify the current sentence. Our model outperforms the\nstate-of-the-art results by 2%-3% on two benchmarking datasets for sequential\nsentence classification in medical scientific abstracts."},
{"paper": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "abstract": "We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple\nand computationally-efficient model for learning bilingual distributed\nrepresentations of words which can scale to large monolingual datasets and does\nnot require word-aligned parallel training data. Instead it trains directly on\nmonolingual data and extracts a bilingual signal from a smaller set of raw-text\nsentence-aligned data.This is achieved using a novel sampled bag-of-words\ncross-lingual objective, which is used to regularize two noise-contrastive\nlanguage models for efficient cross-lingual feature learning. We show that\nbilingual embeddings learned using the proposed model outperform\nstate-of-the-art methods on a cross-lingual document classification task as\nwell as a lexical translation task on WMT11 data."},
{"paper": "Large-Scale Multi-Label Text Classification on EU Legislation", "abstract": "We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EURLEX, annotated with ~4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning.Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT's maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases."},
{"paper": "Rethinking Complex Neural Network Architectures for Document Classification", "abstract": "Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective.We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification."},
{"paper": "TopicEq: A Joint Topic and Mathematical Equation Model for Scientific Texts", "abstract": "Scientific documents rely on both mathematics and text to communicate ideas. Inspired by the topical correspondence between mathematical equations and word\ncontexts observed in scientific texts, we propose a novel topic model that\njointly generates mathematical equations and their surrounding text (TopicEq).Using an extension of the correlated topic model, the context is generated from\na mixture of latent topics, and the equation is generated by an RNN that\ndepends on the latent topic activations. To experiment with this model, we\ncreate a corpus of 400K equation-context pairs extracted from a range of\nscientific articles from arXiv, and fit the model using a variational\nautoencoder approach. Experimental results show that this joint model\nsignificantly outperforms existing topic models and equation models for\nscientific texts. Moreover, we qualitatively show that the model effectively\ncaptures the relationship between topics and mathematics, enabling novel\napplications such as topic-aware equation generation, equation topic inference,\nand topic-aware alignment of mathematical symbols and words."},
{"paper": "Practical Text Classification With Large Pre-Trained Language Models", "abstract": "Multi-emotion sentiment classification is a natural language processing (NLP)\nproblem with valuable use cases on real-world data. We demonstrate that\nlarge-scale unsupervised language modeling combined with finetuning offers a\npractical solution to this task on difficult datasets, including those with\nlabel class imbalance and domain-specific context.By training an\nattention-based Transformer network (Vaswani et al. 2017) on 40GB of text\n(Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our\nmodel achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional\nemotion classification problem (Mohammad et al. 2018), based on the Plutchik\nwheel of emotions (Plutchik 1979). These results are competitive with state of\nthe art models, including strong F1 scores on difficult (emotion) categories\nsuch as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive\nresults on rare categories such as Anticipation (0.42) and Surprise (0.37). Furthermore, we demonstrate our application on a real world text classification\ntask. We create a narrowly collected text dataset of real tweets on several\ntopics, and show that our finetuned model outperforms general purpose\ncommercially available APIs for sentiment and multidimensional emotion\nclassification on this dataset by a significant margin. We also perform a\nvariety of additional studies, investigating properties of deep learning\narchitectures, datasets and algorithms for achieving practical multidimensional\nsentiment classification. Overall, we find that unsupervised language modeling\nand finetuning is a simple framework for achieving high quality results on\nreal-world sentiment classification."},
{"paper": "Enhancing Opinion Role Labeling with Semantic-Aware Word Representations from Semantic Role Labeling", "abstract": "Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate.As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47{\\%} higher F-scores than the better one."},
{"paper": "Speeding up Word Mover's Distance and its variants via properties of distances between embeddings", "abstract": "The Word Mover's Distance (WMD) proposed by Kusner et al. is a distance between documents that takes advantage of semantic relations among words that are captured by their embeddings. This distance proved to be quite effective, obtaining state-of-art error rates for classification tasks, but is also impracticable for large collections/documents due to its computational complexity.For circumventing this problem, variants of WMD have been proposed. Among them, Relaxed Word Mover's Distance (RWMD) is one of the most successful due to its simplicity, effectiveness, and also because of its fast implementations. Relying on assumptions that are supported by empirical properties of the distances between embeddings, we propose an approach to speed up both WMD and RWMD. Experiments over 10 datasets suggest that our approach leads to a significant speed-up in document classification tasks while maintaining the same error rates."},
{"paper": "Position-Aware Tagging for Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages.Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness."},
{"paper": "Rep the Set: Neural Networks for Learning Set Representations", "abstract": "In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts.Many conventional machine learning algorithms are unable to process this kind of representations, since sets may vary in cardinality and elements lack a meaningful ordering. In this paper, we present a new neural network architecture, called RepSet, that can handle examples that are represented as sets of vectors. The proposed model computes the correspondences between an input set and some hidden sets by solving a series of network flow problems. This representation is then fed to a standard neural network architecture to produce the output. The architecture allows end-to-end gradient-based learning. We demonstrate RepSet on classification tasks, including text categorization, and graph classification, and we show that the proposed neural network achieves performance better or comparable to state-of-the-art algorithms."},
{"paper": "A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "abstract": "Wide usage of social media platforms has increased the risk of aggression, which results in mental stress and affects the lives of people negatively like psychological agony, fighting behavior, and disrespect to others. Majority of such conversations contains code-mixed languages[28].Additionally, the way used to express thought or communication style also changes from one social media plat-form to another platform (e.g., communication styles are different in twitter and Facebook). These all have increased the complexity of the problem. To solve these problems, we have introduced a unified and robust multi-modal deep learning architecture which works for English code-mixed dataset and uni-lingual English dataset both.The devised system, uses psycho-linguistic features and very ba-sic linguistic features. Our multi-modal deep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and Disconnected RNN(with Glove and FastText embedding, both). Finally, the system takes the decision based on model averaging. We evaluated our system on English Code-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from Kaggle. Experimental results show that our proposed system outperforms all the previous approaches on English code-mixed dataset and uni-lingual English dataset."},
{"paper": "High Accuracy Rule-based Question Classification using Question Syntax and Semantics", "abstract": "We present in this paper a purely rule-based system for Question Classification which we divide into two parts: The first is the extraction of relevant words from a question by use of its structure, and the second is the classification of questions based on rules that associate these words to Concepts. We achieve an accuracy of 97.2{\\%}, close to a 6 point improvement over the previous State of the Art of 91.6{\\%}.Additionally, we believe that machine learning algorithms can be applied on top of this method to further improve accuracy."},
{"paper": "BERT of all trades, master of some", "abstract": "This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw.The competition consisted of 2 subtasks in 3 languages (Bengali, English and Hindi) where the participants{'} task was to classify aggression in short texts from social media and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model placed first in English and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively."},
{"paper": "A Comparative Study of Feature Types for Age-Based Text Classification", "abstract": "The ability to automatically determine the age audience of a novel provides many opportunities for the development of information retrieval tools. Firstly, developers of book recommendation systems and electronic libraries may be interested in filtering texts by the age of the most likely readers.Further, parents may want to select literature for children. Finally, it will be useful for writers and publishers to determine which features influence whether the texts are suitable for children. In this article, we compare the empirical effectiveness of various types of linguistic features for the task of age-based classification of fiction texts. For this purpose, we collected a text corpus of book previews labeled with one of two categories -- children's or adult. We evaluated the following types of features: readability indices, sentiment, lexical, grammatical and general features, and publishing attributes. The results obtained show that the features describing the text at the document level can significantly increase the quality of machine learning models."},
{"paper": "BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text", "abstract": "There is a growing interest in understanding how humans initiate and hold conversations. The affective understanding of conversations focuses on the problem of how speakers use emotions to react to a situation and to each other.In the CL-Aff Shared Task, the organizers released Get it #OffMyChest dataset, which contains Reddit comments from casual and confessional conversations, labeled for their disclosure and supportiveness characteristics. In this paper, we introduce a predictive ensemble model exploiting the finetuned contextualized word embeddings, RoBERTa and ALBERT. We show that our model outperforms the base models in all considered metrics, achieving an improvement of $3\\%$ in the F1 score. We further conduct statistical analysis and outline deeper insights into the given dataset while providing a new characterization of impact for the dataset."},
{"paper": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings", "abstract": "One-hot CNN (convolutional neural network) has been shown to be effective for\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\ngeneral framework which jointly trains a linear model with a non-linear feature\ngenerator consisting of `text region embedding + pooling'.Under this\nframework, we explore a more sophisticated region embedding method using Long\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\neffective and efficient use of LSTM for this purpose in the supervised and\nsemi-supervised settings. The best results were obtained by combining region\nembeddings in the form of LSTM and convolution layers trained on unlabeled\ndata. The results indicate that on this task, embeddings of text regions, which\ncan convey complex concepts, are more useful than embeddings of single words in\nisolation. We report performances exceeding the previous best results on four\nbenchmark datasets."},
{"paper": "Hierarchical Attentional Hybrid Neural Networks for Document Classification", "abstract": "Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently.Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention mechanisms for document classification tasks. The main contribution of this work is the use of convolution layers to extract more meaningful, generalizable and abstract features by the hierarchical representation. The proposed method in this paper improves the results of the current attention-based approaches for document classification."},
{"paper": "Inferring the source of official texts: can SVM beat ULMFiT?", "abstract": "Official Gazettes are a rich source of relevant information to the public. Their careful examination may lead to the detection of frauds and irregularities that may prevent mismanagement of public funds.This paper presents a dataset composed of documents from the Official Gazette of the Federal District, containing both samples with document source annotation and unlabeled ones. We train, evaluate and compare a transfer learning based model that uses ULMFiT with traditional bag-of-words models that use SVM and Naive Bayes as classifiers. We find the SVM to be competitive, its performance being marginally worse than the ULMFiT while having much faster train and inference time and being less computationally expensive. Finally, we conduct ablation analysis to assess the performance impact of the ULMFiT parts."},
{"paper": "Neural Attentive Bag-of-Entities Model for Text Classification", "abstract": "This study proposes a Neural Attentive Bag-of-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts.We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec."},
{"paper": "Graph Star Net for Generalized Multi-Task Learning", "abstract": "In this work, we present graph star net (GraphStar), a novel and unified graph neural net architecture which utilizes message-passing relay and attention mechanism for multiple prediction tasks - node classification, graph classification and link prediction. GraphStar addresses many earlier challenges facing graph neural nets and achieves non-local representation without increasing the model depth or bearing heavy computational costs.We also propose a new method to tackle topic-specific sentiment analysis based on node classification and text classification as graph classification. Our work shows that 'star nodes' can learn effective graph-data representation and improve on current methods for the three tasks. Specifically, for graph classification and link prediction, GraphStar outperforms the current state-of-the-art models by 2-5% on several key benchmarks."},
{"paper": "Simplifying Graph Convolutional Networks", "abstract": "Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation.In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN."},
{"paper": "Universal Sentence Encoder", "abstract": "We present models for encoding sentences into embedding vectors that\nspecifically target transfer learning to other NLP tasks. The models are\nefficient and result in accurate performance on diverse transfer tasks.Two\nvariants of the encoding models allow for trade-offs between accuracy and\ncompute resources. For both variants, we investigate and report the\nrelationship between model complexity, resource consumption, the availability\nof transfer task training data, and task performance. Comparisons are made with\nbaselines that use word level transfer learning via pretrained word embeddings\nas well as baselines do not use any transfer learning. We find that transfer\nlearning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good\nperformance with minimal amounts of supervised training data for a transfer\ntask. We obtain encouraging results on Word Embedding Association Tests (WEAT)\ntargeted at detecting model bias. Our pre-trained sentence encoding models are\nmade freely available for download and on TF Hub."},
{"paper": "How to Fine-Tune BERT for Text Classification?", "abstract": "Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks.In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets."},
{"paper": "Simple Spectral Graph Convolution", "abstract": "Graph Convolutional Networks (GCNs) have drawn significant attention and become promising methods for learning graph representations. The most GCNs  suffer the performance loss when the depth of the model increases.Similarly to CNNs, without specially designed architectures, the performance of a network degrades quickly. Some researchers argue  that the required neighbourhood size and neural network depth are two completely orthogonal aspects of graph representation. Thus, several methods extend the neighbourhood by aggregating k-hop neighbourhoods of nodes while using shallow neural networks. However, these methods still encounter oversmoothing, high computation and storage costs. In this paper, we use the Markov diffusion kernel to derive a variant of GCN called Simple Spectral Graph Convolution (S^2GC) which is closely related to spectral models and combines strengths of both spatial and spectral methods. Our spectral analysis shows that our simple spectral graph convolution used in S^2GC is a low-pass filter which partitions networks into a few large parts. Our experimental evaluation demonstrates that S^2GC with a linear learner is competitive in text and node classification tasks. Moreover, S^2GC is comparable to other state-of-the-art methods for node clustering and community prediction tasks."},
{"paper": "RMDL: Random Multimodel Deep Learning for Classification", "abstract": "The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification.Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems."},
{"paper": "Message Passing Attention Networks for Document Understanding", "abstract": "Graph neural networks have recently emerged as a very effective framework for processing graph-structured data. These models have achieved state-of-the-art performance in many tasks.Most graph neural networks can be described in terms of message passing, vertex update, and readout functions. In this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). We also propose several hierarchical variants of MPAD. Experiments conducted on 10 standard text classification datasets show that our architectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the different components on performance. Code is publicly available at: https://github.com/giannisnik/mpad ."},
{"paper": "DocBERT: BERT for Document Classification", "abstract": "We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels.Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work."},
{"paper": "Modeling Label Semantics for Predicting Emotional Reactions", "abstract": "Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. They ignore information that may be conveyed by the emotion labels themselves.We propose that the semantics of emotion labels can guide a model's attention when representing the input story. Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task."},
{"paper": "ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation", "abstract": "We present ProxEmo, a novel end-to-end emotion prediction algorithm for socially aware robot navigation among pedestrians. Our approach predicts the perceived emotions of a pedestrian from walking gaits, which is then used for emotion-guided navigation taking into account social and proxemic constraints.To classify emotions, we propose a multi-view skeleton graph convolution-based model that works on a commodity camera mounted onto a moving robot. Our emotion recognition is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. It achieves a mean average emotion prediction precision of 82.47% on the Emotion-Gait benchmark dataset. We outperform current state-of-art algorithms for emotion recognition from 3D gaits. We highlight its benefits in terms of navigation in indoor scenes using a Clearpath Jackal robot."},
{"paper": "An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes", "abstract": "Background and Objective: Code assignment is of paramount importance in many levels in modern hospitals, from ensuring accurate billing process to creating a valid record of patient care history. However, the coding process is tedious and subjective, and it requires medical coders with extensive training.This study aims to evaluate the performance of deep-learning-based systems to automatically map clinical notes to ICD-9 medical codes. Methods: The evaluations of this research are focused on end-to-end learning methods without manually defined rules. Traditional machine learning algorithms, as well as state-of-the-art deep learning methods such as Recurrent Neural Networks and Convolution Neural Networks, were applied to the Medical Information Mart for Intensive Care (MIMIC-III) dataset. An extensive number of experiments was applied to different settings of the tested algorithm. Results: Findings showed that the deep learning-based methods outperformed other conventional machine learning methods. From our assessment, the best models could predict the top 10 ICD-9 codes with 0.6957 F1 and 0.8967 accuracy and could estimate the top 10 ICD-9 categories with 0.7233 F1 and 0.8588 accuracy. Our implementation also outperformed existing work under certain evaluation metrics. Conclusion: A set of standard metrics was utilized in assessing the performance of ICD-9 code assignment on MIMIC-III dataset. All the developed evaluation tools and resources are available online, which can be used as a baseline for further research."},
{"paper": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model", "abstract": "In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings.In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom."},
{"paper": "Adaptively Connected Neural Networks", "abstract": "This paper presents a novel adaptively connected neural network (ACNet) to\nimprove the traditional convolutional neural networks (CNNs) {in} two aspects. First, ACNet employs a flexible way to switch global and local inference in\nprocessing the internal feature representations by adaptively determining the\nconnection status among the feature nodes (e.g., pixels of the feature maps)\n\\footnote{In a computer vision domain, a node refers to a pixel of a feature\nmap{, while} in {the} graph domain, a node denotes a graph node.}.We can show\nthat existing CNNs, the classical multilayer perceptron (MLP), and the recently\nproposed non-local network (NLN) \\cite{nonlocalnn17} are all special cases of\nACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive\nexperimental analyses on {a variety of benchmarks (i.e.,} ImageNet-1k\nclassification, COCO 2017 detection and segmentation, CUHK03 person\nre-identification, CIFAR analysis, and Cora document categorization)\ndemonstrate that {ACNet} cannot only achieve state-of-the-art performance but\nalso overcome the limitation of the conventional MLP and CNN\n\\footnote{Corresponding author: Liang Lin ("},
{"paper": "MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network", "abstract": "In Multi-Label Text Classification (MLTC), one sample can belong to more than one class. It is observed that most MLTC tasks, there are dependencies or correlations among labels.Existing methods tend to ignore the relationship among labels. In this paper, a graph attention network-based model is proposed to capture the attentive dependency structure among the labels. The graph attention network uses a feature matrix and a correlation matrix to capture and explore the crucial dependencies between the labels and generate classifiers for the task. The generated classifiers are applied to sentence feature vectors obtained from the text feature extraction network(BiLSTM) to enable end-to-end training. Attention allows the system to assign different weights to neighbor nodes per label, thus allowing it to learn the dependencies among labels implicitly. The results of the proposed model are validated on five real-world MLTC datasets. The proposed model achieves similar or better performance compared to the previous state-of-the-art models."},
{"paper": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "abstract": "Understanding unstructured text is a major goal within natural language\nprocessing. Comprehension tests pose questions based on short text passages to\nevaluate such understanding.In this work, we investigate machine comprehension\non the challenging {\\it MCTest} benchmark. Partly because of its limited size,\nprior work on {\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks\narranged in a parallel hierarchy. The parallel hierarchy enables our model to\ncompare the passage, question, and answer from a variety of trainable\nperspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of\nsentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training\ndata, our Parallel-Hierarchical model sets a new state of the art for {\\it\nMCTest}, outperforming previous feature-engineered approaches slightly and\nprevious neural approaches by a significant margin (over 15\\% absolute)."},
{"paper": "Open Question Answering with Weakly Supervised Embedding Models", "abstract": "Building computers able to answer questions on any subject is a long standing\ngoal of artificial intelligence. Promising progress has recently been achieved\nby methods that learn to map questions to logical forms or database queries.Such approaches can be effective but at the cost of either large amounts of\nhuman-labeled data or by defining lexicons and grammars tailored by\npractitioners. In this paper, we instead take the radical approach of learning\nto map questions to vectorial feature representations. By mapping answers into\nthe same space one can query any knowledge base independent of its schema,\nwithout requiring any grammar or lexicon. Our method is trained with a new\noptimization procedure combining stochastic gradient descent followed by a\nfine-tuning step using the weak supervision provided by blending automatically\nand collaboratively generated resources. We empirically demonstrate that our\nmodel can capture meaningful signals from its noisy supervision leading to\nmajor improvements over paralex, the only existing method able to be trained on\nsimilar weakly labeled data."},
{"paper": "CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension", "abstract": "We present a new dataset for machine comprehension in the medical domain. Our\ndataset uses clinical case reports with around 100,000 gap-filling queries\nabout these cases.We apply several baselines and state-of-the-art neural\nreaders to the dataset, and observe a considerable gap in performance (20% F1)\nbetween the best human and machine readers. We analyze the skills required for\nsuccessful answering and show how reader performance varies depending on the\napplicable skills. We find that inferences using domain knowledge and object\ntracking are the most frequently required skills, and that recognizing omitted\ninformation and spatio-temporal reasoning are the most difficult for the\nmachines."},
{"paper": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension", "abstract": "Conversational machine comprehension requires the understanding of the\nconversation history, such as previous question/answer pairs, the document\ncontext, and the current question. To enable traditional, single-turn models to\nencode the history comprehensively, we introduce Flow, a mechanism that can\nincorporate intermediate representations generated during the process of\nanswering previous questions, through an alternating parallel processing\nstructure.Compared to approaches that concatenate previous questions/answers\nas input, Flow integrates the latent semantics of the conversation history more\ndeeply. Our model, FlowQA, shows superior performance on two recently proposed\nconversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The\neffectiveness of Flow also shows in other tasks. By reducing sequential\ninstruction understanding to conversational machine comprehension, FlowQA\noutperforms the best models on all three domains in SCONE, with +1.8% to +4.4%\nimprovement in accuracy."},
{"paper": "Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification", "abstract": "Extreme multi-label text classification (XMTC) aims at tagging a document with most relevant labels from an extremely large-scale label set. It is a challenging problem especially for the tail labels because there are only few training documents to build classifier.This paper is motivated to better explore the semantic relationship between each document and extreme labels by taking advantage of both document content and label correlation. Our objective is to establish an explicit label-aware representation for each document with a hybrid attention deep neural network model(LAHA). LAHA consists of three parts. The first part adopts a multi-label self-attention mechanism to detect the contribution of each word to labels. The second part exploits the label structure and document content to determine the semantic connection between words and labels in a same latent space. An adaptive fusion strategy is designed in the third part to obtain the final label-aware document representation so that the essence of previous two parts can be sufficiently integrated. Extensive experiments have been conducted on six benchmark datasets by comparing with the state-of-the-art methods. The results show the superiority of our proposed LAHA method, especially on the tail labels."},
{"paper": "Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification", "abstract": "For many applications of question answering (QA), being able to explain why a given model chose an answer is critical. However, the lack of labeled data for answer justifications makes learning this difficult and expensive.Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications, where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either. We propose a neural network architecture for QA that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection. Our approach is informed by a set of features designed to combine both learned representations and explicit features to capture the connection between questions, answers, and answer justifications. We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9{\\%} rated highly relevant) and answer selection (+6{\\%}"},
{"paper": "Large-scale Simple Question Answering with Memory Networks", "abstract": "Training large-scale question answering systems is complicated because\ntraining sources usually cover a small portion of the range of possible\nquestions. This paper studies the impact of multitask and transfer learning for\nsimple question answering; a setting for which the reasoning required to answer\nis quite easy, as long as one can retrieve the correct evidence given a\nquestion, which can be difficult in large-scale conditions.To this end, we\nintroduce a new dataset of 100k questions that we use in conjunction with\nexisting benchmarks. We conduct our study within the framework of Memory\nNetworks (Weston et al., 2015) because this perspective allows us to eventually\nscale up to more complex reasoning, and show that Memory Networks can be\nsuccessfully trained to achieve excellent performance."},
{"paper": "VICTOR: a Dataset for Brazilian Legal Documents Classification", "abstract": "This paper describes VICTOR, a novel dataset built from Brazil{'}s Supreme Court digitalized legal documents, composed of more than 45 thousand appeals, which includes roughly 692 thousand documents{---}about 4.6 million pages. The dataset contains labeled text data and supports two types of tasks: document type classification; and theme assignment, a multilabel problem.We present baseline results using bag-of-words models, convolutional neural networks, recurrent neural networks and boosting algorithms. We also experiment using linear-chain Conditional Random Fields to leverage the sequential nature of the lawsuits, which we find to lead to improvements on document type classification. Finally we compare a theme classification approach where we use domain knowledge to filter out the less informative document pages to the default one where we use all pages. Contrary to the Court experts{'} expectations, we find that using all available data is the better method. We make the dataset available in three versions of different sizes and contents to encourage explorations of better models and techniques."},
{"paper": "SentenceMIM: A Latent Variable Language Model", "abstract": "We introduce sentenceMIM, a probabilistic auto-encoder for language modelling, trained with Mutual Information Machine (MIM) learning. Previous attempts to learn variational auto-encoders for language data have had mixed success, with empirical performance well below state-of-the-art auto-regressive models, a key barrier being the occurrence of posterior collapse with VAEs.The recently proposed MIM framework encourages high mutual information between observations and latent variables, and is more robust against posterior collapse. This paper formulates a MIM model for text data, along with a corresponding learning algorithm. We demonstrate excellent perplexity (PPL) results on several datasets, and show that the framework learns a rich latent space, allowing for interpolation between sentences of different lengths with a fixed-dimensional latent representation. We also demonstrate the versatility of sentenceMIM by utilizing a trained model for question-answering, a transfer learning task, without fine-tuning. To the best of our knowledge, this is the first latent variable model (LVM) for text modelling that achieves competitive performance with non-LVM models."},
{"paper": "FQuAD: French Question Answering Dataset", "abstract": "Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years.However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In order to track the progress of French Question Answering models we propose a leader-board and we have made the 1.0 version of our dataset freely available at https://illuin-tech.github.io/FQuAD-explorer/."},
{"paper": "Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering", "abstract": "The dominant neural architectures in question answer retrieval are based on\nrecurrent or convolutional encoders configured with complex word matching\nlayers. Given that recent architectural innovations are mostly new word\ninteraction layers or attention-based matching mechanisms, it seems to be a\nwell-established fact that these components are mandatory for good performance.Unfortunately, the memory and computation cost incurred by these complex\nmechanisms are undesirable for practical applications. As such, this paper\ntackles the question of whether it is possible to achieve competitive\nperformance with simple neural architectures. We propose a simple but novel\ndeep learning architecture for fast and efficient question-answer ranking and\nretrieval. More specifically, our proposed model, \\textsc{HyperQA}, is a\nparameter efficient neural network that outperforms other parameter intensive\nmodels such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple\nQA benchmarks. The novelty behind \\textsc{HyperQA} is a pairwise ranking\nobjective that models the relationship between question and answer embeddings\nin Hyperbolic space instead of Euclidean space. This empowers our model with a\nself-organizing ability and enables automatic discovery of latent hierarchies\nwhile learning embeddings of questions and answers. Our model requires no\nfeature engineering, no similarity matrix matching, no complicated attention\nmechanisms nor over-parameterized layers and yet outperforms and remains\ncompetitive to many models that have these functionalities on multiple\nbenchmarks."},
{"paper": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding", "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens.However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Our proposed method allows information integration beyond local windows, which is especially beneficial for question answering (QA) and language modeling tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks."},
{"paper": "MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller", "abstract": "Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document.In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In detail, our method has two novel aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory. Our proposed architecture is widely applicable to other models. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed method outperforms existing methods, especially for lengthy documents."},
{"paper": "Neural Variational Inference for Text Processing", "abstract": "Recent advances in neural variational inference have spawned a renaissance in\ndeep latent variable models. In this paper we introduce a generic variational\ninference framework for generative and conditional models of text.While\ntraditional variational methods derive an analytic approximation for the\nintractable distributions over latent variables, here we construct an inference\nnetwork conditioned on the discrete text input to provide the variational\ndistribution. We validate this framework on two very different text modelling\napplications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document\nrepresentation with a bag-of-words generative model and achieves the lowest\nreported perplexities on two standard test corpora. The neural answer selection\nmodel employs a stochastic representation layer within an attention mechanism\nto extract the semantics between a question and answer pair. On two question\nanswering benchmarks this model exceeds all previous published benchmarks."},
{"paper": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "abstract": "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it.SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6\\% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE."},
{"paper": "Multi-style Generative Reading Comprehension", "abstract": "This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque.The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success."},
{"paper": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "abstract": "Training recurrent neural networks to model long term dependencies is\ndifficult. Hence, we propose to use external linguistic knowledge as an\nexplicit signal to inform the model which memories it should utilize.Specifically, external knowledge is used to augment a sequence with typed edges\nbetween arbitrarily distant elements, and the resulting graph is decomposed\ninto directed acyclic subgraphs. We introduce a model that encodes such graphs\nas explicit memory in recurrent neural networks, and use it to model\ncoreference relations in text. We apply our model to several text comprehension\ntasks and achieve new state-of-the-art results on all considered benchmarks,\nincluding CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out\nof the 20 tasks with only 1000 training examples per task. Analysis of the\nlearned representations further demonstrates the ability of our model to encode\nfine-grained entity information across a document."},
{"paper": "Self-Attentive Associative Memory", "abstract": "Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory.In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering."},
{"paper": "Language Models are Unsupervised Multitask Learners", "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically\r\napproached with supervised learning on taskspecific datasets. We demonstrate that language\r\nmodels begin to learn these tasks without any explicit supervision when trained on a new dataset\r\nof millions of webpages called WebText.When\r\nconditioned on a document plus questions, the answers generated by the language model reach 55\r\nF1 on the CoQA dataset - matching or exceeding\r\nthe performance of 3 out of 4 baseline systems\r\nwithout using the 127,000+ training examples. The capacity of the language model is essential\r\nto the success of zero-shot task transfer and increasing it improves performance in a log-linear\r\nfashion across tasks. Our largest model, GPT-2,\r\nis a 1.5B parameter Transformer that achieves\r\nstate of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting\r\nbut still underfits WebText. Samples from the\r\nmodel reflect these improvements and contain coherent paragraphs of text. These findings suggest\r\na promising path towards building language processing systems which learn to perform tasks from\r\ntheir naturally occurring demonstrations."},
{"paper": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection", "abstract": "We propose TANDA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset.We then perform a second fine-tuning step to adapt the transferred model to the target domain. We demonstrate the benefits of our approach for answer sentence selection, which is a well-known inference task in Question Answering. We built a large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. Our approach establishes the state of the art on two well-known benchmarks, WikiQA and TREC-QA, achieving MAP scores of 92% and 94.3%, respectively, which largely outperform the previous highest scores of 83.4% and 87.5%, obtained in very recent work. We empirically show that TANDA generates more stable and robust models reducing the effort required for selecting optimal hyper-parameters. Additionally, we show that the transfer step of TANDA makes the adaptation step more robust to noise. This enables a more effective use of noisy datasets for fine-tuning. Finally, we also confirm the positive impact of TANDA in an industrial setting, using domain specific datasets subject to different types of noise."},
{"paper": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering", "abstract": "A popular recent approach to answering open-domain questions is to first\nsearch for question-related passages and then apply reading comprehension\nmodels to extract answers. Existing methods usually extract answers from single\npassages independently.But some questions require a combination of evidence\nfrom across different sources to answer correctly. In this paper, we propose\ntwo models which make use of multiple passages to generate their answers. Both\nuse an answer-reranking approach which reorders the answer candidates generated\nby an existing state-of-the-art QA model. We propose two methods, namely,\nstrength-based re-ranking and coverage-based re-ranking, to make use of the\naggregated evidence from different passages to better determine the answer. Our\nmodels have achieved state-of-the-art results on three public open-domain QA\ndatasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with\nabout 8 percentage points of improvement over the former two datasets."},
{"paper": "Reading Wikipedia to Answer Open-Domain Questions", "abstract": "This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles).Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task."},
{"paper": "Densely Connected Attention Propagation for Reading Comprehension", "abstract": "We propose DecaProp (Densely Connected Attention Propagation), a new densely\nconnected neural architecture for reading comprehension (RC). There are two\ndistinct characteristics of our model.Firstly, our model densely connects all\npairwise layers of the network, modeling relationships between passage and\nquery across all hierarchical levels. Secondly, the dense connectors in our\nnetwork are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for\nefficiently forging connections throughout the network. We conduct extensive\nexperiments on four challenging RC benchmarks. Our proposed approach achieves\nstate-of-the-art results on all four, outperforming existing baselines by up to\n$2.6\\%-14.2\\%$ in absolute F1 score."},
{"paper": "Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering", "abstract": "In this paper, we propose a novel end-to-end neural architecture for ranking\ncandidate answers, that adapts a hierarchical recurrent neural network and a\nlatent topic clustering module. With our proposed model, a text is encoded to a\nvector representation from an word-level to a chunk-level to effectively\ncapture the entire meaning.In particular, by adapting the hierarchical\nstructure, our model shows very small performance degradations in longer text\ncomprehension while other state-of-the-art recurrent neural network models\nsuffer from it. Additionally, the latent topic clustering module extracts\nsemantic information from target samples. This clustering module is useful for\nany text related tasks by allowing each data sample to find its nearest topic\ncluster, thus helping the neural network model analyze the entire data. We\nevaluate our models on the Ubuntu Dialogue Corpus and consumer electronic\ndomain question answering dataset, which is related to Samsung products. The\nproposed model shows state-of-the-art results for ranking question-answer\npairs."},
{"paper": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy.In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."},
{"task": "Twitter Sentiment Analysis", "papers": {}},
{"paper": "Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots", "abstract": "In this paper, we study the problem of employing pre-trained language models for multi-turn response selection in retrieval-based chatbots. A new model, named Speaker-Aware BERT (SA-BERT), is proposed in order to make the model aware of the speaker change information, which is an important and intrinsic property of multi-turn dialogues.Furthermore, a speaker-aware disentanglement strategy is proposed to tackle the entangled dialogues. This strategy selects a small number of most important utterances as the filtered context according to the speakers' information in them. Finally, domain adaptation is performed to incorporate the in-domain knowledge into pre-trained language models. Experiments on five public datasets show that our proposed model outperforms the present models on all metrics by large margins and achieves new state-of-the-art performances for multi-turn response selection."},
{"paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."},
{"task": "Multimodal Sentiment Analysis", "papers": {"0": "Gated Mechanism for Attention Based Multimodal Sentiment Analysis", "1": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis"}},
{"task": "Distractor Generation", "papers": {"0": "A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies"}},
{"task": "Aspect-Based Sentiment Analysis", "papers": {"0": "A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "1": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence", "2": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models", "3": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models", "4": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence", "5": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention"}},
{"task": "Sentiment Analysis", "papers": {"0": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "1": "Fine-grained Sentiment Classification using BERT", "2": "Sentiment Classification Using Document Embeddings Trained with Cosine Similarity", "3": "Unsupervised Data Augmentation for Consistency Training", "4": "Unsupervised Data Augmentation for Consistency Training", "5": "A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors", "6": "Unsupervised Data Augmentation for Consistency Training", "7": "Unsupervised Data Augmentation for Consistency Training", "8": "GPU Kernels for Block-Sparse Weights", "9": "Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments", "10": "The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning", "11": "Attentional Encoder Network for Targeted Sentiment Classification", "12": "BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs", "13": "Bag of Tricks for Efficient Text Classification", "14": "Mazajak: An Online Arabic Sentiment Analyser", "15": "Mazajak: An Online Arabic Sentiment Analyser", "16": "Mazajak: An Online Arabic Sentiment Analyser", "17": "RobBERT: a Dutch RoBERTa-based Language Model", "18": "Pre-Training with Whole Word Masking for Chinese BERT", "19": "Pre-Training with Whole Word Masking for Chinese BERT", "20": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models", "21": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models", "22": "AraBERT: Transformer-based Model for Arabic Language Understanding", "23": "AraBERT: Transformer-based Model for Arabic Language Understanding", "24": "AraBERT: Transformer-based Model for Arabic Language Understanding", "25": "ASTD: Arabic Sentiment Tweets Dataset", "26": "What Can We Learn From Almost a Decade of Food Tweets"}},
{"task": "Text Infilling", "papers": {}},
{"task": "Conditional Text Generation", "papers": {}},
{"task": "Table-to-Text Generation", "papers": {"0": "Table-to-text Generation by Structure-aware Seq2seq Learning", "1": "Describing a Knowledge Base"}},
{"paper": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code."},
{"task": "Visual Storytelling", "papers": {"0": "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling"}},
{"task": "Spelling Correction", "papers": {}},
{"task": "Paraphrase Generation", "papers": {"0": "Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator"}},
{"paper": "Gated Mechanism for Attention Based Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis has recently gained popularity because of its relevance to social media posts, customer service calls and video blogs. In this paper, we address three aspects of multimodal sentiment analysis; 1.Cross modal interaction learning, i.e. how multiple modalities contribute to the sentiment, 2. Learning long-term dependencies in multimodal interactions and 3. Fusion of unimodal and cross modal cues. Out of these three, we find that learning cross modal interactions is beneficial for this problem. We perform experiments on two benchmark datasets, CMU Multimodal Opinion level Sentiment Intensity (CMU-MOSI) and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Our approach on both these tasks yields accuracies of 83.9% and 81.1% respectively, which is 1.6% and 1.34% absolute improvement over current state-of-the-art."},
{"task": "Text Style Transfer", "papers": {"0": "Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites", "1": "SentiInc: Incorporating Sentiment Information into Sentiment Transfer Without Parallel Data"}},
{"task": "Data-to-Text Generation", "papers": {"0": "Investigating Pretrained Language Models for Graph-to-Text Generation", "1": "Pragmatically Informative Text Generation", "2": "Investigating Pretrained Language Models for Graph-to-Text Generation", "3": "Investigating Pretrained Language Models for Graph-to-Text Generation", "4": "Text-to-Text Pre-Training for Data-to-Text Tasks", "5": "A Hierarchical Model for Data-to-Text Generation", "6": "A Hierarchical Model for Data-to-Text Generation", "7": "A Hierarchical Model for Data-to-Text Generation", "8": "A Hierarchical Model for Data-to-Text Generation", "9": "Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech", "10": "Text-to-Text Pre-Training for Data-to-Text Tasks", "11": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs", "12": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "13": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "14": "Deep Graph Convolutional Encoders for Structured Data to Text Generation"}},
{"task": "Text Generation", "papers": {"0": "Long Text Generation via Adversarial Training with Leaked Information", "1": "Long Text Generation via Adversarial Training with Leaked Information", "2": "Adversarial Ranking for Language Generation", "3": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders", "4": "Generating Text through Adversarial Training using Skip-Thought Vectors", "5": "An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation", "6": "A Graph-to-Sequence Model for AMR-to-Text Generation", "7": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"}},
{"paper": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis", "abstract": "Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis.In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source: https://github.com/jbdel/MOSEI_UMONS."},
{"paper": "A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies", "abstract": "In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use.There is still room for DG quality improvement. Second, the existing DG designs are mainly for single distractor generation. However, for practical MCQ preparation, multiple distractors are desired. Aiming at these goals, in this paper, we present a new distractor generation scheme with multi-tasking and negative answer training strategies for effectively generating \\textit{multiple} distractors. The experimental results show that (1) our model advances the state-of-the-art result from 28.65 to 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse and show strong distracting power for multiple choice question."},
{"paper": "What Can We Learn From Almost a Decade of Food Tweets", "abstract": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data.We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus."},
{"paper": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models", "abstract": "This work focuses on sentence-level aspect-based sentiment\r\nanalysis for restaurant reviews. A two-stage sentiment analysis algorithm\r\nis proposed.In this method, first a lexicalized domain ontology is used to\r\npredict the sentiment and as a back-up algorithm a neural network with\r\na rotatory attention mechanism (LCR-Rot) is utilized. Furthermore, two\r\nfeatures are added to the backup algorithm. The first extension changes\r\nthe order in which the rotatory attention mechanism operates (LCRRot-inv). The second extension runs over the rotatory attention mechanism for multiple iterations (LCR-Rot-hop). Using the SemEval-2015\r\nand SemEval-2016 data, we conclude that the two-stage method outperforms the baseline methods, albeit with a small percentage. Moreover,\r\nwe find that the method where we iterate multiple times over a rotatory\r\nattention mechanism has the best performance."},
{"paper": "ASTD: Arabic Sentiment Tweets Dataset", "abstract": "NoneNone"},
{"paper": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention", "abstract": "The Web has become the main platform where people express their opinions about entities of interest and their associated aspects. Aspect-Based Sentiment Analysis (ABSA) aims to automatically compute the sentiment towards these aspects from opinionated text.In this paper we extend the state-of-the-art Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two directions. First we replace the non-contextual word embeddings with deep contextual word embeddings in order to better cope with the word semantics in a given text. Second, we use hierarchical attention by adding an extra attention layer to the HAABSA high-level representations in order to increase the method flexibility in modeling the input data. Using two standard datasets (SemEval 2015 and SemEval 2016) we show that the proposed extensions improve the accuracy of the built model for ABSA."},
{"paper": "Table-to-text Generation by Structure-aware Seq2seq Learning", "abstract": "Table-to-text generation aims to generate a description for a factual table\nwhich can be viewed as a set of field-value records. To encode both the content\nand the structure of a table, we propose a novel structure-aware seq2seq\narchitecture which consists of field-gating encoder and description generator\nwith dual attention.In the encoding phase, we update the cell memory of the\nLSTM unit by a field gate and its corresponding field value in order to\nincorporate field information into table representation. In the decoding phase,\ndual attention mechanism which contains word level attention and field level\nattention is proposed to model the semantic relevance between the generated\ndescription and the table. We conduct experiments on the \\texttt{WIKIBIO}\ndataset which contains over 700k biographies and corresponding infoboxes from\nWikipedia. The attention visualizations and case studies show that our model is\ncapable of generating coherent and informative descriptions based on the\ncomprehensive understanding of both the content and the structure of a table. Automatic evaluations also show our model outperforms the baselines by a great\nmargin. Code for this work is available on\nhttps://github.com/tyliupku/wiki2bio."},
{"paper": "Describing a Knowledge Base", "abstract": "We aim to automatically generate natural language descriptions about an input\nstructured knowledge base (KB). We build our generation framework based on a\npointer network which can copy facts from the input KB, and add two attention\nmechanisms: (i) slot-aware attention to capture the association between a slot\ntype and its corresponding slot value; and (ii) a new \\emph{table position\nself-attention} to capture the inter-dependencies among related slots.For\nevaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we\npropose a KB reconstruction based metric by extracting a KB from the generation\noutput and comparing it with the input KB. We also create a new data set which\nincludes 106,216 pairs of structured KBs and their corresponding natural\nlanguage descriptions for two distinct entity types. Experiments show that our\napproach significantly outperforms state-of-the-art methods. The reconstructed\nKB achieves 68.8% - 72.6% F-score."},
{"paper": "A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "abstract": "Aspect-based sentiment analysis (ABSA) task is a multi-grained task of natural language processing and consists of two subtasks: aspect term extraction (ATE) and aspect polarity classification (APC). Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction.Besides, the existing researches do not pay attention to the research of the Chinese-oriented ABSA task. Based on the local context focus (LCF) mechanism, this paper firstly proposes a multi-task learning model for Chinese-oriented aspect-based sentiment analysis, namely LCF-ATEPC. Compared with existing models, this model equips the capability of extracting aspect term and inferring aspect term polarity synchronously, moreover, this model is effective to analyze both Chinese and English comments simultaneously and the experiment on a multilingual mixed dataset proved its availability. By integrating the domain-adapted BERT model, the LCF-ATEPC model achieved the state-of-the-art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets. Besides, the experimental results on the most commonly used SemEval-2014 task4 Restaurant and Laptop datasets outperform the state-of-the-art performance on the ATE and APC subtask."},
{"paper": "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling", "abstract": "Though impressive results have been achieved in visual captioning, the task\nof generating abstract stories from photo streams is still a little-tapped\nproblem. Different from captions, stories have more expressive language styles\nand contain many imaginary concepts that do not appear in the images.Thus it\nposes challenges to behavioral cloning algorithms. Furthermore, due to the\nlimitations of automatic metrics on evaluating story quality, reinforcement\nlearning methods with hand-crafted rewards also face difficulties in gaining an\noverall performance boost. Therefore, we propose an Adversarial REward Learning\n(AREL) framework to learn an implicit reward function from human\ndemonstrations, and then optimize policy search with the learned reward\nfunction. Though automatic eval- uation indicates slight performance boost over\nstate-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation\nshows that our approach achieves significant improvement in generating more\nhuman-like stories than SOTA systems."},
{"paper": "Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator", "abstract": "In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we\npropose a novel method for obtaining sentence-level embeddings.This is\nobtained by a simple method in the context of solving the paraphrase generation\ntask. If we use a sequential encoder-decoder model for generating paraphrase,\nwe would like the generated paraphrase to be semantically close to the original\nsentence. One way to ensure this is by adding constraints for true paraphrase\nembeddings to be close and unrelated paraphrase candidate sentence embeddings\nto be far. This is ensured by using a sequential pair-wise discriminator that\nshares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being\ntoo large. This loss is used in combination with a sequential encoder-decoder\nnetwork. We also validated our method by evaluating the obtained embeddings for\na sentiment analysis task. The proposed method results in semantic embeddings\nand outperforms the state-of-the-art on the paraphrase generation and sentiment\nanalysis task on standard datasets. These results are also shown to be\nstatistically significant."},
{"paper": "SentiInc: Incorporating Sentiment Information into Sentiment Transfer Without Parallel Data", "abstract": "Sentiment-to-sentiment transfer involves changing the sentiment of the given text while preserving the underlying information. In this work, we present a model SentiInc for sentiment-to-sentiment transfer using unpaired mono-sentiment data.Existing sentiment-to-sentiment transfer models ignore the valuable sentiment-specific details already present in the text. We address this issue by providing a simple framework for encoding sentiment-specific information in the target sentence while preserving the content information. This is done by incorporating sentiment based loss in the back-translation based style transfer. Extensive experiments over the Yelp dataset show that the SentiInc outperforms state-of-the-art methods by a margin of as large as equation ~11% in G-score. The results also demonstrate that our model produces sentiment-accurate and information-preserved sentences."},
{"paper": "AraBERT: Transformer-based Model for Arabic Language Understanding", "abstract": "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle.Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/arabert hoping to encourage research and applications for Arabic NLP."},
{"paper": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models", "abstract": "Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context.We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods."},
{"paper": "Deep Graph Convolutional Encoders for Structured Data to Text Generation", "abstract": "Most previous work on neural text generation from graph-structured data\nrelies on standard sequence-to-sequence methods. These approaches linearise the\ninput graph to be fed to a recurrent neural network.In this paper, we propose\nan alternative encoder based on graph convolutional networks that directly\nexploits the input structure. We report results on two graph-to-sequence\ndatasets that empirically show the benefits of explicitly encoding the input\ngraph structure."},
{"paper": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning", "abstract": "Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging.In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., {dog, frisbee, catch, throw}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., \"a man throws a frisbee and his dog catches it\"). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6 v.s. 63.5 in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9 to 78.4 in dev accuracy) by generating additional context."},
{"paper": "RobBERT: a Dutch RoBERTa-based Language Model", "abstract": "Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version.Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model's fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications."},
{"paper": "A Graph-to-Sequence Model for AMR-to-Text Generation", "abstract": "The problem of AMR-to-text generation is to recover a text representing the\nsame meaning as an input AMR graph. The current state-of-the-art method uses a\nsequence-to-sequence model, leveraging LSTM for encoding a linearized AMR\nstructure.Although being able to model non-local semantic information, a\nsequence LSTM can lose information from the AMR graph structure, and thus faces\nchallenges with large graphs, which result in long sequences. We introduce a\nneural graph-to-sequence model, using a novel LSTM structure for directly\nencoding graph-level semantics. On a standard benchmark, our model shows\nsuperior results to existing methods in the literature."},
{"paper": "An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation", "abstract": "Generating semantically coherent responses is still a major challenge in\ndialogue generation. Different from conventional text generation tasks, the\nmapping between inputs and responses in conversations is more complicated,\nwhich highly demands the understanding of utterance-level semantic dependency,\na relation between the whole meanings of inputs and outputs.To address this\nproblem, we propose an Auto-Encoder Matching (AEM) model to learn such\ndependency. The model contains two auto-encoders and one mapping module. The\nauto-encoders learn the semantic representations of inputs and responses, and\nthe mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our\nmodel is capable of generating responses of high coherence and fluency compared\nto baseline models. The code is available at https://github.com/lancopku/AMM"},
{"paper": "Generating Text through Adversarial Training using Skip-Thought Vectors", "abstract": "GANs have been shown to perform exceedingly well on tasks pertaining to image generation and style transfer. In the field of language modelling, word embeddings such as GLoVe and word2vec are state-of-the-art methods for applying neural network models on textual data.Attempts have been made to utilize GANs with word embeddings for text generation. This study presents an approach to text generation using Skip-Thought sentence embeddings with GANs based on gradient penalty functions and f-measures. The proposed architecture aims to reproduce writing style in the generated text by modelling the way of expression at a sentence level across all the works of an author. Extensive experiments were run in different embedding settings on a variety of tasks including conditional text generation and language generation. The model outperforms baseline text generation networks across several automated evaluation metrics like BLEU-n, METEOR and ROUGE. Further, wide applicability and effectiveness in real life tasks are demonstrated through human judgement scores."},
{"paper": "Pre-Training with Whole Word Masking for Chinese BERT", "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks. Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task. The proposed models are verified on various NLP tasks, across sentence-level to document-level, including machine reading comprehension (CMRC 2018, DRCD, CJRC), natural language inference (XNLI), sentiment classification (ChnSentiCorp), sentence pair matching (LCQMC, BQ Corpus), and document classification (THUCNews). Experimental results on these datasets show that the whole word masking could bring another significant gain. Moreover, we also examine the effectiveness of the Chinese pre-trained models: BERT, ERNIE, BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, and RoBERTa-wwm-ext-large. We release all the pre-trained models: \\url{https://github.com/ymcui/Chinese-BERT-wwm"},
{"paper": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders", "abstract": "The variational autoencoder (VAE) is a popular combination of deep latent\nvariable model and accompanying variational learning technique. By using a\nneural inference network to approximate the model's posterior on latent\nvariables, VAEs efficiently parameterize a lower bound on marginal data\nlikelihood that can be optimized directly via gradient methods.In practice,\nhowever, VAE training often results in a degenerate local optimum known as\n\"posterior collapse\" where the model learns to ignore the latent variable and\nthe approximate posterior mimics the prior. In this paper, we investigate\nposterior collapse from the perspective of training dynamics. We find that\nduring the initial stages of training the inference network fails to\napproximate the model's true posterior, which is a moving target. As a result,\nthe model is encouraged to ignore the latent encoding and posterior collapse\noccurs. Based on this observation, we propose an extremely simple modification\nto VAE training to reduce inference lag: depending on the model's current\nmutual information between latent variable and observation, we aggressively\noptimize the inference network before performing each model update. Despite\nintroducing neither new model components nor significant complexity over basic\nVAE, our approach is able to avoid the problem of collapse that has plagued a\nlarge amount of previous work. Empirically, our approach outperforms strong\nautoregressive baselines on text and image benchmarks in terms of held-out\nlikelihood, and is competitive with more complex techniques for avoiding\ncollapse while being substantially faster."},
{"paper": "Adversarial Ranking for Language Generation", "abstract": "Generative adversarial networks (GANs) have great successes on synthesizing\ndata. However, the existing GANs restrict the discriminator to be a binary\nclassifier, and thus limit their learning capacity for tasks that need to\nsynthesize output with rich structures such as natural language descriptions.In this paper, we propose a novel generative adversarial network, RankGAN, for\ngenerating high-quality language descriptions. Rather than training the\ndiscriminator to learn and assign absolute binary predicate for individual data\nsample, the proposed RankGAN is able to analyze and rank a collection of\nhuman-written and machine-written sentences by giving a reference group. By\nviewing a set of data samples collectively and evaluating their quality through\nrelative ranking scores, the discriminator is able to make better assessment\nwhich in turn helps to learn a better generator. The proposed RankGAN is\noptimized through the policy gradient technique. Experimental results on\nmultiple public datasets clearly demonstrate the effectiveness of the proposed\napproach."},
{"paper": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs", "abstract": "Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected.In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models which encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively."},
{"paper": "Long Text Generation via Adversarial Training with Leaked Information", "abstract": "Automatically generating coherent and semantically meaningful text has many\napplications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN)\nthat use a discriminative model to guide the training of the generative model\nas a reinforcement learning policy has shown promising results in text\ngeneration.However, the scalar guiding signal is only available after the\nentire text has been generated and lacks intermediate information about text\nstructure during the generative process. As such, it limits its success when\nthe length of the generated text samples is long (more than 20 words). In this\npaper, we propose a new framework, called LeakGAN, to address the problem for\nlong text generation. We allow the discriminative net to leak its own\nhigh-level extracted features to the generative net to further help the\nguidance. The generator incorporates such informative signals into all\ngeneration steps through an additional Manager module, which takes the\nextracted features of current generated words and outputs a latent vector to\nguide the Worker module for next-word generation. Our extensive experiments on\nsynthetic data and various real-world tasks with Turing test demonstrate that\nLeakGAN is highly effective in long text generation and also improves the\nperformance in short text generation scenarios. More importantly, without any\nsupervision, LeakGAN would be able to implicitly learn sentence structures only\nthrough the interaction between Manager and Worker."},
{"paper": "Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech", "abstract": "While there is a large body of research studying deep learning methods for text generation from structured data, almost all of it focuses purely on English. In this paper, we study the effectiveness of machine translation based pre-training for data-to-text generation in non-English languages.Since the structured data is generally expressed in English, text generation into other languages involves elements of translation, transliteration and copying - elements already encoded in neural machine translation systems. Moreover, since data-to-text corpora are typically small, this task can benefit greatly from pre-training. Based on our experiments on Czech, a morphologically complex language, we find that pre-training lets us train end-to-end models with significantly improved performance, as judged by automatic metrics and human evaluation. We also show that this approach enjoys several desirable properties, including improved performance in low data scenarios and robustness to unseen slot values."},
{"paper": "A Hierarchical Model for Data-to-Text Generation", "abstract": "Transcribing structured data into natural language descriptions has emerged as a challenging task, referred to as \"data-to-text\". These structures generally regroup multiple elements, as well as their attributes.Most attempts rely on translation encoder-decoder methods which linearize elements into a sequence. This however loses most of the structure contained in the data. In this work, we propose to overpass this limitation with a hierarchical model that encodes the data-structure at the element-level and the structure level. Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics."},
{"paper": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "abstract": "End-to-end neural data-to-text (D2T) generation has recently emerged as an alternative to pipeline-based architectures. However, it has faced challenges in generalizing to new domains and generating semantically consistent text.In this work, we present DataTuner, a neural, end-to-end data-to-text generation system that makes minimal assumptions about the data representation and the target domain. We take a two-stage generation-reranking approach, combining a fine-tuned language model with a semantic fidelity classifier. Each of our components is learnt end-to-end without the need for dataset-specific heuristics, entity delexicalization, or post-processing. We show that DataTuner achieves state of the art results on the automated metrics across four major D2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with a fluency assessed by human annotators nearing or exceeding the human-written reference texts. We further demonstrate that the model-based semantic fidelity scorer in DataTuner is a better assessment tool compared to traditional, heuristic-based measures. Our generated text has a significantly better semantic fidelity than the state of the art across all four datasets"},
{"paper": "Pragmatically Informative Text Generation", "abstract": "We improve the informativeness of models for conditional text generation\nusing techniques from computational pragmatics. These techniques formulate\nlanguage production as a game between speakers and listeners, in which a\nspeaker should generate output text that a listener can use to correctly\nidentify the original input that the text describes.While such approaches are\nwidely used in cognitive science and grounded language learning, they have\nreceived less attention for more standard language generation tasks. We\nconsider two pragmatic modeling methods for text generation: one where\npragmatics is imposed by information preservation, and another where pragmatics\nis imposed by explicit modeling of distractors. We find that these methods\nimprove the performance of strong existing systems for abstractive\nsummarization and generation from structured meaning representations."},
{"paper": "Mazajak: An Online Arabic Sentiment Analyser", "abstract": "Sentiment analysis (SA) is one of the most useful natural language processing applications. Literature is flooding with many papers and systems addressing this task, but most of the work is focused on English.In this paper, we present {``}Mazajak{''}, an online system for Arabic SA. The system is based on a deep learning model, which achieves state-of-the-art results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such system should assist various applications and research that rely on sentiment analysis as a tool."},
{"paper": "Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites", "abstract": "This paper shows that standard assessment methodology for style transfer has several significant problems. First, the standard metrics for style accuracy and semantics preservation vary significantly on different re-runs.Therefore one has to report error margins for the obtained results. Second, starting with certain values of bilingual evaluation understudy (BLEU) between input and output and accuracy of the sentiment transfer the optimization of these two standard metrics diverge from the intuitive goal of the style transfer task. Finally, due to the nature of the task itself, there is a specific dependence between these two metrics that could be easily manipulated. Under these circumstances, we suggest taking BLEU between input and human-written reformulations into consideration for benchmarks. We also propose three new architectures that outperform state of the art in terms of this metric."},
{"paper": "Text-to-Text Pre-Training for Data-to-Text Tasks", "abstract": "We study the pre-train + fine-tune strategy for data-to-text tasks. Fine-tuning T5 achieves state-of-the-art results on the WebNLG, MultiWoz and ToTTo benchmarks.Such transfer learning enables training of fully end-to-end models that do not rely on any intermediate planning steps, delexicalization or copy mechanisms. T5 pre-training also enables stronger generalization, as evidenced by large improvements on out-of-domain test sets. We hope our work serves as a useful baseline for future research, as pre-training becomes ever more prevalent for data-to-text tasks."},
{"paper": "BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs", "abstract": "In this paper we describe our attempt at producing a state-of-the-art Twitter\nsentiment classifier using Convolutional Neural Networks (CNNs) and Long Short\nTerm Memory (LSTMs) networks. Our system leverages a large amount of unlabeled\ndata to pre-train word embeddings.We then use a subset of the unlabeled data\nto fine tune the embeddings using distant supervision. The final CNNs and LSTMs\nare trained on the SemEval-2017 Twitter dataset where the embeddings are fined\ntuned again. To boost performances we ensemble several CNNs and LSTMs together. Our approach achieved first rank on all of the five English subtasks amongst 40\nteams."},
{"paper": "Investigating Pretrained Language Models for Graph-to-Text Generation", "abstract": "Graph-to-text generation, a subtask of data-to-text generation, aims to generate fluent texts from graph-based data. Many graph-to-text models have shown strong performance in this task employing specialized graph encoders.However, recent approaches employ large pretrained language models (PLMs) achieving state-of-the-art results in data-to-text generation. In this paper, we aim to investigate the impact of large PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. Our analysis shows that PLMs such as BART and T5 achieve state-of-the-art results in graph-to-text benchmarks without explicitly encoding the graph structure. We also demonstrate that task-adaptive pretraining strategies are beneficial to the target task, improving even further the state of the art in two benchmarks for graph-to-text generation. In a final analysis, we investigate possible reasons for the PLMs' success on graph-to-text tasks. We find evidence that their knowledge about the world gives them a big advantage, especially when generating texts from KGs."},
{"paper": "Bag of Tricks for Efficient Text Classification", "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation.We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute."},
{"paper": "The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning", "abstract": "Recent advances in deep learning have facilitated the demand of neural models for real applications. In practice, these applications often need to be deployed with limited resources while keeping high accuracy.This paper touches the core of neural models in NLP, word embeddings, and presents a new embedding distillation framework that remarkably reduces the dimension of word embeddings without compromising accuracy. A novel distillation ensemble approach is also proposed that trains a high-efficient student model using multiple teacher models. In our approach, the teacher models play roles only during training such that the student model operates on its own without getting supports from the teacher models during decoding, which makes it eighty times faster and lighter than other typical ensemble methods. All models are evaluated on seven document classification datasets and show a significant advantage over the teacher models for most cases. Our analysis depicts insightful transformation of word embeddings from distillation and suggests a future direction to ensemble approaches using neural models."},
{"paper": "Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments", "abstract": "This paper introduces PyDCI, a new implementation of Distributional\nCorrespondence Indexing (DCI) written in Python. DCI is a transfer learning\nmethod for cross-domain and cross-lingual text classification for which we had\nprovided an implementation (here called JaDCI) built on top of JaTeCS, a Java\nframework for text classification.PyDCI is a stand-alone version of DCI that\nexploits scikit-learn and the SciPy stack. We here report on new experiments\nthat we have carried out in order to test PyDCI, and in which we use as\nbaselines new high-performing methods that have appeared after DCI was\noriginally proposed. These experiments show that, thanks to a few subtle ways\nin which we have improved DCI, PyDCI outperforms both JaDCI and the\nabove-mentioned high-performing methods, and delivers the best known results on\nthe two popular benchmarks on which we had tested DCI, i.e.,\nMultiDomainSentiment (a.k.a. MDS -- for cross-domain adaptation) and\nWebis-CLS-10 (for cross-lingual adaptation). PyDCI, together with the code\nallowing to replicate our experiments, is available at\nhttps://github.com/AlexMoreo/pydci ."},
{"paper": "GPU Kernels for Block-Sparse Weights", "abstract": "We\u2019re releasing highly optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. The kernels allow for efficient evaluation and differentiation of linear layers, including convolutional layers, with flexibly configurable block-sparsity patterns in the weight matrix.We find that depending on the sparsity, these kernels can run orders of magnitude faster than the best available alternatives such as cuBLAS. Using the kernels we improve upon the state-of-the-art in text sentiment analysis and generative modeling of text and images. By releasing our kernels in the open we aim to spur further\r\nadvancement in model and algorithm design."},
{"paper": "Attentional Encoder Network for Targeted Sentiment Classification", "abstract": "Targeted sentiment classification aims at determining the sentimental\ntendency towards specific targets. Most of the previous approaches model\ncontext and target words with RNN and attention.However, RNNs are difficult to\nparallelize and truncated backpropagation through time brings difficulty in\nremembering long-term patterns. To address this issue, this paper proposes an\nAttentional Encoder Network (AEN) which eschews recurrence and employs\nattention based encoders for the modeling between context and target. We raise\nthe label unreliability issue and introduce label smoothing regularization. We\nalso apply pre-trained BERT to this task and obtain new state-of-the-art\nresults. Experiments and analysis demonstrate the effectiveness and lightweight\nof our model."},
{"paper": "A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors", "abstract": "Motivations like domain adaptation, transfer learning, and feature learning\nhave fueled interest in inducing embeddings for rare or unseen words, n-grams,\nsynsets, and other textual features. This paper introduces a la carte\nembedding, a simple and general alternative to the usual word2vec-based\napproaches for building such representations that is based upon recent\ntheoretical results for GloVe-like embeddings.Our method relies mainly on a\nlinear transformation that is efficiently learnable using pretrained word\nvectors and linear regression. This transform is applicable on the fly in the\nfuture when a new text feature or rare word is encountered, even if only a\nsingle usage example is available. We introduce a new dataset showing how the a\nla carte method requires fewer examples of words in context to learn\nhigh-quality embeddings and we obtain state-of-the-art results on a nonce task\nand some unsupervised document classification tasks."},
{"paper": "Sentiment Classification Using Document Embeddings Trained with Cosine Similarity", "abstract": "In document-level sentiment classification, each document must be mapped to a fixed length vector. Document embedding models map each document to a dense, low-dimensional vector in continuous vector space.This paper proposes training document embeddings using cosine similarity instead of dot product. Experiments on the IMDB dataset show that accuracy is improved when using cosine similarity compared to using dot product, while using feature combination with Naive Bayes weighted bag of n-grams achieves a new state of the art accuracy of 97.42{\\%}. Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine"},
{"paper": "Fine-grained Sentiment Classification using BERT", "abstract": "Sentiment classification is an important process in understanding people's perception towards a product, service, or topic. Many natural language processing models have been proposed to solve the sentiment classification problem.However, most of them have focused on binary sentiment classification. In this paper, we use a promising deep learning model called BERT to solve the fine-grained sentiment classification task. Experiments show that our model outperforms other popular models for this task without sophisticated architecture. We also demonstrate the effectiveness of transfer learning in natural language processing in the process."},
{"paper": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence", "abstract": "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained\nopinion polarity towards a specific aspect, is a challenging subtask of\nsentiment analysis (SA). In this paper, we construct an auxiliary sentence from\nthe aspect and convert ABSA to a sentence-pair classification task, such as\nquestion answering (QA) and natural language inference (NLI).We fine-tune the\npre-trained model from BERT and achieve new state-of-the-art results on\nSentiHood and SemEval-2014 Task 4 datasets."},
{"paper": "Unsupervised Data Augmentation for Consistency Training", "abstract": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise.In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda."}
]